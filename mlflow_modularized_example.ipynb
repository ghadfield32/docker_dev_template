{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/mlops/config.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/mlops/config.py\n",
        "\"\"\"Central MLflow configuration for consistent experiment tracking.\"\"\"\n",
        "import os\n",
        "\n",
        "# ─── MLflow configuration ──────────────────────────────────────────────────\n",
        "# Use Docker service-name so this works inside the compose network\n",
        "# Falls back to local file store for standalone usage\n",
        "TRACKING_URI = os.getenv(\"MLFLOW_TRACKING_URI\", \"http://mlflow:5000\")\n",
        "EXPERIMENT_NAME = \"iris_classification\"\n",
        "ARTIFACT_ROOT = os.getenv(\"MLFLOW_ARTIFACT_ROOT\", \"./mlruns\")\n",
        "\n",
        "# ─── Model registry ────────────────────────────────────────────────────────\n",
        "MODEL_NAME = \"iris_classifier\"\n",
        "MODEL_STAGE_PRODUCTION = \"Production\"\n",
        "MODEL_STAGE_STAGING = \"Staging\"\n",
        "\n",
        "# ─── Dataset defaults ──────────────────────────────────────────────────────\n",
        "RANDOM_STATE = 42\n",
        "TEST_SIZE = 0.2 \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/mlops/logging.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/mlops/logging.py\n",
        "\"\"\"MLflow logging utilities for metrics, parameters, and artifacts.\"\"\"\n",
        "import mlflow\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Dict, Any, Optional\n",
        "from matplotlib.figure import Figure\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    confusion_matrix\n",
        ")\n",
        "\n",
        "\n",
        "def log_model_metrics(y_true, y_pred, prefix: str = \"\") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Calculate and log model evaluation metrics.\n",
        "    \n",
        "    Args:\n",
        "        y_true: True labels\n",
        "        y_pred: Predicted labels\n",
        "        prefix: Optional prefix for metric names\n",
        "        \n",
        "    Returns:\n",
        "        Dictionary of calculated metrics\n",
        "    \"\"\"\n",
        "    metrics = {\n",
        "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
        "        \"precision_macro\": precision_score(y_true, y_pred, average='macro'),\n",
        "        \"recall_macro\": recall_score(y_true, y_pred, average='macro'),\n",
        "        \"f1_macro\": f1_score(y_true, y_pred, average='macro')\n",
        "    }\n",
        "    \n",
        "    # Add prefix if provided\n",
        "    if prefix:\n",
        "        metrics = {f\"{prefix}_{k}\": v for k, v in metrics.items()}\n",
        "    \n",
        "    # Log metrics to MLflow\n",
        "    mlflow.log_metrics(metrics)\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "\n",
        "def _log_fig(fig: Figure, artifact_name: str) -> None:\n",
        "    \"\"\"Log a Matplotlib figure directly without temp files.\"\"\"\n",
        "    mlflow.log_figure(fig, artifact_file=artifact_name)\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "def log_confusion_matrix(y_true, y_pred, *, class_names=None,\n",
        "                         artifact_name: str = \"confusion_matrix.png\"):\n",
        "    \"\"\"Create + log confusion matrix using mlflow.log_figure.\"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "                xticklabels=class_names, yticklabels=class_names, ax=ax)\n",
        "    ax.set(xlabel=\"Predicted\", ylabel=\"Actual\", title=\"Confusion Matrix\")\n",
        "    _log_fig(fig, artifact_name)\n",
        "\n",
        "\n",
        "def log_feature_importance(feature_names: list, importances: list,\n",
        "                           artifact_name: str = \"feature_importance.png\"):\n",
        "    \"\"\"Bar plot logged via mlflow.log_figure (no disk I/O).\"\"\"\n",
        "    imp_df = (pd.DataFrame({\"feature\": feature_names,\n",
        "                            \"importance\": importances})\n",
        "              .sort_values(\"importance\"))\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    sns.barplot(data=imp_df, x=\"importance\", y=\"feature\", ax=ax)\n",
        "    ax.set_title(\"Feature Importances\")\n",
        "    _log_fig(fig, artifact_name)\n",
        "\n",
        "\n",
        "def log_parameters(params: Dict[str, Any]) -> None:\n",
        "    \"\"\"\n",
        "    Log parameters to MLflow.\n",
        "    \n",
        "    Args:\n",
        "        params: Dictionary of parameter names and values\n",
        "    \"\"\"\n",
        "    mlflow.log_params(params)\n",
        "\n",
        "\n",
        "def log_dataset_info(X_train, X_test, y_train, y_test) -> None:\n",
        "    \"\"\"\n",
        "    Log dataset information as parameters.\n",
        "    \n",
        "    Args:\n",
        "        X_train: Training features\n",
        "        X_test: Test features\n",
        "        y_train: Training labels\n",
        "        y_test: Test labels\n",
        "    \"\"\"\n",
        "    dataset_params = {\n",
        "        \"train_size\": len(X_train),\n",
        "        \"test_size\": len(X_test),\n",
        "        \"n_features\": (X_train.shape[1] if hasattr(X_train, 'shape') \n",
        "                       else len(X_train[0])),\n",
        "        \"n_classes\": (len(set(y_train)) if hasattr(y_train, '__iter__') \n",
        "                      else 1)\n",
        "    }\n",
        "    \n",
        "    log_parameters(dataset_params) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/mlops/experiment.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/mlops/experiment.py\n",
        "\n",
        "\"\"\"Training utilities with MLflow integration.\"\"\"\n",
        "import mlflow\n",
        "import optuna\n",
        "from typing import Optional\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from .config import RANDOM_STATE, TEST_SIZE\n",
        "from .experiment_utils import setup_mlflow_experiment\n",
        "\n",
        "# Re-export for convenience\n",
        "__all__ = ['setup_mlflow_experiment', 'load_and_prepare_iris_data',\n",
        "           'train_logistic_regression', 'train_random_forest_with_optimization']\n",
        "from .logging import (\n",
        "    log_model_metrics,\n",
        "    log_confusion_matrix,\n",
        "    log_feature_importance,\n",
        "    log_dataset_info,\n",
        "    log_parameters\n",
        ")\n",
        "\n",
        "\n",
        "def load_and_prepare_iris_data(test_size: float = TEST_SIZE,\n",
        "                               random_state: int = RANDOM_STATE):\n",
        "    \"\"\"\n",
        "    Load and prepare the Iris dataset.\n",
        "    \n",
        "    Args:\n",
        "        test_size: Fraction of data to use for testing\n",
        "        random_state: Random state for reproducibility\n",
        "        \n",
        "    Returns:\n",
        "        Tuple of (X_train_scaled, X_test_scaled, y_train, y_test, \n",
        "                 feature_names, target_names, scaler)\n",
        "    \"\"\"\n",
        "    # Load dataset\n",
        "    iris = load_iris()\n",
        "    X, y = iris.data, iris.target\n",
        "    \n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=random_state\n",
        "    )\n",
        "    \n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    \n",
        "    return (X_train_scaled, X_test_scaled, y_train, y_test,\n",
        "            iris.feature_names, iris.target_names, scaler)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/mlops/model_registry.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/mlops/model_registry.py\n",
        "\"\"\"MLflow model registry utilities.\"\"\"\n",
        "import mlflow\n",
        "from typing import Optional, Dict, Any\n",
        "from .config import MODEL_NAME, MODEL_STAGE_PRODUCTION\n",
        "\n",
        "\n",
        "def register_model(model_uri: str, \n",
        "                   model_name: Optional[str] = None,\n",
        "                   description: Optional[str] = None) -> str:\n",
        "    \"\"\"\n",
        "    Register a model in the MLflow model registry using the fluent client API.\n",
        "    \n",
        "    Args:\n",
        "        model_uri: URI of the model to register\n",
        "        model_name: Name for the registered model\n",
        "        description: Optional description\n",
        "        \n",
        "    Returns:\n",
        "        Model version\n",
        "    \"\"\"\n",
        "    name = model_name or MODEL_NAME\n",
        "    client = mlflow.tracking.MlflowClient()\n",
        "    \n",
        "    try:\n",
        "        # Create registered model if it doesn't exist\n",
        "        if not client.get_registered_model(name, silent=True):\n",
        "            client.create_registered_model(name)\n",
        "            print(f\"Created new registered model: {name}\")\n",
        "            \n",
        "        # Create new version\n",
        "        mv = client.create_model_version(\n",
        "            name=name,\n",
        "            source=model_uri,\n",
        "            description=description\n",
        "        )\n",
        "        print(f\"Created version {mv.version} of model {name}\")\n",
        "        return mv.version\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Failed to register model: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def promote_model_to_stage(model_name: Optional[str] = None,\n",
        "                           version: Optional[str] = None,\n",
        "                           stage: str = MODEL_STAGE_PRODUCTION) -> None:\n",
        "    \"\"\"\n",
        "    Promote a model version to a specific stage using the fluent client.\n",
        "    \n",
        "    Args:\n",
        "        model_name: Name of the registered model\n",
        "        version: Version to promote (if None, promotes latest)\n",
        "        stage: Target stage\n",
        "    \"\"\"\n",
        "    name = model_name or MODEL_NAME\n",
        "    client = mlflow.tracking.MlflowClient()\n",
        "    \n",
        "    try:\n",
        "        # Get latest version if not specified\n",
        "        if version is None:\n",
        "            latest = client.get_latest_versions(name, stages=[\"None\"])\n",
        "            if not latest:\n",
        "                raise ValueError(f\"No versions found for model {name}\")\n",
        "            version = latest[0].version\n",
        "        \n",
        "        # Transition to stage\n",
        "        client.transition_model_version_stage(\n",
        "            name=name,\n",
        "            version=version,\n",
        "            stage=stage\n",
        "        )\n",
        "        print(f\"Promoted model {name} version {version} to {stage}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Failed to promote model: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def load_model_from_registry(model_name: Optional[str] = None,\n",
        "                             stage: str = MODEL_STAGE_PRODUCTION):\n",
        "    \"\"\"\n",
        "    Load a model from the registry by name and stage.\n",
        "    \n",
        "    Args:\n",
        "        model_name: Name of the registered model\n",
        "        stage: Stage to load from\n",
        "        \n",
        "    Returns:\n",
        "        Loaded model\n",
        "    \"\"\"\n",
        "    name = model_name or MODEL_NAME\n",
        "    model_uri = f\"models:/{name}/{stage}\"\n",
        "    \n",
        "    try:\n",
        "        model = mlflow.sklearn.load_model(model_uri)\n",
        "        print(f\"Loaded model {name} from {stage} stage\")\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load model from registry: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def load_model_from_run(run_id: str, artifact_path: str = \"model\"):\n",
        "    \"\"\"\n",
        "    Load a model from a specific run.\n",
        "    \n",
        "    Args:\n",
        "        run_id: MLflow run ID\n",
        "        artifact_path: Path to the model artifact\n",
        "        \n",
        "    Returns:\n",
        "        Loaded model\n",
        "    \"\"\"\n",
        "    model_uri = f\"runs:/{run_id}/{artifact_path}\"\n",
        "    \n",
        "    try:\n",
        "        model = mlflow.sklearn.load_model(model_uri)\n",
        "        print(f\"Loaded model from run {run_id}\")\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load model from run: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def get_model_info(model_name: Optional[str] = None,\n",
        "                   stage: str = MODEL_STAGE_PRODUCTION) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Get information about a registered model using the fluent client.\n",
        "    \n",
        "    Args:\n",
        "        model_name: Name of the registered model\n",
        "        stage: Stage to get info for\n",
        "        \n",
        "    Returns:\n",
        "        Model information dictionary\n",
        "    \"\"\"\n",
        "    name = model_name or MODEL_NAME\n",
        "    client = mlflow.tracking.MlflowClient()\n",
        "    \n",
        "    try:\n",
        "        model_version = client.get_latest_versions(name, stages=[stage])[0]\n",
        "        \n",
        "        return {\n",
        "            \"name\": model_version.name,\n",
        "            \"version\": model_version.version,\n",
        "            \"stage\": model_version.current_stage,\n",
        "            \"description\": model_version.description,\n",
        "            \"creation_timestamp\": model_version.creation_timestamp,\n",
        "            \"last_updated_timestamp\": model_version.last_updated_timestamp,\n",
        "            \"run_id\": model_version.run_id\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to get model info: {e}\")\n",
        "        raise \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/mlops/training.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/mlops/training.py\n",
        "\"\"\"Training utilities with MLflow integration.\"\"\"\n",
        "import mlflow\n",
        "from mlflow import sklearn  # type: ignore\n",
        "from mlflow import models  # type: ignore\n",
        "import optuna\n",
        "from optuna.integration.mlflow import MLflowCallback\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Optional, Tuple, List, Callable, cast, Any, Dict, NoReturn, TypeAlias\n",
        "from numpy.typing import NDArray\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.utils import Bunch\n",
        "\n",
        "from .config import RANDOM_STATE, TEST_SIZE\n",
        "from .experiment_utils import setup_mlflow_experiment\n",
        "from .logging import (\n",
        "    log_model_metrics,\n",
        "    log_confusion_matrix,\n",
        "    log_feature_importance,\n",
        "    log_dataset_info,\n",
        "    log_parameters\n",
        ")\n",
        "from .explainer import build_and_log_dashboard\n",
        "\n",
        "# Type aliases for complex types\n",
        "FloatArray: TypeAlias = NDArray[np.float64]\n",
        "IntArray: TypeAlias = NDArray[np.int64]\n",
        "DatasetTuple: TypeAlias = Tuple[FloatArray, FloatArray, IntArray, IntArray, List[str], List[str], StandardScaler]\n",
        "\n",
        "def load_and_prepare_iris_data(\n",
        "    test_size: float = TEST_SIZE,\n",
        "    random_state: int = RANDOM_STATE\n",
        ") -> DatasetTuple:\n",
        "    \"\"\"\n",
        "    Load and prepare the Iris dataset.\n",
        "    \n",
        "    Args:\n",
        "        test_size: Fraction of data to use for testing\n",
        "        random_state: Random state for reproducibility\n",
        "        \n",
        "    Returns:\n",
        "        Tuple of (X_train_scaled, X_test_scaled, y_train, y_test, \n",
        "                 feature_names, target_names, scaler)\n",
        "    \"\"\"\n",
        "    # Load dataset\n",
        "    iris: Any = load_iris()\n",
        "    X: NDArray[np.float64] = cast(NDArray[np.float64], iris.data)\n",
        "    y: NDArray[np.int64] = cast(NDArray[np.int64], iris.target)\n",
        "    feature_names: List[str] = list(iris.feature_names)\n",
        "    target_names: List[str] = list(iris.target_names)\n",
        "    \n",
        "    # Split data\n",
        "    X_train: NDArray[np.float64]\n",
        "    X_test: NDArray[np.float64]\n",
        "    y_train: NDArray[np.int64]\n",
        "    y_test: NDArray[np.int64]\n",
        "    X_train, X_test, y_train, y_test = cast(\n",
        "        Tuple[NDArray[np.float64], NDArray[np.float64], NDArray[np.int64], NDArray[np.int64]],\n",
        "        train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
        "    )\n",
        "    \n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled: NDArray[np.float64] = cast(NDArray[np.float64], scaler.fit_transform(X_train))\n",
        "    X_test_scaled: NDArray[np.float64] = cast(NDArray[np.float64], scaler.transform(X_test))\n",
        "    \n",
        "    return (X_train_scaled, X_test_scaled, y_train, y_test,\n",
        "            feature_names, target_names, scaler)\n",
        "\n",
        "\n",
        "# === (A) Logistic-Regression baseline =====================================\n",
        "def train_logistic_regression_autolog(\n",
        "    X_train: FloatArray,\n",
        "    y_train: IntArray,\n",
        "    X_test: FloatArray,\n",
        "    y_test: IntArray,\n",
        "    feature_names: list[str],\n",
        "    target_names: list[str],\n",
        "    run_name: str = \"lr_autolog\",\n",
        "    register: bool = True,\n",
        "    dashboard: bool = False,\n",
        "    dashboard_port: int | None = None,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Fit + evaluate a Logistic-Regression baseline.\n",
        "\n",
        "    Key improvements\n",
        "    ----------------\n",
        "    • Manually logs `accuracy` (and friends) via `log_model_metrics`\n",
        "      so downstream code can rely on the key.  \n",
        "    • Keeps the robust signature / input_example logic.\n",
        "    \"\"\"\n",
        "    setup_mlflow_experiment()\n",
        "    mlflow.sklearn.autolog(log_models=True)\n",
        "\n",
        "    with mlflow.start_run(run_name=run_name) as run:\n",
        "        model = LogisticRegression(random_state=RANDOM_STATE, max_iter=1_000).fit(\n",
        "            X_train, y_train\n",
        "        )\n",
        "\n",
        "        # ── 1) manual metric logging ──────────────────────────────────────\n",
        "        y_pred_test = model.predict(X_test)\n",
        "        log_model_metrics(y_test, y_pred_test)              # <-- new\n",
        "        log_confusion_matrix(y_test, y_pred_test, class_names=target_names)\n",
        "\n",
        "        # ── 2) model artefact with signature ──────────────────────────────\n",
        "        signature = mlflow.models.infer_signature(X_train, model.predict(X_train))\n",
        "        sklearn.log_model(\n",
        "            model,\n",
        "            artifact_path=\"model\",\n",
        "            registered_model_name=\"iris_logistic_regression\" if register else None,\n",
        "            signature=signature,\n",
        "            input_example=X_test[:5],\n",
        "        )\n",
        "\n",
        "        mlflow.evaluate(\n",
        "            model=f\"runs:/{run.info.run_id}/model\",\n",
        "            data=X_test,\n",
        "            targets=y_test,\n",
        "            model_type=\"classifier\",\n",
        "            evaluator_config={\"label_list\": list(range(len(target_names)))},\n",
        "        )\n",
        "\n",
        "        # ── 3) optional dashboard ─────────────────────────────────────────\n",
        "        if dashboard:\n",
        "            X_test_df = pd.DataFrame(X_test, columns=feature_names)\n",
        "            build_and_log_dashboard(\n",
        "                model, X_test_df, y_test, labels=target_names,\n",
        "                run=run, port=dashboard_port, serve=True\n",
        "            )\n",
        "        return run.info.run_id\n",
        "\n",
        "\n",
        "\n",
        "def create_rf_objective(\n",
        "    X_train: NDArray[np.float64],\n",
        "    y_train: NDArray[np.int64],\n",
        "    X_valid: NDArray[np.float64],\n",
        "    y_valid: NDArray[np.int64],\n",
        ") -> Callable[[optuna.trial.Trial], float]:\n",
        "    \"\"\"\n",
        "    Optuna objective that returns validation accuracy – *no* MLflow calls here.\n",
        "    All logging is delegated to Optuna's MLflowCallback.\n",
        "    \"\"\"\n",
        "    def objective(trial: optuna.trial.Trial) -> float:\n",
        "        params: Dict[str, Any] = {\n",
        "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 10, 200),\n",
        "            \"max_depth\": trial.suggest_int(\"max_depth\", 2, 20),\n",
        "            \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 20),\n",
        "            \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 10),\n",
        "            \"random_state\": RANDOM_STATE,\n",
        "        }\n",
        "        clf = RandomForestClassifier(**params)\n",
        "        clf.fit(X_train, y_train)\n",
        "        preds = clf.predict(X_valid)\n",
        "        return accuracy_score(y_valid, preds)\n",
        "\n",
        "    return objective\n",
        "\n",
        "\n",
        "# === (B) Random-Forest with Optuna ========================================\n",
        "def train_random_forest_with_optimization(\n",
        "    X_train: FloatArray,\n",
        "    y_train: IntArray,\n",
        "    X_test: FloatArray,\n",
        "    y_test: IntArray,\n",
        "    feature_names: list[str],\n",
        "    target_names: list[str],\n",
        "    *,\n",
        "    n_trials: int = 50,\n",
        "    run_name: str = \"rf_optimization\",\n",
        "    register: bool = True,\n",
        "    dashboard: bool = False,\n",
        "    dashboard_port: int | None = None,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Optuna hyper-parameter search + robust metric logging (now includes `accuracy`).\n",
        "    \"\"\"\n",
        "    setup_mlflow_experiment()\n",
        "    disable_flag = mlflow.sklearn.autolog(disable=True)\n",
        "\n",
        "    try:\n",
        "        with mlflow.start_run(run_name=run_name) as parent:\n",
        "            log_dataset_info(X_train, X_test, y_train, y_test)\n",
        "\n",
        "            study = optuna.create_study(direction=\"maximize\")\n",
        "            study.optimize(\n",
        "                create_rf_objective(X_train, y_train, X_test, y_test),\n",
        "                n_trials=n_trials,\n",
        "                callbacks=[\n",
        "                    MLflowCallback(\n",
        "                        tracking_uri=mlflow.get_tracking_uri(),\n",
        "                        metric_name=\"accuracy\",\n",
        "                        mlflow_kwargs={\"nested\": True},\n",
        "                    )\n",
        "                ],\n",
        "            )\n",
        "\n",
        "            best_model = RandomForestClassifier(**study.best_params).fit(X_train, y_train)\n",
        "\n",
        "            # ── 1) manual metric logging ──────────────────────────────────\n",
        "            y_pred_test = best_model.predict(X_test)\n",
        "            log_model_metrics(y_test, y_pred_test)           # <-- new\n",
        "            log_confusion_matrix(y_test, y_pred_test, class_names=target_names)\n",
        "            log_feature_importance(feature_names, best_model.feature_importances_)\n",
        "\n",
        "            mlflow.log_metric(\"best_accuracy\", study.best_value)\n",
        "\n",
        "            # ── 2) model artefact with signature ──────────────────────────\n",
        "            signature = mlflow.models.infer_signature(X_train, best_model.predict(X_train))\n",
        "            sklearn.log_model(\n",
        "                best_model,\n",
        "                artifact_path=\"model\",\n",
        "                registered_model_name=\"iris_random_forest\" if register else None,\n",
        "                signature=signature,\n",
        "                input_example=X_test[:5],\n",
        "            )\n",
        "\n",
        "            mlflow.evaluate(\n",
        "                model=f\"runs:/{parent.info.run_id}/model\",\n",
        "                data=X_test,\n",
        "                targets=y_test,\n",
        "                model_type=\"classifier\",\n",
        "                evaluator_config={\"label_list\": list(range(len(target_names)))},\n",
        "            )\n",
        "\n",
        "            if dashboard:\n",
        "                X_test_df = pd.DataFrame(X_test, columns=feature_names)\n",
        "                build_and_log_dashboard(\n",
        "                    best_model, X_test_df, y_test, labels=target_names,\n",
        "                    run=parent, port=dashboard_port, serve=True\n",
        "                )\n",
        "            return parent.info.run_id\n",
        "    finally:\n",
        "        if not disable_flag:\n",
        "            mlflow.sklearn.autolog(disable=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# === (C) Robust comparator ===============================================\n",
        "def compare_models(\n",
        "    experiment_name: Optional[str] = None,\n",
        "    metric_key: str = \"accuracy\",\n",
        "    maximize: bool = True,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Print the best run according to *metric_key* while gracefully\n",
        "    falling-back to common alternates when the preferred key is missing.\n",
        "    \"\"\"\n",
        "    from .experiment_utils import get_best_run\n",
        "\n",
        "    fallback_keys = [\"accuracy_score\", \"best_accuracy\"]\n",
        "    try:\n",
        "        best = get_best_run(experiment_name, metric_key, maximize)\n",
        "        rid = best[\"run_id\"]\n",
        "\n",
        "        # choose first key that exists\n",
        "        score = best.get(f\"metrics.{metric_key}\")\n",
        "        if score is None:\n",
        "            for alt in fallback_keys:\n",
        "                score = best.get(f\"metrics.{alt}\")\n",
        "                if score is not None:\n",
        "                    metric_key = alt\n",
        "                    break\n",
        "\n",
        "        model_type = best.get(\"params.model_type\", \"unknown\")\n",
        "        print(f\"🏆 Best run: {rid}\")\n",
        "        print(f\"📈 {metric_key}: {score if score is not None else 'N/A'}\")\n",
        "        print(f\"🔖 Model type: {model_type}\")\n",
        "    except Exception as err:\n",
        "        print(f\"❌ Error comparing models: {err}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/mlops/explainer.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/mlops/explainer.py\n",
        "from __future__ import annotations\n",
        "import os\n",
        "import socket\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from typing import Any, Sequence, Optional\n",
        "from contextlib import closing\n",
        "\n",
        "import mlflow\n",
        "import psutil  # lightweight; already added to pyproject deps\n",
        "from sklearn.utils.multiclass import type_of_target\n",
        "from explainerdashboard import (\n",
        "    ClassifierExplainer,\n",
        "    RegressionExplainer,\n",
        "    ExplainerDashboard,\n",
        ")\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "__all__ = [\"build_and_log_dashboard\", \"load_dashboard_yaml\", \"_first_free_port\", \"_port_details\"]\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "def _port_details(port: int) -> str:\n",
        "    \"\"\"\n",
        "    Return a one-line string with PID & cmdline of the process\n",
        "    listening on *port*, or '' if none / not discoverable.\n",
        "    \"\"\"\n",
        "    for c in psutil.net_connections(kind=\"tcp\"):\n",
        "        if c.status == psutil.CONN_LISTEN and c.laddr and c.laddr.port == port:\n",
        "            try:\n",
        "                p = psutil.Process(c.pid)\n",
        "                return f\"[PID {p.pid} – {p.name()}] cmd={p.cmdline()}\"\n",
        "            except psutil.Error:\n",
        "                return f\"[PID {c.pid}] (no detail)\"\n",
        "    return \"\"\n",
        "\n",
        "def _first_free_port(start: int = 8050, tries: int = 50) -> int:\n",
        "    \"\"\"Return first free TCP port ≥ *start* on localhost.\"\"\"\n",
        "    for port in range(start, start + tries):\n",
        "        try:\n",
        "            with closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as s:\n",
        "                s.settimeout(0.05)\n",
        "                s.bind((\"127.0.0.1\", port))\n",
        "                return port\n",
        "        except OSError:\n",
        "            # Port is in use, try next one\n",
        "            continue\n",
        "    raise RuntimeError(\"⚠️  No free ports found in range\")\n",
        "\n",
        "def _next_free_port(start: int = 8050, tries: int = 50) -> int:\n",
        "    \"\"\"Return the first free TCP port ≥ *start*. (Alias for backward compatibility)\"\"\"\n",
        "    return _first_free_port(start, tries)\n",
        "\n",
        "def _port_in_use(port: int) -> bool:\n",
        "    \"\"\"Check if a port is already in use on any interface.\"\"\"\n",
        "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "        s.settimeout(0.05)\n",
        "        # Check both localhost and 0.0.0.0 to be thorough\n",
        "        try:\n",
        "            # First check localhost (127.0.0.1)\n",
        "            if s.connect_ex((\"127.0.0.1\", port)) == 0:\n",
        "                return True\n",
        "            # Also check if anything is bound to all interfaces\n",
        "            if s.connect_ex((\"0.0.0.0\", port)) == 0:\n",
        "                return True\n",
        "        except (socket.gaierror, OSError):\n",
        "            # If we can't connect, assume port is free\n",
        "            pass\n",
        "        return False\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "def build_and_log_dashboard(\n",
        "    model: Any,\n",
        "    X_test,                        # 2-D ndarray / DataFrame\n",
        "    y_test,                        # 1-D labels or targets\n",
        "    *,\n",
        "    # ---- new kwargs mirrored from ExplainerDashboard ----------------------\n",
        "    cats: Optional[Sequence[str]] = None,\n",
        "    idxs: Optional[Sequence[Any]] = None,\n",
        "    descriptions: Optional[dict[str, str]] = None,\n",
        "    target: Optional[str] = None,\n",
        "    labels: Optional[Sequence[str]] = None,\n",
        "    X_background=None,\n",
        "    model_output: str = \"probability\",\n",
        "    shap: str = \"guess\",\n",
        "    shap_interaction: bool = True,\n",
        "    simple: bool = False,\n",
        "    mode: str = \"external\",        # inline · jupyterlab · external\n",
        "    title: str = \"Model Explainer\",\n",
        "    # ---- infra ------------------------------------------------------------\n",
        "    run: mlflow.ActiveRun | None = None,\n",
        "    port: int | None = None,\n",
        "    serve: bool = False,\n",
        "    save_yaml: bool = True,\n",
        "    output_dir: os.PathLike | str | None = None,\n",
        ") -> Path:\n",
        "    \"\"\"\n",
        "    Build + log ExplainerDashboard, with improved port handling:\n",
        "\n",
        "    • If *port* is None we auto-select the first free port ≥ 8050  \n",
        "    • If *port* is occupied we print owner details & **abort** (caller decides)  \n",
        "      -- avoids silent failure that confused you earlier.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Path to the saved ``dashboard.yaml``    (or the HTML file if save_yaml=False)\n",
        "    \"\"\"\n",
        "    # 1️⃣ Pick correct explainer ------------------------------------------------\n",
        "    problem = type_of_target(y_test)\n",
        "    if problem in {\"continuous\", \"continuous-multioutput\"}:\n",
        "        ExplainerCls = RegressionExplainer\n",
        "        # RegressionExplainer doesn't support 'labels' or 'model_output' parameters\n",
        "        explainer_kwargs = {\n",
        "            \"cats\": cats,\n",
        "            \"idxs\": idxs,\n",
        "            \"descriptions\": descriptions,\n",
        "            \"target\": target,\n",
        "            \"X_background\": X_background,\n",
        "            \"shap\": shap,\n",
        "        }\n",
        "    else:\n",
        "        ExplainerCls = ClassifierExplainer\n",
        "        # ClassifierExplainer supports all parameters\n",
        "        explainer_kwargs = {\n",
        "            \"cats\": cats,\n",
        "            \"idxs\": idxs,\n",
        "            \"descriptions\": descriptions,\n",
        "            \"target\": target,\n",
        "            \"labels\": labels,\n",
        "            \"X_background\": X_background,\n",
        "            \"model_output\": model_output,\n",
        "            \"shap\": shap,\n",
        "        }\n",
        "\n",
        "    # Filter out None values to avoid issues\n",
        "    explainer_kwargs = {k: v for k, v in explainer_kwargs.items() if v is not None}\n",
        "    \n",
        "    explainer = ExplainerCls(\n",
        "        model,\n",
        "        X_test,\n",
        "        y_test,\n",
        "        **explainer_kwargs\n",
        "    )\n",
        "\n",
        "    dash = ExplainerDashboard(\n",
        "        explainer,\n",
        "        title=title,\n",
        "        shap_interaction=shap_interaction,\n",
        "        simple=simple,\n",
        "        mode=mode,\n",
        "    )\n",
        "\n",
        "    # 2️⃣ Persist + log artefacts ----------------------------------------------\n",
        "    out_dir = Path(output_dir or \".\")\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    html_path = out_dir / \"explainer_dashboard.html\"\n",
        "    dash.save_html(html_path)\n",
        "    mlflow.log_artifact(str(html_path))\n",
        "\n",
        "    yaml_path: Path | None = None\n",
        "    if save_yaml:\n",
        "        yaml_path = out_dir / \"dashboard.yaml\"\n",
        "        dash.to_yaml(yaml_path)\n",
        "        mlflow.log_artifact(str(yaml_path))\n",
        "\n",
        "    # 3️⃣ Optional serving ----------------------------------------------------\n",
        "    if serve:\n",
        "        chosen = port or _first_free_port()\n",
        "        if _port_in_use(chosen):\n",
        "            details = _port_details(chosen)\n",
        "            raise RuntimeError(\n",
        "                f\"❌ Port {chosen} already in use {details}. \"\n",
        "                \"Either pass a different --port or stop the process.\"\n",
        "            )\n",
        "        logging.info(\"🌐 Serving dashboard on http://0.0.0.0:%s\", chosen)\n",
        "        dash.run(chosen, host=\"0.0.0.0\", use_waitress=True, open_browser=False)\n",
        "\n",
        "    return yaml_path or html_path\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "def load_dashboard_yaml(path: os.PathLike | str) -> ExplainerDashboard:\n",
        "    \"\"\"Reload a YAML config – unchanged but kept for public API.\"\"\"\n",
        "    return ExplainerDashboard.from_config(path) \n",
        "\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/mlops/utils.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/mlops/utils.py\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "_added_src_flag: bool = False          # module-level cache\n",
        "\n",
        "def project_root() -> Path:\n",
        "    \"\"\"\n",
        "    Return the absolute path to the repo root *without* relying on __file__.\n",
        "\n",
        "    • If running from a .py file, use that file's parent/parent (…/src/..)\n",
        "    • If running interactively (no __file__), fall back to CWD.\n",
        "    \"\"\"\n",
        "    if \"__file__\" in globals():\n",
        "        return Path(__file__).resolve().parent.parent\n",
        "    return Path.cwd()\n",
        "\n",
        "def ensure_src_on_path(verbose: bool = True) -> None:\n",
        "    \"\"\"\n",
        "    Ensure <repo-root>/src is the *first* entry in sys.path exactly once.\n",
        "    The verbose flag prints the helper line the first time only.\n",
        "    \"\"\"\n",
        "    import sys\n",
        "    global _added_src_flag\n",
        "    root = project_root()\n",
        "    src_path = root / \"src\"\n",
        "\n",
        "    if str(src_path) not in sys.path:\n",
        "        sys.path.insert(0, str(src_path))\n",
        "        if verbose and not _added_src_flag:\n",
        "            print(f\"🔧 Added {src_path} to sys.path\")\n",
        "        _added_src_flag = True\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/mlops/experiment_utils.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/mlops/experiment_utils.py\n",
        "\"\"\"MLflow experiment utilities.\"\"\"\n",
        "import os\n",
        "import pathlib\n",
        "import mlflow\n",
        "import mlflow.tracking\n",
        "from typing import Optional, Dict, Any\n",
        "import requests\n",
        "\n",
        "from .config import EXPERIMENT_NAME, TRACKING_URI\n",
        "\n",
        "_HEALTH_ENDPOINTS = (\"/health\", \"/version\")\n",
        "\n",
        "\n",
        "def _ping_tracking_server(uri: str, timeout: float = 2.0) -> bool:\n",
        "    \"\"\"Return True iff an HTTP MLflow server is reachable at *uri*.\"\"\"\n",
        "    if not uri.startswith(\"http\"):\n",
        "        return False                        # file store – nothing to ping\n",
        "    try:\n",
        "        # Use new health endpoints\n",
        "        for ep in _HEALTH_ENDPOINTS:\n",
        "            response = requests.get(uri.rstrip(\"/\") + ep, timeout=timeout)\n",
        "            response.raise_for_status()\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "\n",
        "def _fallback_uri() -> str:\n",
        "    \"\"\"Return a local file-based URI relative to the repo root.\"\"\"\n",
        "    root = pathlib.Path.cwd()\n",
        "    return f\"file:{root}/mlruns\"\n",
        "\n",
        "\n",
        "def setup_mlflow_experiment(experiment_name: Optional[str] = None) -> None:\n",
        "    \"\"\"\n",
        "    Idempotently configure MLflow **and** guarantee the experiment exists.\n",
        "    • If TRACKING_URI points at an HTTP server that is unreachable, we fall\n",
        "      back to a local file store so the script never crashes.\n",
        "    \n",
        "    Args:\n",
        "        experiment_name: Name of the experiment. If None, uses default.\n",
        "    \"\"\"\n",
        "    # Use provided name or fall back to config default\n",
        "    exp_name = experiment_name or EXPERIMENT_NAME\n",
        "    \n",
        "    uri = TRACKING_URI\n",
        "    if not _ping_tracking_server(uri):\n",
        "        local_uri = _fallback_uri()\n",
        "        print(\n",
        "            f\"⚠️  MLflow server unreachable at {uri} – \"\n",
        "            f\"falling back to local store {local_uri}\"\n",
        "        )\n",
        "        uri = local_uri\n",
        "\n",
        "    mlflow.set_tracking_uri(uri)\n",
        "\n",
        "    if mlflow.get_experiment_by_name(exp_name) is None:\n",
        "        mlflow.create_experiment(\n",
        "            exp_name,\n",
        "            artifact_location=os.getenv(\"MLFLOW_ARTIFACT_ROOT\",\n",
        "                                        _fallback_uri())\n",
        "        )\n",
        "    mlflow.set_experiment(exp_name)\n",
        "    print(f\"🗂  Using MLflow experiment '{exp_name}' @ {uri}\")\n",
        "\n",
        "\n",
        "def get_best_run(\n",
        "    experiment_name: Optional[str] = None,\n",
        "    metric_key: str = \"accuracy\",\n",
        "    maximize: bool = True,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Return a *shallow* dict with run_id, metrics.*, and params.* keys\n",
        "    so downstream code can use predictable dotted paths.\n",
        "    \"\"\"\n",
        "    exp_name = experiment_name or EXPERIMENT_NAME\n",
        "    setup_mlflow_experiment(exp_name)\n",
        "\n",
        "    client = mlflow.tracking.MlflowClient()\n",
        "    exp = mlflow.get_experiment_by_name(exp_name)\n",
        "    if exp is None:\n",
        "        raise ValueError(f\"Experiment '{exp_name}' not found\")\n",
        "\n",
        "    order = \"DESC\" if maximize else \"ASC\"\n",
        "    run = client.search_runs(\n",
        "        [exp.experiment_id],\n",
        "        order_by=[f\"metrics.{metric_key} {order}\"],\n",
        "        max_results=1,\n",
        "    )[0]\n",
        "\n",
        "    # Build a *flat* mapping -------------------------------------------------\n",
        "    flat: Dict[str, Any] = {\"run_id\": run.info.run_id}\n",
        "\n",
        "    # Metrics\n",
        "    for k, v in run.data.metrics.items():\n",
        "        flat[f\"metrics.{k}\"] = v\n",
        "\n",
        "    # Params\n",
        "    for k, v in run.data.params.items():\n",
        "        flat[f\"params.{k}\"] = v\n",
        "\n",
        "    # Tags (optional but handy)\n",
        "    for k, v in run.data.tags.items():\n",
        "        flat[f\"tags.{k}\"] = v\n",
        "\n",
        "    return flat\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔧 Added /workspace/src/src to sys.path\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🌸 Iris Classification with MLflow\n",
            "==================================================\n",
            "✓ Training samples: 120 | Test: 30\n",
            "🗂  Using MLflow experiment 'iris_classification' @ http://mlflow:5000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/06/25 18:15:36 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n",
            "2025/06/25 18:15:36 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/06/25 18:15:38 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n",
            "Registered model 'iris_logistic_regression' already exists. Creating a new version of this model...\n",
            "2025/06/25 18:15:39 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: iris_logistic_regression, version 2\n",
            "Created version '2' of model 'iris_logistic_regression'.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "621cdc1c7efc426ca3f5ddaab70e09f5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ab54f52e03e1412084201fd344642f6d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/06/25 18:15:39 WARNING mlflow.models.evaluation.evaluators.classifier: According to the evaluation dataset label values, the model type looks like None, but you specified model type 'classifier'. Please verify that you set the `model_type` and `dataset` arguments correctly.\n",
            "2025/06/25 18:15:39 INFO mlflow.models.evaluation.evaluators.classifier: The evaluation dataset is inferred as multiclass dataset, number of classes is inferred as 3. If this is incorrect, please specify the `label_list` parameter in `evaluator_config`.\n",
            "2025/06/25 18:15:39 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n",
            "2025/06/25 18:15:40 INFO mlflow.models.evaluation.evaluators.shap: Shap explainer ExactExplainer is used.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🏃 View run lr_baseline at: http://mlflow:5000/#/experiments/1/runs/643f66f3664946d5b9b94fee5c29679c\n",
            "🧪 View experiment at: http://mlflow:5000/#/experiments/1\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[9], line 87\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m🚀 ExplainerDashboard running on http://localhost:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mport\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 87\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdashboard\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_bool_env\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEXPLAINER_DASHBOARD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdashboard_port\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEXPLAINER_PORT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m8050\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[9], line 44\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(dashboard, dashboard_port)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✓ Training samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(X_train)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Test: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(X_test)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# 2 Logistic Regression --------------------------------------------------\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m lr_run \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_logistic_regression_autolog\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeat_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtgt_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr_baseline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregister\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdashboard\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdashboard\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdashboard_port\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdashboard_port\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✓ Logistic run \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr_run[:\u001b[38;5;241m8\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# 3 Random Forest + Optuna ----------------------------------------------\u001b[39;00m\n",
            "File \u001b[0;32m/workspace/src/mlops/training.py:121\u001b[0m, in \u001b[0;36mtrain_logistic_regression_autolog\u001b[0;34m(X_train, y_train, X_test, y_test, feature_names, target_names, run_name, register, dashboard, dashboard_port)\u001b[0m\n\u001b[1;32m    112\u001b[0m signature \u001b[38;5;241m=\u001b[39m mlflow\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39minfer_signature(X_train, model\u001b[38;5;241m.\u001b[39mpredict(X_train))\n\u001b[1;32m    113\u001b[0m sklearn\u001b[38;5;241m.\u001b[39mlog_model(\n\u001b[1;32m    114\u001b[0m     model,\n\u001b[1;32m    115\u001b[0m     artifact_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    118\u001b[0m     input_example\u001b[38;5;241m=\u001b[39mX_test[:\u001b[38;5;241m5\u001b[39m],\n\u001b[1;32m    119\u001b[0m )\n\u001b[0;32m--> 121\u001b[0m \u001b[43mmlflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mruns:/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mrun\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclassifier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluator_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel_list\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtarget_names\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# ── 3) optional dashboard ─────────────────────────────────────────\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dashboard:\n",
            "File \u001b[0;32m/app/.venv/lib/python3.10/site-packages/mlflow/models/evaluation/deprecated.py:23\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_databricks_uri(tracking_uri):\n\u001b[1;32m     13\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `mlflow.evaluate` API has been deprecated as of MLflow 3.0.0. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease use these new alternatives:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m     22\u001b[0m     )\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/app/.venv/lib/python3.10/site-packages/mlflow/models/evaluation/base.py:1790\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, data, model_type, targets, predictions, dataset_path, feature_names, evaluators, evaluator_config, extra_metrics, custom_artifacts, env_manager, model_config, inference_params, model_id, _called_from_genai_evaluate)\u001b[0m\n\u001b[1;32m   1787\u001b[0m predictions_expected_in_model_output \u001b[38;5;241m=\u001b[39m predictions \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1790\u001b[0m     evaluate_result \u001b[38;5;241m=\u001b[39m \u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1791\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1792\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1794\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1795\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1796\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevaluator_name_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluator_name_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1797\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevaluator_name_to_conf_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluator_name_to_conf_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1798\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1799\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_artifacts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_artifacts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1800\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredictions_expected_in_model_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1801\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevaluators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1802\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1803\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1804\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, _ServedPyFuncModel):\n",
            "File \u001b[0;32m/app/.venv/lib/python3.10/site-packages/mlflow/models/evaluation/base.py:1031\u001b[0m, in \u001b[0;36m_evaluate\u001b[0;34m(model, model_type, model_id, dataset, run_id, evaluator_name_list, evaluator_name_to_conf_map, extra_metrics, custom_artifacts, predictions, evaluators)\u001b[0m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eval_\u001b[38;5;241m.\u001b[39mevaluator\u001b[38;5;241m.\u001b[39mcan_evaluate(model_type\u001b[38;5;241m=\u001b[39mmodel_type, evaluator_config\u001b[38;5;241m=\u001b[39meval_\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[1;32m   1030\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m configure_autologging_for_evaluation(enable_tracing\u001b[38;5;241m=\u001b[39mshould_enable_tracing):\n\u001b[0;32m-> 1031\u001b[0m         eval_result \u001b[38;5;241m=\u001b[39m \u001b[43meval_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1036\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[43m            \u001b[49m\u001b[43mevaluator_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1038\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcustom_artifacts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_artifacts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1043\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m eval_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1044\u001b[0m         eval_results\u001b[38;5;241m.\u001b[39mappend(eval_result)\n",
            "File \u001b[0;32m/app/.venv/lib/python3.10/site-packages/mlflow/models/evaluation/default_evaluator.py:947\u001b[0m, in \u001b[0;36mBuiltInEvaluator.evaluate\u001b[0;34m(self, model_type, dataset, run_id, evaluator_config, model, extra_metrics, custom_artifacts, predictions, model_id, **kwargs)\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m TempDir() \u001b[38;5;28;01mas\u001b[39;00m temp_dir, matplotlib\u001b[38;5;241m.\u001b[39mrc_context(_matplotlib_config):\n\u001b[1;32m    946\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemp_dir \u001b[38;5;241m=\u001b[39m temp_dir\n\u001b[0;32m--> 947\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_metrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_artifacts\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/app/.venv/lib/python3.10/site-packages/mlflow/models/evaluation/evaluators/shap.py:250\u001b[0m, in \u001b[0;36mShapEvaluator._evaluate\u001b[0;34m(self, model, extra_metrics, custom_artifacts, **kwargs)\u001b[0m\n\u001b[1;32m    247\u001b[0m     _adjust_axis_tick()\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _suppress_class_imbalance_errors(\u001b[38;5;167;01mValueError\u001b[39;00m, log_warning\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_image_artifact\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mplot_beeswarm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshap_beeswarm_plot\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mplot_summary\u001b[39m():\n\u001b[1;32m    256\u001b[0m     shap\u001b[38;5;241m.\u001b[39msummary_plot(shap_values, show\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, color_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "File \u001b[0;32m/app/.venv/lib/python3.10/site-packages/mlflow/models/evaluation/default_evaluator.py:334\u001b[0m, in \u001b[0;36mBuiltInEvaluator._log_image_artifact\u001b[0;34m(self, do_plot, artifact_name)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m     pyplot\u001b[38;5;241m.\u001b[39mclf()\n\u001b[0;32m--> 334\u001b[0m     \u001b[43mdo_plot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m     pyplot\u001b[38;5;241m.\u001b[39msavefig(artifact_file_local_path, bbox_inches\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtight\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[0;32m/app/.venv/lib/python3.10/site-packages/mlflow/models/evaluation/evaluators/shap.py:245\u001b[0m, in \u001b[0;36mShapEvaluator._evaluate.<locals>.plot_beeswarm\u001b[0;34m()\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mplot_beeswarm\u001b[39m():\n\u001b[0;32m--> 245\u001b[0m     \u001b[43mshap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplots\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeeswarm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshap_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m     _adjust_color_bar()\n\u001b[1;32m    247\u001b[0m     _adjust_axis_tick()\n",
            "File \u001b[0;32m/app/.venv/lib/python3.10/site-packages/shap/plots/_beeswarm.py:448\u001b[0m, in \u001b[0;36mbeeswarm\u001b[0;34m(shap_values, max_display, order, clustering, cluster_threshold, color, axis_color, alpha, ax, show, log_scale, color_bar, s, plot_size, color_bar_label, group_remaining_features)\u001b[0m\n\u001b[1;32m    446\u001b[0m     cvals[cvals_imp \u001b[38;5;241m>\u001b[39m vmax] \u001b[38;5;241m=\u001b[39m vmax\n\u001b[1;32m    447\u001b[0m     cvals[cvals_imp \u001b[38;5;241m<\u001b[39m vmin] \u001b[38;5;241m=\u001b[39m vmin\n\u001b[0;32m--> 448\u001b[0m     \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshaps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnan_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mys\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnan_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcvals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlinewidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzorder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrasterized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshaps\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m safe_isinstance(color, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatplotlib.colors.Colormap\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(color, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolors\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[0;32m/app/.venv/lib/python3.10/site-packages/matplotlib/_api/deprecation.py:453\u001b[0m, in \u001b[0;36mmake_keyword_only.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m name_idx:\n\u001b[1;32m    448\u001b[0m     warn_deprecated(\n\u001b[1;32m    449\u001b[0m         since, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing the \u001b[39m\u001b[38;5;132;01m%(name)s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%(obj_type)s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    450\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositionally is deprecated since Matplotlib \u001b[39m\u001b[38;5;132;01m%(since)s\u001b[39;00m\u001b[38;5;124m; the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    451\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter will become keyword-only in \u001b[39m\u001b[38;5;132;01m%(removal)s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    452\u001b[0m         name\u001b[38;5;241m=\u001b[39mname, obj_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/app/.venv/lib/python3.10/site-packages/matplotlib/__init__.py:1521\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m   1519\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1520\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1521\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[43m            \u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1523\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcbook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1524\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1526\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1527\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[1;32m   1528\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
            "File \u001b[0;32m/app/.venv/lib/python3.10/site-packages/matplotlib/axes/_axes.py:5065\u001b[0m, in \u001b[0;36mAxes.scatter\u001b[0;34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, edgecolors, colorizer, plotnonfinite, **kwargs)\u001b[0m\n\u001b[1;32m   5062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ymargin \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.05\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m x\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   5063\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_ymargin(\u001b[38;5;241m0.05\u001b[39m)\n\u001b[0;32m-> 5065\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcollection\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5066\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request_autoscale_view()\n\u001b[1;32m   5068\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m collection\n",
            "File \u001b[0;32m/app/.venv/lib/python3.10/site-packages/matplotlib/axes/_base.py:2369\u001b[0m, in \u001b[0;36m_AxesBase.add_collection\u001b[0;34m(self, collection, autolim)\u001b[0m\n\u001b[1;32m   2367\u001b[0m datalim \u001b[38;5;241m=\u001b[39m collection\u001b[38;5;241m.\u001b[39mget_datalim(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransData)\n\u001b[1;32m   2368\u001b[0m points \u001b[38;5;241m=\u001b[39m datalim\u001b[38;5;241m.\u001b[39mget_points()\n\u001b[0;32m-> 2369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatalim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminpos\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   2370\u001b[0m     \u001b[38;5;66;03m# By definition, if minpos (minimum positive value) is set\u001b[39;00m\n\u001b[1;32m   2371\u001b[0m     \u001b[38;5;66;03m# (i.e., non-inf), then min(points) <= minpos <= max(points),\u001b[39;00m\n\u001b[1;32m   2372\u001b[0m     \u001b[38;5;66;03m# and minpos would be superfluous. However, we add minpos to\u001b[39;00m\n\u001b[1;32m   2373\u001b[0m     \u001b[38;5;66;03m# the call so that self.dataLim will update its own minpos.\u001b[39;00m\n\u001b[1;32m   2374\u001b[0m     \u001b[38;5;66;03m# This ensures that log scales see the correct minimum.\u001b[39;00m\n\u001b[1;32m   2375\u001b[0m     points \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([points, [datalim\u001b[38;5;241m.\u001b[39mminpos]])\n\u001b[1;32m   2376\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_datalim(points)\n",
            "File \u001b[0;32m/app/.venv/lib/python3.10/site-packages/numpy/core/_methods.py:64\u001b[0m, in \u001b[0;36m_all\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_all\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# Parsing keyword arguments is currently fairly slow, so avoid it for now\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m where \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mumr_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m umr_all(a, axis, dtype, out, keepdims, where\u001b[38;5;241m=\u001b[39mwhere)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# %%writefile src/examples/iris_classification_example.py\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Iris Classification Example (argparse-free, notebook-safe).\n",
        "\n",
        "Configuration:\n",
        "    • export EXPLAINER_DASHBOARD=1   # launch dashboard\n",
        "    • export EXPLAINER_PORT=8150     # optional port override\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "import os, logging\n",
        "\n",
        "from src.mlops.utils import ensure_src_on_path\n",
        "ensure_src_on_path()\n",
        "\n",
        "from src.mlops.training import (\n",
        "    load_and_prepare_iris_data,\n",
        "    train_logistic_regression_autolog,\n",
        "    train_random_forest_with_optimization,\n",
        "    compare_models,\n",
        ")\n",
        "from src.mlops.model_registry import load_model_from_run\n",
        "from src.mlops.experiment_utils import get_best_run\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "\n",
        "def _bool_env(var: str, default: bool = False) -> bool:\n",
        "    v = os.getenv(var)\n",
        "    return default if v is None else v.lower() in {\"1\", \"true\", \"yes\"}\n",
        "\n",
        "\n",
        "def main(*, dashboard: bool = False, dashboard_port: int | None = None) -> None:\n",
        "    print(\"🌸 Iris Classification with MLflow\\n\" + \"=\" * 50)\n",
        "\n",
        "    # 1 Load data ------------------------------------------------------------\n",
        "    X_train, X_test, y_train, y_test, feat_names, tgt_names, _ = (\n",
        "        load_and_prepare_iris_data()\n",
        "    )\n",
        "    print(f\"✓ Training samples: {len(X_train)} | Test: {len(X_test)}\")\n",
        "\n",
        "    # 2 Logistic Regression --------------------------------------------------\n",
        "    lr_run = train_logistic_regression_autolog(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        X_test,\n",
        "        y_test,\n",
        "        feat_names,\n",
        "        tgt_names,\n",
        "        run_name=\"lr_baseline\",\n",
        "        register=True,\n",
        "        dashboard=dashboard,\n",
        "        dashboard_port=dashboard_port,\n",
        "    )\n",
        "    print(f\"✓ Logistic run {lr_run[:8]}\")\n",
        "\n",
        "    # 3 Random Forest + Optuna ----------------------------------------------\n",
        "    rf_run = train_random_forest_with_optimization(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        X_test,\n",
        "        y_test,\n",
        "        feat_names,\n",
        "        tgt_names,\n",
        "        n_trials=20,\n",
        "        run_name=\"rf_optimized\",\n",
        "        register=True,\n",
        "        dashboard=dashboard,\n",
        "        dashboard_port=dashboard_port,\n",
        "    )\n",
        "    print(f\"✓ RF run {rf_run[:8]}\")\n",
        "\n",
        "    # 4 Compare & test best --------------------------------------------------\n",
        "    compare_models()\n",
        "    best = get_best_run()\n",
        "    mdl = load_model_from_run(best[\"run_id\"])\n",
        "    acc = (mdl.predict(X_test) == y_test).mean()\n",
        "    print(f\"🏆 Best model accuracy: {acc:.4f}\")\n",
        "\n",
        "    if dashboard:\n",
        "        port = dashboard_port or int(os.getenv(\"EXPLAINER_PORT\", \"8050\"))\n",
        "        print(f\"\\n🚀 ExplainerDashboard running on http://localhost:{port}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main(\n",
        "        dashboard=_bool_env(\"EXPLAINER_DASHBOARD\", True),\n",
        "        dashboard_port=int(os.getenv(\"EXPLAINER_PORT\", \"8050\")),\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting tests/test_mlflow_integration.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile tests/test_mlflow_integration.py\n",
        "\n",
        "\"\"\"Tests for MLflow integration modules.\"\"\"\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add src to path\n",
        "sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))\n",
        "\n",
        "from mlops.experiment import setup_mlflow_experiment\n",
        "from mlops.training import (\n",
        "    load_and_prepare_iris_data, \n",
        "    train_logistic_regression\n",
        ")\n",
        "from mlops.model_registry import load_model_from_run\n",
        "\n",
        "\n",
        "def test_data_loading():\n",
        "    \"\"\"Test that data loading works correctly.\"\"\"\n",
        "    data = load_and_prepare_iris_data()\n",
        "    X_train, X_test, y_train, y_test, feature_names, target_names, scaler = data\n",
        "    \n",
        "    assert len(X_train) > 0\n",
        "    assert len(X_test) > 0\n",
        "    assert len(feature_names) == 4\n",
        "    assert len(target_names) == 3\n",
        "    assert X_train.shape[1] == 4  # 4 features\n",
        "\n",
        "\n",
        "def test_experiment_setup():\n",
        "    \"\"\"Test that MLflow experiment setup works.\"\"\"\n",
        "    # This should not raise an exception\n",
        "    setup_mlflow_experiment(\"test_experiment\")\n",
        "    \n",
        "\n",
        "def test_model_training_and_loading():\n",
        "    \"\"\"Test end-to-end model training and loading.\"\"\"\n",
        "    # Load data\n",
        "    data = load_and_prepare_iris_data()\n",
        "    X_train, X_test, y_train, y_test, feature_names, target_names, scaler = data\n",
        "    \n",
        "    # Train a simple model\n",
        "    run_id = train_logistic_regression(\n",
        "        X_train, y_train, X_test, y_test,\n",
        "        feature_names, target_names,\n",
        "        run_name=\"test_lr\",\n",
        "        register=False  # Don't register for tests\n",
        "    )\n",
        "    \n",
        "    assert run_id is not None\n",
        "    assert len(run_id) > 0\n",
        "    \n",
        "    # Load the model back\n",
        "    model = load_model_from_run(run_id, \"model\")\n",
        "    \n",
        "    # Test prediction\n",
        "    predictions = model.predict(X_test)\n",
        "    assert len(predictions) == len(y_test)\n",
        "    \n",
        "    # Check accuracy is reasonable (should be > 0.8 for iris)\n",
        "    accuracy = (predictions == y_test).mean()\n",
        "    assert accuracy > 0.8\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run tests\n",
        "    test_data_loading()\n",
        "    print(\"✓ Data loading test passed\")\n",
        "    \n",
        "    test_experiment_setup()\n",
        "    print(\"✓ Experiment setup test passed\")\n",
        "    \n",
        "    test_model_training_and_loading()\n",
        "    print(\"✓ Model training and loading test passed\")\n",
        "    \n",
        "    print(\"\\nAll tests passed! 🎉\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting tests/test_explainer.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile tests/test_explainer.py\n",
        "import sys\n",
        "import os\n",
        "import pytest\n",
        "from pathlib import Path\n",
        "\n",
        "# Add src to path\n",
        "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
        "\n",
        "def test_yaml_roundtrip(tmp_path):\n",
        "    \"\"\"Test that a dashboard can be saved to YAML and reloaded.\"\"\"\n",
        "    from src.mlops.explainer import build_and_log_dashboard, load_dashboard_yaml\n",
        "    from sklearn.datasets import load_iris\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    import mlflow\n",
        "    import pandas as pd\n",
        "\n",
        "    iris = load_iris()\n",
        "    X, y = iris.data, iris.target\n",
        "    X_df = pd.DataFrame(X, columns=iris.feature_names)\n",
        "    model = LogisticRegression(max_iter=1000).fit(X, y)\n",
        "    with mlflow.start_run():\n",
        "        yaml_path = build_and_log_dashboard(\n",
        "            model, X_df, y,\n",
        "            serve=False,\n",
        "            save_yaml=True,\n",
        "            output_dir=tmp_path\n",
        "        )\n",
        "        # Reload\n",
        "        dash = load_dashboard_yaml(yaml_path)\n",
        "        assert dash.explainer.model.__class__.__name__ == \"LogisticRegression\"\n",
        "\n",
        "\n",
        "def test_build_dashboard(tmp_path):\n",
        "    \"\"\"Test that a dashboard can be built and saved.\"\"\"\n",
        "    from src.mlops.explainer import build_and_log_dashboard\n",
        "    from sklearn.datasets import load_iris\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    import mlflow\n",
        "    import pandas as pd\n",
        "\n",
        "    iris = load_iris()\n",
        "    X, y = iris.data, iris.target\n",
        "    X_df = pd.DataFrame(X, columns=iris.feature_names)\n",
        "    model = LogisticRegression(max_iter=1000).fit(X, y)\n",
        "    with mlflow.start_run():\n",
        "        html = build_and_log_dashboard(\n",
        "            model, X_df, y,\n",
        "            serve=False,\n",
        "            save_yaml=False,\n",
        "            output_dir=tmp_path\n",
        "        )\n",
        "        assert html.exists() and html.suffix == \".html\" \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
