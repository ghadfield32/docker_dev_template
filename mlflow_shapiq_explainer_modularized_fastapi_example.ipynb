{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/backend/ML/mlops/config.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/backend/ML/mlops/config.py\n",
        "\"\"\"Central MLflow configuration for consistent experiment tracking.\"\"\"\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Constants\n",
        "RANDOM_STATE = 42\n",
        "TEST_SIZE = 0.2\n",
        "\n",
        "# MLflow settings\n",
        "MLFLOW_TRACKING_URI = os.getenv(\"MLFLOW_TRACKING_URI\", \"file:./mlruns\")\n",
        "MLFLOW_EXPERIMENT_NAME = \"iris_classification\"\n",
        "\n",
        "# Ensure MLflow directory exists\n",
        "mlruns_dir = Path(\"./mlruns\")\n",
        "mlruns_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Set MLflow tracking URI\n",
        "os.environ[\"MLFLOW_TRACKING_URI\"] = MLFLOW_TRACKING_URI\n",
        "\n",
        "# ─── MLflow configuration ──────────────────────────────────────────────────\n",
        "# Use Docker service-name so this works inside the compose network\n",
        "# Falls back to local file store for standalone usage\n",
        "TRACKING_URI = os.getenv(\"MLFLOW_TRACKING_URI\", \"http://mlflow:5000\")\n",
        "EXPERIMENT_NAME = \"iris_classification\"\n",
        "ARTIFACT_ROOT = os.getenv(\"MLFLOW_ARTIFACT_ROOT\", \"./mlruns\")\n",
        "\n",
        "# ─── Model registry ────────────────────────────────────────────────────────\n",
        "MODEL_NAME = \"iris_classifier\"\n",
        "MODEL_STAGE_PRODUCTION = \"Production\"\n",
        "MODEL_STAGE_STAGING = \"Staging\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/backend/ML/mlops/logging.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/backend/ML/mlops/logging.py\n",
        "\"\"\"\n",
        "Extended MLflow logging helpers.\n",
        "\"\"\"\n",
        "from __future__ import annotations\n",
        "import mlflow\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Dict, Any, Sequence, Optional\n",
        "from matplotlib.figure import Figure\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_recall_fscore_support,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    roc_auc_score,\n",
        "    log_loss,\n",
        "    matthews_corrcoef,\n",
        ")\n",
        "\n",
        "\n",
        "def _log_fig(fig: Figure, name: str) -> None:\n",
        "    \"\"\"Log a Matplotlib figure directly without temp files.\"\"\"\n",
        "    mlflow.log_figure(fig, artifact_file=name)\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "def log_full_metrics(\n",
        "    y_true, y_pred, *, label_list: Optional[Sequence[int]] = None, prefix: str = \"\"\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Compute & log *all* useful classification metrics.\n",
        "\n",
        "    Returns a flat dict so callers can unit-test easily.\n",
        "\n",
        "    Args:\n",
        "        y_true: True labels\n",
        "        y_pred: Predicted labels\n",
        "        label_list: Optional list of label integers (for compatibility)\n",
        "        prefix: Optional prefix for metric names\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of all calculated metrics\n",
        "    \"\"\"\n",
        "    # (1) macro metrics ------------------------------------------------------\n",
        "    macro = precision_recall_fscore_support(\n",
        "        y_true, y_pred, average=\"macro\", zero_division=\"warn\"\n",
        "    )\n",
        "    metrics: Dict[str, float] = {\n",
        "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
        "        \"precision_macro\": float(macro[0]),\n",
        "        \"recall_macro\": float(macro[1]),\n",
        "        \"f1_macro\": float(macro[2]),\n",
        "    }\n",
        "\n",
        "    # (2) per-class ----------------------------------------------------------\n",
        "    report = classification_report(y_true, y_pred, output_dict=True, zero_division=\"warn\")\n",
        "    if isinstance(report, dict):\n",
        "        for klass, d in report.items():\n",
        "            if isinstance(klass, str) and klass.isdigit():  # skip 'accuracy', 'macro avg', …\n",
        "                k = int(klass)\n",
        "                if isinstance(d, dict):\n",
        "                    precision_val = d.get(\"precision\", 0.0)\n",
        "                    recall_val = d.get(\"recall\", 0.0)\n",
        "                    f1_val = d.get(\"f1-score\", 0.0)\n",
        "                    support_val = d.get(\"support\", 0.0)\n",
        "\n",
        "                    metrics[f\"precision_{k}\"] = float(precision_val) if precision_val is not None else 0.0\n",
        "                    metrics[f\"recall_{k}\"] = float(recall_val) if recall_val is not None else 0.0\n",
        "                    metrics[f\"f1_{k}\"] = float(f1_val) if f1_val is not None else 0.0\n",
        "                    metrics[f\"support_{k}\"] = float(support_val) if support_val is not None else 0.0\n",
        "\n",
        "    # (3) derived – try/except so we never crash ----------------------------\n",
        "    try:\n",
        "        metrics[\"roc_auc_ovr_weighted\"] = roc_auc_score(\n",
        "            y_true, pd.get_dummies(y_pred), multi_class=\"ovr\", average=\"weighted\"\n",
        "        )\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        metrics[\"log_loss\"] = log_loss(y_true, pd.get_dummies(y_pred))\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        metrics[\"mcc\"] = matthews_corrcoef(y_true, y_pred)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # (4) optional prefix for nested CV, etc. -------------------------------\n",
        "    if prefix:\n",
        "        metrics = {f\"{prefix}_{k}\": v for k, v in metrics.items()}\n",
        "\n",
        "    mlflow.log_metrics(metrics)\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def log_confusion_matrix(\n",
        "    y_true, y_pred, *, class_names: Optional[Sequence[str]] = None, artifact_name: str = \"confusion_matrix.png\"\n",
        ") -> None:\n",
        "    \"\"\"Create + log confusion matrix using mlflow.log_figure.\"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    sns.heatmap(\n",
        "        cm,\n",
        "        annot=True,\n",
        "        fmt=\"d\",\n",
        "        cmap=\"Blues\",\n",
        "        xticklabels=class_names if class_names is not None else \"auto\",\n",
        "        yticklabels=class_names if class_names is not None else \"auto\",\n",
        "        ax=ax,\n",
        "    )\n",
        "    ax.set_xlabel(\"Predicted\")\n",
        "    ax.set_ylabel(\"Actual\")\n",
        "    ax.set_title(\"Confusion Matrix\")\n",
        "    _log_fig(fig, artifact_name)\n",
        "\n",
        "\n",
        "def log_feature_importance(\n",
        "    feature_names: list, importances: list, artifact_name: str = \"feature_importance.png\"\n",
        "):\n",
        "    \"\"\"Bar plot logged via mlflow.log_figure (no disk I/O).\"\"\"\n",
        "    imp_df = (\n",
        "        pd.DataFrame({\"feature\": feature_names, \"importance\": importances})\n",
        "        .sort_values(\"importance\")\n",
        "    )\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    sns.barplot(data=imp_df, x=\"importance\", y=\"feature\", ax=ax)\n",
        "    ax.set_title(\"Feature Importances\")\n",
        "    _log_fig(fig, artifact_name)\n",
        "\n",
        "\n",
        "def log_parameters(params: Dict[str, Any]) -> None:\n",
        "    \"\"\"\n",
        "    Log parameters to MLflow.\n",
        "\n",
        "    Args:\n",
        "        params: Dictionary of parameter names and values\n",
        "    \"\"\"\n",
        "    mlflow.log_params(params)\n",
        "\n",
        "\n",
        "def log_dataset_info(X_train, X_test, y_train, y_test) -> None:\n",
        "    \"\"\"\n",
        "    Log dataset information as parameters.\n",
        "\n",
        "    Args:\n",
        "        X_train: Training features\n",
        "        X_test: Test features\n",
        "        y_train: Training labels\n",
        "        y_test: Test labels\n",
        "    \"\"\"\n",
        "    dataset_params = {\n",
        "        \"train_size\": len(X_train),\n",
        "        \"test_size\": len(X_test),\n",
        "        \"n_features\": (X_train.shape[1] if hasattr(X_train, \"shape\") else len(X_train[0])),\n",
        "        \"n_classes\": (len(set(y_train)) if hasattr(y_train, \"__iter__\") else 1),\n",
        "    }\n",
        "\n",
        "    log_parameters(dataset_params)\n",
        "\n",
        "\n",
        "# Legacy compatibility - keep old function name as alias\n",
        "log_model_metrics = log_full_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/backend/ML/mlops/experiment_utils.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/backend/ML/mlops/experiment_utils.py\n",
        "\"\"\"MLflow experiment utilities.\"\"\"\n",
        "import os\n",
        "import pathlib\n",
        "import mlflow\n",
        "import mlflow.tracking\n",
        "from typing import Optional, Dict, Any\n",
        "import requests\n",
        "\n",
        "from src.backend.ML.mlops.config import EXPERIMENT_NAME, TRACKING_URI\n",
        "\n",
        "import re, shutil, logging\n",
        "from src.backend.ML.mlops.config import ARTIFACT_ROOT\n",
        "\n",
        "# ─── PATCH: src/backend/ML/mlops/experiment_utils.py ──────────────────────────\n",
        "def _patch_yaml_for_mlflow() -> None:\n",
        "    \"\"\"Ensure MLflow entity objects are YAML-serialisable.\n",
        "\n",
        "    MLflow serialises *Metric*, *Param*, and *RunTag* objects to `meta.yaml` via\n",
        "    ``yaml.safe_dump``.  Older MLflow versions ship their own dumper that\n",
        "    already knows these classes, but many environments mix package versions\n",
        "    (e.g. Conda ↔︎ pip) which means the representers are **missing** and YAML\n",
        "    falls back to the *object* representer.  Our previous fix attempted to call\n",
        "    ``obj.to_dict()`` – unfortunately :class:`mlflow.entities.Metric` does **NOT**\n",
        "    implement that method, leading to the runtime error you just saw:\n",
        "\n",
        "        AttributeError: 'Metric' object has no attribute 'to_dict'\n",
        "\n",
        "    The new implementation maps the public attributes explicitly instead of\n",
        "    assuming a convenience helper exists.  It is fully compatible with both\n",
        "    MLflow ≤2.11 and PyYAML ≥6.\n",
        "    \"\"\"\n",
        "\n",
        "    import yaml\n",
        "\n",
        "    try:\n",
        "        from mlflow.entities import Metric, Param, RunTag\n",
        "        from mlflow.utils.yaml_utils import YamlSafeDumper as _ML_DUMPER\n",
        "    except Exception:\n",
        "        # If MLflow isn't imported yet (unit-tests), just return.\n",
        "        return\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # Helper → guaranteed, stable mapping for every entity type ----------\n",
        "    # ------------------------------------------------------------------\n",
        "    def _metric_to_dict(m: Metric) -> dict[str, int | float | str]:\n",
        "        return {\n",
        "            \"key\": m.key,\n",
        "            \"value\": m.value,\n",
        "            \"timestamp\": m.timestamp,\n",
        "            \"step\": m.step,\n",
        "        }\n",
        "\n",
        "    def _param_to_dict(p: Param) -> dict[str, str]:\n",
        "        return {\"key\": p.key, \"value\": p.value}\n",
        "\n",
        "    def _runtag_to_dict(t: RunTag) -> dict[str, str]:\n",
        "        return {\"key\": t.key, \"value\": t.value}\n",
        "\n",
        "    _DICT_MAPPERS = {\n",
        "        Metric: _metric_to_dict,\n",
        "        Param: _param_to_dict,\n",
        "        RunTag: _runtag_to_dict,\n",
        "    }\n",
        "\n",
        "    def _represent_entity(dumper: yaml.Dumper, obj):  # noqa: D401\n",
        "        \"\"\"Generic PyYAML representer for MLflow entities.\"\"\"\n",
        "        return dumper.represent_dict(_DICT_MAPPERS[type(obj)](obj))\n",
        "\n",
        "    # Target both the stock SafeDumper **and** MLflow's own dumper so we cover\n",
        "    # all call-sites.  Registering twice is idempotent; we guard with `in`.\n",
        "    for dumper_cls in (yaml.SafeDumper, _ML_DUMPER):\n",
        "        for cls in _DICT_MAPPERS:\n",
        "            if cls not in dumper_cls.yaml_representers:\n",
        "                yaml.add_representer(cls, _represent_entity, Dumper=dumper_cls)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "\n",
        "def _is_http_uri(uri: str) -> bool:\n",
        "    return uri.startswith(\"http\")\n",
        "\n",
        "def _local_path_from_uri(uri: str) -> pathlib.Path:\n",
        "    return pathlib.Path(uri.replace(\"file:\", \"\", 1)) if uri.startswith(\"file:\") else pathlib.Path(uri)\n",
        "\n",
        "_HEALTH_ENDPOINTS = (\"/health\", \"/version\")\n",
        "_hex32 = re.compile(r\"^[0-9a-f]{32}$\", re.I)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def _ping_tracking_server(uri: str, timeout: float = 2.0) -> bool:\n",
        "    \"\"\"Return True iff an HTTP MLflow server is reachable at *uri*.\"\"\"\n",
        "    if not uri.startswith(\"http\"):\n",
        "        return False                        # file store – nothing to ping\n",
        "    try:\n",
        "        # Use new health endpoints\n",
        "        for ep in _HEALTH_ENDPOINTS:\n",
        "            response = requests.get(uri.rstrip(\"/\") + ep, timeout=timeout)\n",
        "            response.raise_for_status()\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "\n",
        "# ── experiment_utils.py ───────────────────────────────────────────────────\n",
        "import pathlib, shutil, logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def _ensure_trash(root: pathlib.Path) -> pathlib.Path:\n",
        "    \"\"\"Return the path to <root>/.trash, creating it if needed.\"\"\"\n",
        "    trash_path = root / \".trash\"\n",
        "    trash_path.mkdir(exist_ok=True)\n",
        "    return trash_path\n",
        "\n",
        "\n",
        "def _sanitize_mlruns_dir(root: pathlib.Path) -> None:\n",
        "    \"\"\"\n",
        "    Remove **only** invalid directories while preserving MLflow's mandatory\n",
        "    '.trash' folder.  Idempotent and safe to call repeatedly.\n",
        "    \"\"\"\n",
        "    trash_root = _ensure_trash(root)\n",
        "\n",
        "    # 1️⃣  Clean *inside* .trash (broken experiments that never got meta.yaml)\n",
        "    for p in list(trash_root.iterdir()):\n",
        "        if p.is_dir() and (not p.name.isdigit() or not (p / \"meta.yaml\").exists()):\n",
        "            logger.warning(\"🧹 Purging corrupt trash dir %s\", p)\n",
        "            shutil.rmtree(p, ignore_errors=True)\n",
        "\n",
        "    # 2️⃣  Sweep root level (skip .trash itself)\n",
        "    for p in list(root.iterdir()):\n",
        "        # Preserve critical MLflow folders (models registry & artefacts)\n",
        "        if p in {trash_root, root / \"models\", root / \"artifacts\"}:\n",
        "            continue  # KEEP – required by MLflow\n",
        "\n",
        "        remove = (\n",
        "            p.is_dir()\n",
        "            and (\n",
        "                not p.name.isdigit()                 # junk (non-experiment dirs)\n",
        "                or not (p / \"meta.yaml\").exists()    # corrupt experiment missing meta\n",
        "            )\n",
        "        )\n",
        "        if remove:\n",
        "            logger.warning(\"🧹 Removing stray MLflow dir %s\", p)\n",
        "            shutil.rmtree(p, ignore_errors=True)\n",
        "\n",
        "\n",
        "def _fallback_uri() -> str:\n",
        "    \"\"\"Local file store outside default ./mlruns to avoid collisions.\"\"\"\n",
        "    local = pathlib.Path.cwd() / \"mlruns_local\"\n",
        "    local.mkdir(exist_ok=True)\n",
        "    _sanitize_mlruns_dir(local)\n",
        "    _ensure_trash(local)          # make doubly sure\n",
        "    return f\"file:{local}\"\n",
        "\n",
        "\n",
        "# ── src/backend/ML/mlops/experiment_utils.py ──\n",
        "_resolved_uri: str | None = None         # module-level cache\n",
        "\n",
        "\n",
        "\n",
        "def setup_mlflow_experiment(experiment_name: str | None = None) -> None:\n",
        "    \"\"\"\n",
        "    Initialise MLflow tracking & experiment **safely**, no matter whether a\n",
        "    remote server is reachable or we fall back to the local file store.\n",
        "    \"\"\"\n",
        "    _patch_yaml_for_mlflow()\n",
        "    global _resolved_uri\n",
        "    exp_name = experiment_name or EXPERIMENT_NAME\n",
        "\n",
        "    if _resolved_uri is None:\n",
        "        uri = TRACKING_URI\n",
        "        if not _ping_tracking_server(uri):\n",
        "            uri = _fallback_uri()\n",
        "            logger.warning(\"⚠️  MLflow server unreachable – using local store %s\", uri)\n",
        "        mlflow.set_tracking_uri(uri)\n",
        "        _resolved_uri = uri\n",
        "    else:\n",
        "        mlflow.set_tracking_uri(_resolved_uri)\n",
        "\n",
        "    # --- NEW: always clean the store when it's file-based ------------------\n",
        "    if not _is_http_uri(_resolved_uri):\n",
        "        root_path = _local_path_from_uri(_resolved_uri)\n",
        "        _sanitize_mlruns_dir(root_path)\n",
        "    # ----------------------------------------------------------------------\n",
        "\n",
        "    # decide on artifact root only if explicitly configured\n",
        "    artifact_loc = ARTIFACT_ROOT.strip() or None\n",
        "\n",
        "    if mlflow.get_experiment_by_name(exp_name) is None:\n",
        "        mlflow.create_experiment(exp_name, artifact_location=artifact_loc)\n",
        "    mlflow.set_experiment(exp_name)\n",
        "    logger.info(\"🗂 Experiment '%s' @ %s\", exp_name, _resolved_uri)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_best_run(\n",
        "    experiment_name: Optional[str] = None,\n",
        "    metric_key: str = \"accuracy\",\n",
        "    maximize: bool = True,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Return a *shallow* dict with run_id, metrics.*, and params.* keys\n",
        "    so downstream code can use predictable dotted paths.\n",
        "    \"\"\"\n",
        "    exp_name = experiment_name or EXPERIMENT_NAME\n",
        "    setup_mlflow_experiment(exp_name)\n",
        "\n",
        "    client = mlflow.tracking.MlflowClient()\n",
        "    exp = mlflow.get_experiment_by_name(exp_name)\n",
        "    if exp is None:\n",
        "        raise ValueError(f\"Experiment '{exp_name}' not found\")\n",
        "\n",
        "    order = \"DESC\" if maximize else \"ASC\"\n",
        "    run = client.search_runs(\n",
        "        [exp.experiment_id],\n",
        "        order_by=[f\"metrics.{metric_key} {order}\"],\n",
        "        max_results=1,\n",
        "    )[0]\n",
        "\n",
        "    # Build a *flat* mapping -------------------------------------------------\n",
        "    flat: Dict[str, Any] = {\"run_id\": run.info.run_id}\n",
        "\n",
        "    # Metrics\n",
        "    for k, v in run.data.metrics.items():\n",
        "        flat[f\"metrics.{k}\"] = v\n",
        "\n",
        "    # Params\n",
        "    for k, v in run.data.params.items():\n",
        "        flat[f\"params.{k}\"] = v\n",
        "\n",
        "    # Tags (optional but handy)\n",
        "    for k, v in run.data.tags.items():\n",
        "        flat[f\"tags.{k}\"] = v\n",
        "\n",
        "    return flat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/backend/ML/mlops/experiment.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/backend/ML/mlops/experiment.py\n",
        "\n",
        "\"\"\"Training utilities with MLflow integration.\"\"\"\n",
        "import mlflow\n",
        "import optuna\n",
        "from typing import Optional\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from .config import RANDOM_STATE, TEST_SIZE\n",
        "from .experiment_utils import setup_mlflow_experiment\n",
        "\n",
        "# Re-export for convenience\n",
        "__all__ = ['setup_mlflow_experiment', 'load_and_prepare_iris_data',\n",
        "           'train_logistic_regression', 'train_random_forest_with_optimization']\n",
        "from .logging import (\n",
        "    log_model_metrics,\n",
        "    log_confusion_matrix,\n",
        "    log_feature_importance,\n",
        "    log_dataset_info,\n",
        "    log_parameters\n",
        ")\n",
        "\n",
        "\n",
        "def load_and_prepare_iris_data(\n",
        "    test_size: float = TEST_SIZE,\n",
        "    random_state: int = RANDOM_STATE\n",
        ") -> DatasetTuple:\n",
        "    iris = load_iris()\n",
        "    X, y = iris.data, iris.target\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=random_state\n",
        "    )\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled  = scaler.transform(X_test)\n",
        "\n",
        "    # ✅ re-wrap as DataFrame so feature names propagate downstream\n",
        "    import pandas as pd\n",
        "    feat_names = iris.feature_names\n",
        "    X_train_df = pd.DataFrame(X_train_scaled, columns=feat_names)\n",
        "    X_test_df  = pd.DataFrame(X_test_scaled,  columns=feat_names)\n",
        "\n",
        "    return (X_train_df, X_test_df, y_train, y_test,\n",
        "            feat_names, list(iris.target_names), scaler)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/backend/ML/mlops/model_registry.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/backend/ML/mlops/model_registry.py\n",
        "\"\"\"MLflow model registry utilities.\"\"\"\n",
        "import mlflow\n",
        "from typing import Optional, Dict, Any\n",
        "from .config import MODEL_NAME, MODEL_STAGE_PRODUCTION\n",
        "\n",
        "\n",
        "def register_model(model_uri: str,\n",
        "                   model_name: Optional[str] = None,\n",
        "                   description: Optional[str] = None) -> str:\n",
        "    \"\"\"\n",
        "    Register a model in the MLflow model registry using the fluent client API.\n",
        "\n",
        "    Args:\n",
        "        model_uri: URI of the model to register\n",
        "        model_name: Name for the registered model\n",
        "        description: Optional description\n",
        "\n",
        "    Returns:\n",
        "        Model version\n",
        "    \"\"\"\n",
        "    name = model_name or MODEL_NAME\n",
        "    client = mlflow.tracking.MlflowClient()\n",
        "\n",
        "    try:\n",
        "        # Create registered model if it doesn't exist\n",
        "        if not client.get_registered_model(name, silent=True):\n",
        "            client.create_registered_model(name)\n",
        "            print(f\"Created new registered model: {name}\")\n",
        "\n",
        "        # Create new version\n",
        "        mv = client.create_model_version(\n",
        "            name=name,\n",
        "            source=model_uri,\n",
        "            description=description\n",
        "        )\n",
        "        print(f\"Created version {mv.version} of model {name}\")\n",
        "        return mv.version\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to register model: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def promote_model_to_stage(model_name: Optional[str] = None,\n",
        "                           version: Optional[str] = None,\n",
        "                           stage: str = MODEL_STAGE_PRODUCTION) -> None:\n",
        "    \"\"\"\n",
        "    Promote a model version to a specific stage using the fluent client.\n",
        "\n",
        "    Args:\n",
        "        model_name: Name of the registered model\n",
        "        version: Version to promote (if None, promotes latest)\n",
        "        stage: Target stage\n",
        "    \"\"\"\n",
        "    name = model_name or MODEL_NAME\n",
        "    client = mlflow.tracking.MlflowClient()\n",
        "\n",
        "    try:\n",
        "        # Get latest version if not specified\n",
        "        if version is None:\n",
        "            latest = client.get_latest_versions(name, stages=[\"None\"])\n",
        "            if not latest:\n",
        "                raise ValueError(f\"No versions found for model {name}\")\n",
        "            version = latest[0].version\n",
        "\n",
        "        # Transition to stage\n",
        "        client.transition_model_version_stage(\n",
        "            name=name,\n",
        "            version=version,\n",
        "            stage=stage\n",
        "        )\n",
        "        print(f\"Promoted model {name} version {version} to {stage}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to promote model: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def load_model_from_registry(model_name: Optional[str] = None,\n",
        "                             stage: str = MODEL_STAGE_PRODUCTION):\n",
        "    \"\"\"\n",
        "    Load a model from the registry by name and stage.\n",
        "\n",
        "    Args:\n",
        "        model_name: Name of the registered model\n",
        "        stage: Stage to load from\n",
        "\n",
        "    Returns:\n",
        "        Loaded model\n",
        "    \"\"\"\n",
        "    name = model_name or MODEL_NAME\n",
        "    model_uri = f\"models:/{name}/{stage}\"\n",
        "\n",
        "    try:\n",
        "        model = mlflow.sklearn.load_model(model_uri)\n",
        "        print(f\"Loaded model {name} from {stage} stage\")\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load model from registry: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def load_model_from_run(run_id: str, artifact_path: str = \"model\"):\n",
        "    \"\"\"\n",
        "    Load a model from a specific run.\n",
        "\n",
        "    Args:\n",
        "        run_id: MLflow run ID\n",
        "        artifact_path: Path to the model artifact\n",
        "\n",
        "    Returns:\n",
        "        Loaded model\n",
        "    \"\"\"\n",
        "    model_uri = f\"runs:/{run_id}/{artifact_path}\"\n",
        "\n",
        "    try:\n",
        "        model = mlflow.sklearn.load_model(model_uri)\n",
        "        print(f\"Loaded model from run {run_id}\")\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load model from run: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def get_model_info(model_name: Optional[str] = None,\n",
        "                   stage: str = MODEL_STAGE_PRODUCTION) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Get information about a registered model using the fluent client.\n",
        "\n",
        "    Args:\n",
        "        model_name: Name of the registered model\n",
        "        stage: Stage to get info for\n",
        "\n",
        "    Returns:\n",
        "        Model information dictionary\n",
        "    \"\"\"\n",
        "    name = model_name or MODEL_NAME\n",
        "    client = mlflow.tracking.MlflowClient()\n",
        "\n",
        "    try:\n",
        "        model_version = client.get_latest_versions(name, stages=[stage])[0]\n",
        "\n",
        "        return {\n",
        "            \"name\": model_version.name,\n",
        "            \"version\": model_version.version,\n",
        "            \"stage\": model_version.current_stage,\n",
        "            \"description\": model_version.description,\n",
        "            \"creation_timestamp\": model_version.creation_timestamp,\n",
        "            \"last_updated_timestamp\": model_version.last_updated_timestamp,\n",
        "            \"run_id\": model_version.run_id\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to get model info: {e}\")\n",
        "        raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/backend/ML/mlops/training.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/backend/ML/mlops/training.py\n",
        "\"\"\"Training utilities with MLflow integration.\"\"\"\n",
        "import mlflow\n",
        "from mlflow import sklearn  # type: ignore\n",
        "from mlflow import models  # type: ignore\n",
        "import optuna\n",
        "from optuna.integration.mlflow import MLflowCallback\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Optional, Tuple, List, Callable, cast, Any, Dict, TypeAlias\n",
        "from numpy.typing import NDArray\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.utils import Bunch\n",
        "from sklearn.pipeline import Pipeline  # NEW\n",
        "\n",
        "from src.backend.ML.mlops.config import RANDOM_STATE, TEST_SIZE\n",
        "from src.backend.ML.mlops.experiment_utils import setup_mlflow_experiment\n",
        "from src.backend.ML.mlops.logging import (\n",
        "    log_full_metrics,\n",
        "    log_confusion_matrix,\n",
        "    log_feature_importance,\n",
        "    log_dataset_info,\n",
        "    log_parameters\n",
        ")\n",
        "from src.backend.ML.mlops.shapiq_utils import log_shapiq_interactions\n",
        "\n",
        "# Type aliases for complex types\n",
        "FloatArray: TypeAlias = NDArray[np.float64]\n",
        "IntArray: TypeAlias = NDArray[np.int64]\n",
        "DatasetTuple: TypeAlias = Tuple[FloatArray, FloatArray, IntArray, IntArray, List[str], List[str], StandardScaler]\n",
        "\n",
        "\n",
        "def load_and_prepare_iris_data(\n",
        "    test_size: float = TEST_SIZE,\n",
        "    random_state: int = RANDOM_STATE\n",
        ") -> DatasetTuple:\n",
        "    \"\"\"\n",
        "    Load and prepare the Iris dataset.\n",
        "\n",
        "    Args:\n",
        "        test_size: Fraction of data to use for testing\n",
        "        random_state: Random state for reproducibility\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (X_train_scaled, X_test_scaled, y_train, y_test,\n",
        "                 feature_names, target_names, scaler)\n",
        "    \"\"\"\n",
        "    # Load dataset\n",
        "    iris: Any = load_iris()\n",
        "    X: NDArray[np.float64] = cast(NDArray[np.float64], iris.data)\n",
        "    y: NDArray[np.int64] = cast(NDArray[np.int64], iris.target)\n",
        "    feature_names: List[str] = list(iris.feature_names)\n",
        "    target_names: List[str] = list(iris.target_names)\n",
        "\n",
        "    # Split data\n",
        "    X_train: NDArray[np.float64]\n",
        "    X_test: NDArray[np.float64]\n",
        "    y_train: NDArray[np.int64]\n",
        "    y_test: NDArray[np.int64]\n",
        "    X_train, X_test, y_train, y_test = cast(\n",
        "        Tuple[NDArray[np.float64], NDArray[np.float64], NDArray[np.int64], NDArray[np.int64]],\n",
        "        train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
        "    )\n",
        "\n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled: NDArray[np.float64] = cast(NDArray[np.float64], scaler.fit_transform(X_train))\n",
        "    X_test_scaled: NDArray[np.float64] = cast(NDArray[np.float64], scaler.transform(X_test))\n",
        "\n",
        "    return (X_train_scaled, X_test_scaled, y_train, y_test,\n",
        "            feature_names, target_names, scaler)\n",
        "\n",
        "\n",
        "# === (A) LOGISTIC REGRESSION (training only, NO dashboard) ================\n",
        "def train_logistic_regression(\n",
        "    X_train, y_train, X_test, y_test, feature_names, target_names,\n",
        "    *, run_name: str = \"lr_baseline\", register: bool = True\n",
        ") -> str:\n",
        "    \"\"\"Train logistic regression model without dashboard integration.\"\"\"\n",
        "    setup_mlflow_experiment()\n",
        "    # Disable automatic *model* logging so we can register exactly **one**\n",
        "    # version via the explicit ``mlflow.sklearn.log_model`` call below. This\n",
        "    # avoids the duplicate-artifact collision (autolog logs to `model/` by\n",
        "    # default, which then prevents our registered-model call from re-using\n",
        "    # the same artifact path).\n",
        "    mlflow.sklearn.autolog(log_models=False)\n",
        "\n",
        "    with mlflow.start_run(run_name=run_name) as run:\n",
        "        log_dataset_info(X_train, X_test, y_train, y_test)\n",
        "        model = LogisticRegression(random_state=RANDOM_STATE, max_iter=1_000).fit(\n",
        "            X_train, y_train\n",
        "        )\n",
        "\n",
        "        y_pred = model.predict(X_test)\n",
        "        log_full_metrics(y_test, y_pred)\n",
        "        log_confusion_matrix(y_test, y_pred, class_names=target_names)\n",
        "\n",
        "        signature = mlflow.models.infer_signature(X_train, model.predict(X_train))\n",
        "        # Log *one* artefact folder (`model/`) and register → iris_logreg\n",
        "        sklearn.log_model(\n",
        "            model,\n",
        "            name=\"iris_logreg\",                          # NEW – explicit name param (MLflow ≥2.9)\n",
        "            registered_model_name=\"iris_logreg\" if register else None,\n",
        "            signature=signature,\n",
        "            input_example=X_test[:5],\n",
        "        )\n",
        "\n",
        "        # SHAP-IQ: compute & log feature interaction values\n",
        "        X_test_df = pd.DataFrame(X_test, columns=feature_names)\n",
        "        log_shapiq_interactions(model, X_test_df, feature_names, max_order=2)\n",
        "\n",
        "        return run.info.run_id\n",
        "\n",
        "\n",
        "def _create_rf_objective(X_train, y_train, X_test, y_test) -> Callable[[optuna.trial.Trial], float]:\n",
        "    \"\"\"Create Optuna objective function for Random Forest optimization.\"\"\"\n",
        "    def objective(trial: optuna.trial.Trial) -> float:\n",
        "        params = {\n",
        "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 10, 200),\n",
        "            \"max_depth\": trial.suggest_int(\"max_depth\", 2, 20),\n",
        "            \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 20),\n",
        "            \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 10),\n",
        "            \"random_state\": RANDOM_STATE,\n",
        "        }\n",
        "        m = RandomForestClassifier(**params).fit(X_train, y_train)\n",
        "        return float(accuracy_score(y_test, m.predict(X_test)))\n",
        "    return objective\n",
        "\n",
        "\n",
        "# === (B) RANDOM-FOREST + Optuna (training only) ===========================\n",
        "def train_random_forest_optimized(\n",
        "    X_train, y_train, X_test, y_test, feature_names, target_names,\n",
        "    *, n_trials: int = 50, run_name: str = \"rf_optimized\", register: bool = True\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Train an Optuna-tuned Random-Forest inside a Pipeline(StandardScaler→RF)\n",
        "    and log the entire pipeline to MLflow so scaling is reproduced at inference.\n",
        "    \"\"\"\n",
        "    setup_mlflow_experiment()\n",
        "    mlflow.sklearn.autolog(disable=True)\n",
        "\n",
        "    with mlflow.start_run(run_name=run_name) as run:\n",
        "        log_dataset_info(X_train, X_test, y_train, y_test)\n",
        "\n",
        "        study = optuna.create_study(direction=\"maximize\")\n",
        "        study.optimize(\n",
        "            _create_rf_objective(X_train, y_train, X_test, y_test),\n",
        "            n_trials=n_trials,\n",
        "            callbacks=[MLflowCallback(\n",
        "                tracking_uri=mlflow.get_tracking_uri(),\n",
        "                metric_name=\"accuracy\", mlflow_kwargs={\"nested\": True}\n",
        "            )],\n",
        "        )\n",
        "\n",
        "        # 🟢 Pipeline with scaler\n",
        "        best_rf = RandomForestClassifier(**study.best_params, random_state=RANDOM_STATE)\n",
        "        pipeline = Pipeline([(\"scaler\", StandardScaler()), (\"rf\", best_rf)])\n",
        "        pipeline.fit(X_train, y_train)\n",
        "\n",
        "        y_pred = pipeline.predict(X_test)\n",
        "        log_full_metrics(y_test, y_pred)\n",
        "        log_confusion_matrix(y_test, y_pred, class_names=target_names)\n",
        "        log_feature_importance(feature_names, best_rf.feature_importances_)\n",
        "        mlflow.log_metric(\"best_accuracy\", study.best_value)\n",
        "\n",
        "        signature = mlflow.models.infer_signature(X_train, pipeline.predict(X_train))\n",
        "        sklearn.log_model(\n",
        "            pipeline,\n",
        "            name=\"iris_random_forest\",\n",
        "            registered_model_name=\"iris_random_forest\" if register else None,\n",
        "            signature=signature, input_example=X_test[:5],\n",
        "        )\n",
        "\n",
        "        X_test_df = pd.DataFrame(X_test, columns=feature_names)\n",
        "        log_shapiq_interactions(best_rf, X_test_df, feature_names, max_order=2)\n",
        "\n",
        "        return run.info.run_id\n",
        "\n",
        "\n",
        "# === (C) ONE-STOP helper: train both models ===============================\n",
        "def run_all_trainings(*,\n",
        "    test_size: float = TEST_SIZE, random_state: int = RANDOM_STATE, n_trials: int = 50) -> None:\n",
        "    \"\"\"Train both logistic regression and random forest models.\"\"\"\n",
        "    X_tr, X_te, y_tr, y_te, feats, tgts, _ = load_and_prepare_iris_data(\n",
        "        test_size, random_state\n",
        "    )\n",
        "    train_logistic_regression(\n",
        "        X_tr, y_tr, X_te, y_te, feats, tgts, run_name=\"lr_baseline\"\n",
        "    )\n",
        "    train_random_forest_optimized(\n",
        "        X_tr, y_tr, X_te, y_te, feats, tgts,\n",
        "        n_trials=n_trials, run_name=\"rf_optimized\"\n",
        "    )\n",
        "\n",
        "\n",
        "# === (D) Robust comparator ===============================================\n",
        "def compare_models(\n",
        "    experiment_name: Optional[str] = None,\n",
        "    metric_key: str = \"accuracy\",\n",
        "    maximize: bool = True,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Print the best run according to *metric_key* while gracefully\n",
        "    falling-back to common alternates when the preferred key is missing.\n",
        "    \"\"\"\n",
        "    from .experiment_utils import get_best_run\n",
        "\n",
        "    fallback_keys = [\"accuracy_score\", \"best_accuracy\"]\n",
        "    try:\n",
        "        best = get_best_run(experiment_name, metric_key, maximize)\n",
        "        rid = best[\"run_id\"]\n",
        "\n",
        "        # choose first key that exists\n",
        "        score = best.get(f\"metrics.{metric_key}\")\n",
        "        if score is None:\n",
        "            for alt in fallback_keys:\n",
        "                score = best.get(f\"metrics.{alt}\")\n",
        "                if score is not None:\n",
        "                    metric_key = alt\n",
        "                    break\n",
        "\n",
        "        model_type = best.get(\"params.model_type\", \"unknown\")\n",
        "        print(f\"🏆 Best run: {rid}\")\n",
        "        print(f\"📈 {metric_key}: {score if score is not None else 'N/A'}\")\n",
        "        print(f\"🔖 Model type: {model_type}\")\n",
        "    except Exception as err:\n",
        "        print(f\"❌ Error comparing models: {err}\")\n",
        "\n",
        "\n",
        "# Legacy compatibility functions (with dashboard support)\n",
        "train_logistic_regression_autolog = train_logistic_regression\n",
        "train_random_forest_with_optimization = train_random_forest_optimized\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_all_trainings()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/backend/ML/mlops/utils_training_guards.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/backend/ML/mlops/utils_training_guards.py\n",
        "# src/backend/ML/mlops/utils_training_guards.py\n",
        "\n",
        "import logging\n",
        "import shutil\n",
        "\n",
        "try:\n",
        "    # PyTensor exposes its config flags in the configdefaults module\n",
        "    from pytensor.configdefaults import config as pconfig\n",
        "except ImportError as exc:\n",
        "    # If PyTensor isn’t installed or the API changed, disable C-ops fallback\n",
        "    logging.getLogger(__name__).warning(\n",
        "        \"Could not import pytensor.configdefaults.config; \"\n",
        "        \"skipping compiler checks.\"\n",
        "    )\n",
        "    pconfig = None\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def ensure_compiler() -> None:\n",
        "    \"\"\"\n",
        "    Check for the configured C++ compiler (config.cxx).\n",
        "    If it's set but not on PATH, disable C-ops to force Python fallback.\n",
        "    \"\"\"\n",
        "    if not pconfig:\n",
        "        # Nothing to do if config isn't available\n",
        "        return\n",
        "\n",
        "    cxx = getattr(pconfig, \"cxx\", \"\")\n",
        "    if cxx and shutil.which(cxx) is None:\n",
        "        logger.warning(\n",
        "            f\"Configured compiler '{cxx}' not found in PATH. \"\n",
        "            \"Disabling PyTensor C-ops (falling back to pure-Python).\"\n",
        "        )\n",
        "        # Disable C++ compilation\n",
        "        pconfig.cxx = \"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/backend/ML/mlops/training_bayes.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/backend/ML/mlops/training_bayes.py\n",
        "from __future__ import annotations\n",
        "import os\n",
        "import shutil\n",
        "import logging\n",
        "\n",
        "try:\n",
        "    import pymc as pm\n",
        "    import arviz as az\n",
        "except ImportError as exc:\n",
        "    raise ImportError(\"PyMC and ArviZ are required for Bayesian model training\") from exc\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from src.backend.ML.mlops.utils_training_guards import ensure_compiler\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "_BAYES_MODEL_NAME = \"breast_cancer_bayes\"\n",
        "\n",
        "def train_bayes_logreg(\n",
        "    debug: bool = False,\n",
        "    *,\n",
        "    draws: int = 1000,\n",
        "    tune: int = 500,\n",
        "    run_name: str = \"bayes_logreg\",\n",
        "    register: bool = True,\n",
        ") -> tuple[pm.Model, \"az.InferenceData\", str]:\n",
        "    \"\"\"\n",
        "    Bayesian logistic regression with full MLflow logging **including accuracy**.\n",
        "    \"\"\"\n",
        "    from src.backend.ML.mlops.experiment_utils import setup_mlflow_experiment\n",
        "    from src.backend.ML.mlops.logging import log_full_metrics, log_confusion_matrix  # NEW\n",
        "    import mlflow\n",
        "\n",
        "    ensure_compiler()\n",
        "    setup_mlflow_experiment()\n",
        "\n",
        "    data = load_breast_cancer()\n",
        "    X_raw, y_raw = data.data.astype(float), data.target.astype(int)\n",
        "    X_std = (X_raw - X_raw.mean(0)) / X_raw.std(0)\n",
        "    n_feat = X_std.shape[1]\n",
        "\n",
        "    p0, p1 = (y_raw == 0).mean(), (y_raw == 1).mean()\n",
        "    intercept_prior = float(\"-inf\") if p0 == 0 else np.log(p1 / p0)\n",
        "\n",
        "    with mlflow.start_run(run_name=run_name) as run:\n",
        "        with pm.Model() as model:\n",
        "            pm.Data(\"X_shared\", X_std)\n",
        "            pm.Data(\"y_obs\", y_raw)\n",
        "            alpha = pm.Normal(\"alpha\", mu=intercept_prior, sigma=2)\n",
        "            beta = pm.Normal(\"beta\", mu=0, sigma=1, shape=n_feat)\n",
        "            logits = alpha + pm.math.dot(X_std, beta)\n",
        "            pm.Bernoulli(\"y\", p=pm.math.sigmoid(logits), observed=y_raw)\n",
        "\n",
        "            idata = pm.sample(\n",
        "                draws=draws, tune=tune, chains=2, cores=1,\n",
        "                target_accept=0.9, progressbar=not debug\n",
        "            )\n",
        "\n",
        "        # ── Metrics --------------------------------------------------------\n",
        "        posterior = idata.posterior\n",
        "        alpha_s = posterior[\"alpha\"].values.reshape(-1)\n",
        "        beta_s  = posterior[\"beta\"].values.reshape(-1, n_feat)\n",
        "\n",
        "        probs = 1 / (1 + np.exp(-(alpha_s.mean() + X_std @ beta_s.mean(0))))\n",
        "        y_pred = (probs > 0.5).astype(int)\n",
        "\n",
        "        log_full_metrics(y_raw, y_pred)                     # NEW\n",
        "        log_confusion_matrix(y_raw, y_pred,                 # NEW\n",
        "                             class_names=[\"malignant\", \"benign\"])\n",
        "\n",
        "        mlflow.log_metric(\"n_draws\", int(draws))\n",
        "\n",
        "        # ── Model logging (unchanged) -------------------------------------\n",
        "        signature = mlflow.models.infer_signature(X_std, y_raw)\n",
        "\n",
        "        class _BayesLogRegPyFunc(mlflow.pyfunc.PythonModel):\n",
        "            def __init__(self, a, b):\n",
        "                self._a, self._b = a.astype(\"float64\"), b.astype(\"float64\")\n",
        "            def predict(self, _, model_input):\n",
        "                X = np.asarray(model_input, dtype=\"float64\")\n",
        "                logits = self._a[:, None] + self._b @ X.T\n",
        "                return (1 / (1 + np.exp(-logits))).mean(0)\n",
        "\n",
        "        mlflow.pyfunc.log_model(\n",
        "            name=\"breast_cancer_bayes\",\n",
        "            python_model=_BayesLogRegPyFunc(alpha_s, beta_s),\n",
        "            input_example=X_std[:5],\n",
        "            signature=signature,\n",
        "            registered_model_name=_BAYES_MODEL_NAME if register else None,\n",
        "        )\n",
        "\n",
        "        if register:\n",
        "            client = mlflow.tracking.MlflowClient()\n",
        "            mv = client.get_latest_versions(_BAYES_MODEL_NAME, stages=[\"None\"])[0]\n",
        "            client.transition_model_version_stage(\n",
        "                name=_BAYES_MODEL_NAME, version=mv.version,\n",
        "                stage=\"Production\", archive_existing_versions=True,\n",
        "            )\n",
        "\n",
        "        return model, idata, run.info.run_id\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if os.path.isdir(\"mlruns\"):\n",
        "        logger.info(\"Removing existing mlruns directory...\")\n",
        "        shutil.rmtree(\"mlruns\")\n",
        "\n",
        "    model, idata, run_id = train_bayes_logreg(debug=True,\n",
        "                                              draws=50,\n",
        "                                              tune=25,\n",
        "                                              run_name=\"breast_cancer_bayes_logreg\",\n",
        "                                              register=True)\n",
        "    summary_df = az.summary(idata, var_names=[\"alpha\", \"beta\"])\n",
        "    print(summary_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/backend/ML/mlops/explainer.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/backend/ML/mlops/explainer.py\n",
        "from __future__ import annotations\n",
        "import os\n",
        "import socket\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from typing import Any, Sequence, Optional\n",
        "from contextlib import closing\n",
        "\n",
        "import mlflow\n",
        "import psutil  # lightweight; already added to pyproject deps\n",
        "from sklearn.utils.multiclass import type_of_target\n",
        "from explainerdashboard import (\n",
        "    ClassifierExplainer,\n",
        "    RegressionExplainer,\n",
        "    ExplainerDashboard,\n",
        ")\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "__all__ = [\"build_and_log_dashboard\", \"load_dashboard_yaml\", \"dashboard_best_run\", \"_first_free_port\", \"_port_details\"]\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "def _port_details(port: int) -> str:\n",
        "    \"\"\"\n",
        "    Return a one-line string with PID & cmdline of the process\n",
        "    listening on *port*, or '' if none / not discoverable.\n",
        "    \"\"\"\n",
        "    for c in psutil.net_connections(kind=\"tcp\"):\n",
        "        if c.status == psutil.CONN_LISTEN and c.laddr and c.laddr.port == port:\n",
        "            try:\n",
        "                p = psutil.Process(c.pid)\n",
        "                return f\"[PID {p.pid} – {p.name()}] cmd={p.cmdline()}\"\n",
        "            except psutil.Error:\n",
        "                return f\"[PID {c.pid}] (no detail)\"\n",
        "    return \"\"\n",
        "\n",
        "def _first_free_port(start: int = 8050, tries: int = 50) -> int:\n",
        "    \"\"\"Return first free TCP port ≥ *start* on localhost.\"\"\"\n",
        "    for port in range(start, start + tries):\n",
        "        try:\n",
        "            with closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as s:\n",
        "                s.settimeout(0.05)\n",
        "                s.bind((\"127.0.0.1\", port))\n",
        "                return port\n",
        "        except OSError:\n",
        "            # Port is in use, try next one\n",
        "            continue\n",
        "    raise RuntimeError(\"⚠️  No free ports found in range\")\n",
        "\n",
        "def _next_free_port(start: int = 8050, tries: int = 50) -> int:\n",
        "    \"\"\"Return the first free TCP port ≥ *start*. (Alias for backward compatibility)\"\"\"\n",
        "    return _first_free_port(start, tries)\n",
        "\n",
        "def _port_in_use(port: int) -> bool:\n",
        "    \"\"\"Check if a port is already in use on any interface.\"\"\"\n",
        "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "        s.settimeout(0.05)\n",
        "        # Check both localhost and 0.0.0.0 to be thorough\n",
        "        try:\n",
        "            # First check localhost (127.0.0.1)\n",
        "            if s.connect_ex((\"127.0.0.1\", port)) == 0:\n",
        "                return True\n",
        "            # Also check if anything is bound to all interfaces\n",
        "            if s.connect_ex((\"0.0.0.0\", port)) == 0:\n",
        "                return True\n",
        "        except (socket.gaierror, OSError):\n",
        "            # If we can't connect, assume port is free\n",
        "            pass\n",
        "        return False\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# -------------------------------------------------------------- #\n",
        "#  src/mlops/explainer.py (only this function changed)           #\n",
        "# -------------------------------------------------------------- #\n",
        "def build_and_log_dashboard(\n",
        "    model: Any,\n",
        "    X_test,\n",
        "    y_test,\n",
        "    *,\n",
        "    # ---- explainer kwargs (unchanged) -------------------------\n",
        "    cats: Optional[Sequence[str]] = None,\n",
        "    idxs: Optional[Sequence[Any]] = None,\n",
        "    descriptions: Optional[dict[str, str]] = None,\n",
        "    target: Optional[str] = None,\n",
        "    labels: Optional[Sequence[str]] = None,\n",
        "    X_background=None,\n",
        "    model_output: str = \"probability\",\n",
        "    shap: str = \"guess\",\n",
        "    shap_interaction: bool = True,\n",
        "    simple: bool = False,\n",
        "    mode: str = \"dash\",         # 🆕 safest default for docker\n",
        "    title: str = \"Model Explainer\",\n",
        "    # ---- infra -----------------------------------------------\n",
        "    run: mlflow.ActiveRun | None = None,\n",
        "    port: int | None = None,\n",
        "    serve: bool = False,\n",
        "    server_backend: str = \"waitress\",   # 🆕 waitress|gunicorn|jupyterdash\n",
        "    conflict_strategy: str = \"next\",\n",
        "    max_tries: int = 20,\n",
        "    save_yaml: bool = True,\n",
        "    output_dir: os.PathLike | str | None = None,\n",
        ") -> Path:\n",
        "    \"\"\"\n",
        "    Build + (optionally) serve the dashboard.\n",
        "\n",
        "    server_backend\n",
        "        'waitress'    – production WSGI server (binds 0.0.0.0)\n",
        "        'gunicorn'    – spawn via subprocess (needs gunicorn installed)\n",
        "        'jupyterdash' – fallback; use only for notebook demos\n",
        "    \"\"\"\n",
        "    # ------------ build explainer (unchanged) ------------------\n",
        "    problem = type_of_target(y_test)\n",
        "    ExplainerCls = RegressionExplainer if problem.startswith(\"continuous\") else ClassifierExplainer\n",
        "    expl_kwargs = dict(\n",
        "        cats=cats, idxs=idxs, descriptions=descriptions, target=target,\n",
        "        labels=labels, X_background=X_background, model_output=model_output, shap=shap,\n",
        "    )\n",
        "    expl_kwargs = {k: v for k, v in expl_kwargs.items() if v is not None}\n",
        "    explainer = ExplainerCls(model, X_test, y_test, **expl_kwargs)\n",
        "\n",
        "    dash = ExplainerDashboard(\n",
        "        explainer, title=title, shap_interaction=shap_interaction,\n",
        "        simple=simple, mode=mode,\n",
        "    )\n",
        "\n",
        "    out_dir = Path(output_dir or \".\"); out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    html_path = out_dir / \"explainer_dashboard.html\"; dash.save_html(html_path); mlflow.log_artifact(str(html_path))\n",
        "    if save_yaml:\n",
        "        yaml = out_dir / \"dashboard.yaml\"; dash.to_yaml(yaml); mlflow.log_artifact(str(yaml))\n",
        "\n",
        "    # ------------ serve ----------------------------------------\n",
        "    if not serve:\n",
        "        return html_path\n",
        "\n",
        "    chosen = port or _first_free_port()\n",
        "    attempts = 0\n",
        "    while _port_in_use(chosen):\n",
        "        if conflict_strategy == \"raise\":\n",
        "            raise RuntimeError(f\"Port {chosen} in use {_port_details(chosen)}\")\n",
        "        if conflict_strategy == \"kill\":\n",
        "            pid = int((_port_details(chosen) or \"PID 0\").split()[1]); psutil.Process(pid).terminate()\n",
        "            break\n",
        "        attempts += 1\n",
        "        if attempts >= max_tries:\n",
        "            raise RuntimeError(f\"No free port after {max_tries} tries\")\n",
        "        chosen += 1\n",
        "\n",
        "    logging.info(\"🌐 Dashboard on http://0.0.0.0:%s via %s\", chosen, server_backend)\n",
        "\n",
        "    if server_backend == \"waitress\":\n",
        "        dash.run(chosen, host=\"0.0.0.0\", use_waitress=True, mode=\"dash\")\n",
        "    elif server_backend == \"gunicorn\":\n",
        "        import subprocess, shlex\n",
        "        cmd = f\"gunicorn -w 3 -b 0.0.0.0:{chosen} dashboard:app\"\n",
        "        subprocess.Popen(shlex.split(cmd), cwd=str(out_dir))\n",
        "    else:  # jupyterdash\n",
        "        dash.run(chosen, host=\"0.0.0.0\")\n",
        "\n",
        "    return html_path\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "def load_dashboard_yaml(path: os.PathLike | str) -> ExplainerDashboard:\n",
        "    \"\"\"Reload a YAML config – unchanged but kept for public API.\"\"\"\n",
        "    return ExplainerDashboard.from_config(path)\n",
        "\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "def dashboard_best_run(metric: str = \"accuracy\",\n",
        "                       maximize: bool = True,\n",
        "                       *, port: int | None = None) -> None:\n",
        "    \"\"\"\n",
        "    Load the *best* run (by `metric`) from the active experiment and\n",
        "    launch an ExplainerDashboard **once** for that model.\n",
        "\n",
        "    Example\n",
        "    -------\n",
        "    >>> from mlops.explainer import dashboard_best_run\n",
        "    >>> dashboard_best_run(\"accuracy\")      # opens http://0.0.0.0:8050\n",
        "    \"\"\"\n",
        "    from .experiment_utils import get_best_run\n",
        "    from .model_registry  import load_model_from_run\n",
        "    from sklearn.datasets import load_iris\n",
        "    import pandas as pd\n",
        "\n",
        "    best = get_best_run(metric_key=metric, maximize=maximize)\n",
        "    run_id = best[\"run_id\"]\n",
        "    model  = load_model_from_run(run_id)\n",
        "\n",
        "    iris = load_iris()\n",
        "    X_df  = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "    build_and_log_dashboard(\n",
        "        model, X_df, iris.target,\n",
        "        labels=list(iris.target_names),\n",
        "        run=None, serve=True, port=port or 8050\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/backend/ML/mlops/utils.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/backend/ML/mlops/utils.py\n",
        "import os\n",
        "# Add near the top of utils.py\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import inspect\n",
        "\n",
        "def add_project_root_to_sys_path(levels_up: int = 2) -> Path:\n",
        "    \"\"\"\n",
        "    Ensure the repository root (default: two directories up) is on sys.path.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Path\n",
        "        The absolute Path object pointing to the directory inserted.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        here = Path(__file__).resolve()\n",
        "    except NameError:           # running in Jupyter / IPython\n",
        "        # Use the file of the *caller* if possible,\n",
        "        # otherwise fall back to the current working directory.\n",
        "        caller = inspect.stack()[1].filename\n",
        "        here = Path(caller).resolve() if caller != \"<stdin>\" else Path.cwd()\n",
        "\n",
        "    root = here.parents[levels_up]\n",
        "    sys.path.insert(0, str(root))\n",
        "    return root\n",
        "\n",
        "\n",
        "_added_src_flag: bool = False          # module-level cache\n",
        "\n",
        "def project_root() -> Path:\n",
        "    \"\"\"\n",
        "    Return the absolute path to the repo root *without* relying on __file__.\n",
        "\n",
        "    • If running from a .py file, use that file's parent/parent (…/src/..)\n",
        "    • If running interactively (no __file__), fall back to CWD.\n",
        "    \"\"\"\n",
        "    if \"__file__\" in globals():\n",
        "        return Path(__file__).resolve().parent.parent\n",
        "    return Path.cwd()\n",
        "\n",
        "def ensure_src_on_path(verbose: bool = True) -> None:\n",
        "    \"\"\"\n",
        "    Ensure <repo-root>/src is the *first* entry in sys.path exactly once.\n",
        "    The verbose flag prints the helper line the first time only.\n",
        "    \"\"\"\n",
        "    import sys\n",
        "    global _added_src_flag\n",
        "    root = project_root()\n",
        "    src_path = root / \"src\"\n",
        "\n",
        "    if str(src_path) not in sys.path:\n",
        "        sys.path.insert(0, str(src_path))\n",
        "        if verbose and not _added_src_flag:\n",
        "            print(f\"🔧 Added {src_path} to sys.path\")\n",
        "        _added_src_flag = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/backend/ML/mlops/shapiq_utils.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/backend/ML/mlops/shapiq_utils.py\n",
        "\"\"\"\n",
        "SHAP-IQ (Shapley Interaction) utilities for MLflow integration.\n",
        "\n",
        "This module provides functions to compute and log Shapley interaction values\n",
        "for machine learning models. Shapley interactions help understand how features\n",
        "work together to influence model predictions.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import mlflow\n",
        "from shapiq import TabularExplainer\n",
        "from typing import Optional, Sequence, Union\n",
        "import logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\",\n",
        "                        message=\"Not all budget is required due to the border-trick\",\n",
        "                        category=UserWarning,\n",
        "                        module=r\"^shapiq\\.\")\n",
        "\n",
        "\n",
        "def compute_shapiq_interactions(\n",
        "    model,\n",
        "    X: pd.DataFrame,\n",
        "    feature_names: Sequence[str],\n",
        "    max_order: int = 2,\n",
        "    budget: int = 256,\n",
        "    n_samples: Optional[int] = None,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Robust wrapper around shapiq.TabularExplainer to return a tidy DataFrame\n",
        "    with Shapley-interaction values.  Handles the two public APIs:\n",
        "      •  .dict_values   (mapping)\n",
        "      •  .values        (np.ndarray)  →  use  .to_dict()\n",
        "    \"\"\"\n",
        "    logger.info(\n",
        "        \"Computing SHAP-IQ (max_order=%s, budget=%s, n_samples=%s)\",\n",
        "        max_order,\n",
        "        budget,\n",
        "        n_samples,\n",
        "    )\n",
        "\n",
        "    X_sample = (\n",
        "        X.sample(n=n_samples, random_state=42) if n_samples and len(X) > n_samples else X\n",
        "    )\n",
        "\n",
        "    explainer = TabularExplainer(\n",
        "        model=model,\n",
        "        data=X_sample.values,\n",
        "        index=\"k-SII\",\n",
        "        max_order=max_order,\n",
        "    )\n",
        "\n",
        "    rows: list[dict[str, Any]] = []\n",
        "    for i, vec in enumerate(X_sample.values):\n",
        "        try:\n",
        "            iv = explainer.explain(vec, budget=budget)\n",
        "\n",
        "            # --- unify both APIs ------------------------------------------------\n",
        "            if hasattr(iv, \"dict_values\"):                    # shapiq ≥ 0.4\n",
        "                items = iv.dict_values.items()\n",
        "            elif hasattr(iv, \"to_dict\"):                      # fallback\n",
        "                items = iv.to_dict().items()\n",
        "            else:\n",
        "                # last resort – try attribute access\n",
        "                items = dict(iv.values).items()\n",
        "\n",
        "            for combo, val in items:\n",
        "                rows.append(\n",
        "                    {\n",
        "                        \"sample_idx\": i,\n",
        "                        \"combination\": combo,\n",
        "                        \"value\": float(val),\n",
        "                        \"order\": len(combo),\n",
        "                        \"feature_names\": tuple(feature_names[j] for j in combo)\n",
        "                        if combo\n",
        "                        else (),\n",
        "                    }\n",
        "                )\n",
        "        except Exception as exc:  # noqa: BLE001\n",
        "            logger.warning(\"SHAP-IQ failed on sample %s: %s\", i, exc)\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    logger.info(\"✓ %s interaction rows computed\", len(df))\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "def log_shapiq_interactions(\n",
        "    model,\n",
        "    X: pd.DataFrame,\n",
        "    feature_names: Sequence[str],\n",
        "    max_order: int = 2,\n",
        "    top_n: int = 10,\n",
        "    budget: int = 256,\n",
        "    n_samples: Optional[int] = None,\n",
        "    output_path: Optional[str] = None\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Compute Shapley interaction values and log them to MLflow.\n",
        "\n",
        "    This function:\n",
        "    1. Computes interactions using compute_shapiq_interactions\n",
        "    2. Logs the top N interactions as MLflow metrics\n",
        "    3. Saves the full interaction table as CSV and logs as artifact\n",
        "\n",
        "    Args:\n",
        "        model: Trained sklearn-like model.\n",
        "        X: DataFrame of features.\n",
        "        feature_names: List of feature column names.\n",
        "        max_order: Maximum interaction order (default: 2).\n",
        "        top_n: Number of top interactions to log as metrics (default: 10).\n",
        "        budget: Evaluation budget for interaction approximation (default: 256).\n",
        "        n_samples: If provided, sample this many rows for computation.\n",
        "        output_path: Optional path for CSV output (default: \"shapiq_interactions.csv\").\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting SHAP-IQ interaction logging\")\n",
        "\n",
        "    # Compute interactions\n",
        "    df = compute_shapiq_interactions(\n",
        "        model, X, feature_names, max_order, budget, n_samples\n",
        "    )\n",
        "\n",
        "    if df.empty:\n",
        "        logger.warning(\"No interactions computed - skipping logging\")\n",
        "        return\n",
        "\n",
        "    # Aggregate: mean absolute value per combination across all samples\n",
        "    agg = (\n",
        "        df.groupby(['combination', 'feature_names', 'order'])['value']\n",
        "          .apply(lambda x: x.abs().mean())\n",
        "          .reset_index()\n",
        "          .sort_values('value', ascending=False)\n",
        "    )\n",
        "\n",
        "    # Log summary statistics\n",
        "    mlflow.log_metric(\"shapiq_total_interactions\", len(df))\n",
        "    mlflow.log_metric(\"shapiq_unique_combinations\", len(agg))\n",
        "    mlflow.log_metric(\"shapiq_max_order\", max_order)\n",
        "    mlflow.log_metric(\"shapiq_samples_analyzed\", len(X) if n_samples is None else min(n_samples, len(X)))\n",
        "\n",
        "    # Log top N interactions as metrics\n",
        "    logger.info(f\"Logging top {top_n} interactions as MLflow metrics\")\n",
        "    for idx, row in agg.head(top_n).iterrows():\n",
        "        combo = row['combination']\n",
        "        feature_combo = row['feature_names']\n",
        "        value = row['value']\n",
        "        order = row['order']\n",
        "\n",
        "        # Create metric name from feature names or indices\n",
        "        if feature_combo:\n",
        "            name = f\"shapiq_order{order}_{'_x_'.join(feature_combo)}\"\n",
        "        else:\n",
        "            name = f\"shapiq_order{order}_{'_'.join(map(str, combo))}\"\n",
        "\n",
        "        # Sanitize metric name (MLflow has restrictions)\n",
        "        name = name.replace(' ', '_').replace('(', '').replace(')', '').replace(',', '_')[:250]\n",
        "\n",
        "        mlflow.log_metric(name, float(value))\n",
        "\n",
        "    # Log order-specific summaries\n",
        "    order_summary = df.groupby('order')['value'].agg(['count', 'mean', 'std']).fillna(0)\n",
        "    for order_val in order_summary.index:\n",
        "        mlflow.log_metric(f\"shapiq_order{order_val}_count\", order_summary.loc[order_val, 'count'])\n",
        "        mlflow.log_metric(f\"shapiq_order{order_val}_mean_abs\", abs(order_summary.loc[order_val, 'mean']))\n",
        "        if order_summary.loc[order_val, 'std'] > 0:\n",
        "            mlflow.log_metric(f\"shapiq_order{order_val}_std\", order_summary.loc[order_val, 'std'])\n",
        "\n",
        "    # Save and log full DataFrame as artifact\n",
        "    output_file = output_path or \"shapiq_interactions.csv\"\n",
        "\n",
        "    try:\n",
        "        # Add readable feature names to the full DataFrame\n",
        "        df_export = df.copy()\n",
        "        df_export['feature_names_str'] = df_export['feature_names'].apply(lambda x: ' x '.join(x) if x else 'baseline')\n",
        "\n",
        "        df_export.to_csv(output_file, index=False)\n",
        "        mlflow.log_artifact(output_file)\n",
        "        logger.info(f\"Logged SHAP-IQ interactions artifact: {output_file}\")\n",
        "\n",
        "        # Also create and log a summary file\n",
        "        summary_file = output_path.replace('.csv', '_summary.csv') if output_path else \"shapiq_interactions_summary.csv\"\n",
        "        agg_export = agg.copy()\n",
        "        agg_export['feature_names_str'] = agg_export['feature_names'].apply(lambda x: ' x '.join(x) if x else 'baseline')\n",
        "        agg_export.to_csv(summary_file, index=False)\n",
        "        mlflow.log_artifact(summary_file)\n",
        "        logger.info(f\"Logged SHAP-IQ summary artifact: {summary_file}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error saving SHAP-IQ artifacts: {e}\")\n",
        "\n",
        "    logger.info(\"SHAP-IQ interaction logging completed\")\n",
        "\n",
        "\n",
        "def get_top_interactions(\n",
        "    shapiq_df: pd.DataFrame,\n",
        "    top_n: int = 10,\n",
        "    order: Optional[int] = None\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Extract top interactions from a SHAP-IQ DataFrame.\n",
        "\n",
        "    Args:\n",
        "        shapiq_df: DataFrame returned by compute_shapiq_interactions.\n",
        "        top_n: Number of top interactions to return.\n",
        "        order: If provided, filter to interactions of this order only.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with top interactions, aggregated across samples.\n",
        "    \"\"\"\n",
        "    df = shapiq_df.copy()\n",
        "\n",
        "    if order is not None:\n",
        "        df = df[df['order'] == order]\n",
        "\n",
        "    if df.empty:\n",
        "        return df\n",
        "\n",
        "    # Aggregate and sort by absolute mean value\n",
        "    agg = (\n",
        "        df.groupby(['combination', 'feature_names', 'order'])['value']\n",
        "          .agg(['mean', 'std', 'count'])\n",
        "          .reset_index()\n",
        "    )\n",
        "    agg['abs_mean'] = agg['mean'].abs()\n",
        "    agg = agg.sort_values('abs_mean', ascending=False)\n",
        "\n",
        "    return agg.head(top_n)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/backend/ML/examples/shapiq_demo.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/backend/ML/examples/shapiq_demo.py\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "SHAP-IQ Integration Demo\n",
        "\n",
        "This script demonstrates the new SHAP-IQ (Shapley Interaction) functionality\n",
        "integrated into the MLOps pipeline. It shows how Shapley interaction values\n",
        "are computed and logged alongside regular model metrics.\n",
        "\n",
        "Usage:\n",
        "    python src/examples/shapiq_demo.py\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "import logging\n",
        "\n",
        "# ─── Path setup ─────────────────────────────────────────────────────────────\n",
        "from src.backend.ML.mlops.utils import add_project_root_to_sys_path\n",
        "PROJECT_ROOT = add_project_root_to_sys_path(levels_up=2)  # safe in both .py and interactive :contentReference[oaicite:8]{index=8}\n",
        "\n",
        "# ─── Imports ────────────────────────────────────────────────────────────────\n",
        "from src.backend.ML.mlops.training import (\n",
        "    load_and_prepare_iris_data,\n",
        "    train_logistic_regression,\n",
        "    train_random_forest_optimized\n",
        ")\n",
        "from src.backend.ML.mlops.shapiq_utils import (\n",
        "    compute_shapiq_interactions,\n",
        "    log_shapiq_interactions,\n",
        "    get_top_interactions\n",
        ")\n",
        "from src.backend.ML.mlops.experiment_utils import setup_mlflow_experiment, get_best_run\n",
        "import mlflow\n",
        "import pandas as pd\n",
        "\n",
        "# ─── Logging Setup ─────────────────────────────────────────────────────────\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def demo_standalone_shapiq():\n",
        "    \"\"\"Demonstrate standalone SHAP-IQ computation without MLflow logging.\"\"\"\n",
        "    print(\"🔬 SHAP-IQ Standalone Demo\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Load data and train a simple model\n",
        "    X_train, X_test, y_train, y_test, feature_names, target_names, _ = load_and_prepare_iris_data()\n",
        "\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    model = RandomForestClassifier(n_estimators=20, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    print(f\"✓ Trained RandomForest on {len(X_train)} samples\")\n",
        "    print(f\"✓ Test accuracy: {model.score(X_test, y_test):.3f}\")\n",
        "\n",
        "    # Compute SHAP-IQ interactions\n",
        "    X_test_df = pd.DataFrame(X_test, columns=feature_names)\n",
        "    print(f\"\\n🧮 Computing SHAP-IQ interactions...\")\n",
        "\n",
        "    shapiq_df = compute_shapiq_interactions(\n",
        "        model,\n",
        "        X_test_df.head(10),  # Use subset for demo\n",
        "        feature_names,\n",
        "        max_order=2,\n",
        "        budget=128\n",
        "    )\n",
        "\n",
        "    if not shapiq_df.empty:\n",
        "        print(f\"✓ Computed {len(shapiq_df)} interaction values\")\n",
        "\n",
        "        # Show top interactions\n",
        "        top_interactions = get_top_interactions(shapiq_df, top_n=5)\n",
        "        print(f\"\\n🏆 Top 5 Feature Interactions:\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        for idx, row in top_interactions.iterrows():\n",
        "            feature_combo = ' × '.join(row['feature_names'])\n",
        "            if not feature_combo:\n",
        "                feature_combo = \"baseline\"\n",
        "            print(f\"  {feature_combo:30} | Order {row['order']} | {row['abs_mean']:.4f}\")\n",
        "\n",
        "        # Show order breakdown\n",
        "        order_counts = shapiq_df['order'].value_counts().sort_index()\n",
        "        print(f\"\\n📊 Interaction Order Breakdown:\")\n",
        "        for order, count in order_counts.items():\n",
        "            if order == 0:\n",
        "                print(f\"  Order {order} (main effects):     {count:4d} values\")\n",
        "            elif order == 1:\n",
        "                print(f\"  Order {order} (individual):       {count:4d} values\")\n",
        "            elif order == 2:\n",
        "                print(f\"  Order {order} (pairwise):         {count:4d} values\")\n",
        "            else:\n",
        "                print(f\"  Order {order} (higher-order):     {count:4d} values\")\n",
        "    else:\n",
        "        print(\"⚠️  No interactions computed (this can happen with simple models/data)\")\n",
        "\n",
        "\n",
        "def demo_integrated_training():\n",
        "    \"\"\"Demonstrate SHAP-IQ integration in the training pipeline.\"\"\"\n",
        "    print(\"\\n\\n🚀 SHAP-IQ Integrated Training Demo\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Setup MLflow experiment\n",
        "    setup_mlflow_experiment(\"shapiq_demo\")\n",
        "\n",
        "    # Load data\n",
        "    X_train, X_test, y_train, y_test, feature_names, target_names, _ = load_and_prepare_iris_data()\n",
        "    print(f\"✓ Loaded Iris dataset: {len(X_train)} train, {len(X_test)} test samples\")\n",
        "\n",
        "    # Train model with SHAP-IQ integration\n",
        "    print(f\"\\n🤖 Training Logistic Regression with SHAP-IQ...\")\n",
        "    lr_run_id = train_logistic_regression(\n",
        "        X_train, y_train, X_test, y_test,\n",
        "        feature_names, target_names,\n",
        "        run_name=\"lr_with_shapiq\"\n",
        "    )\n",
        "    print(f\"✓ Logistic Regression complete: {lr_run_id[:8]}\")\n",
        "\n",
        "    print(f\"\\n🌲 Training Random Forest with SHAP-IQ...\")\n",
        "    rf_run_id = train_random_forest_optimized(\n",
        "        X_train, y_train, X_test, y_test,\n",
        "        feature_names, target_names,\n",
        "        n_trials=10,  # Reduced for demo\n",
        "        run_name=\"rf_with_shapiq\"\n",
        "    )\n",
        "    print(f\"✓ Random Forest complete: {rf_run_id[:8]}\")\n",
        "\n",
        "    # Show logged SHAP-IQ metrics\n",
        "    print(f\"\\n📊 SHAP-IQ Metrics from MLflow:\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    try:\n",
        "        # Get the latest run (Random Forest)\n",
        "        with mlflow.start_run(run_id=rf_run_id):\n",
        "            run_data = mlflow.get_run(rf_run_id)\n",
        "            metrics = run_data.data.metrics\n",
        "\n",
        "            # Filter SHAP-IQ metrics\n",
        "            shapiq_metrics = {k: v for k, v in metrics.items() if k.startswith('shapiq_')}\n",
        "\n",
        "            if shapiq_metrics:\n",
        "                print(f\"Found {len(shapiq_metrics)} SHAP-IQ metrics:\")\n",
        "                for metric, value in sorted(shapiq_metrics.items()):\n",
        "                    if 'order' in metric and 'count' not in metric:\n",
        "                        print(f\"  {metric:35} = {value:.6f}\")\n",
        "                    elif 'total' in metric or 'unique' in metric or 'max' in metric:\n",
        "                        print(f\"  {metric:35} = {int(value)}\")\n",
        "            else:\n",
        "                print(\"  No SHAP-IQ metrics found (may take longer to compute)\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  Error retrieving metrics: {e}\")\n",
        "\n",
        "    # Compare models\n",
        "    print(f\"\\n🏆 Comparing Models:\")\n",
        "    print(\"-\" * 30)\n",
        "    try:\n",
        "        best_run = get_best_run(\"accuracy\", maximize=True)\n",
        "        run_id = best_run[\"run_id\"]\n",
        "        accuracy = best_run.get(\"metrics.accuracy\", \"N/A\")\n",
        "        print(f\"Best model: {run_id[:8]} (accuracy: {accuracy})\")\n",
        "\n",
        "        # Check if SHAP-IQ metrics are available for best model\n",
        "        shapiq_count = best_run.get(\"metrics.shapiq_total_interactions\")\n",
        "        if shapiq_count:\n",
        "            print(f\"SHAP-IQ interactions: {int(shapiq_count)} computed\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error comparing models: {e}\")\n",
        "\n",
        "\n",
        "def demo_manual_shapiq_logging():\n",
        "    \"\"\"Demonstrate manual SHAP-IQ logging outside of training.\"\"\"\n",
        "    print(f\"\\n\\n🔧 Manual SHAP-IQ Logging Demo\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Load data and train model\n",
        "    X_train, X_test, y_train, y_test, feature_names, target_names, _ = load_and_prepare_iris_data()\n",
        "\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Manual MLflow run with SHAP-IQ logging\n",
        "    setup_mlflow_experiment(\"shapiq_demo\")\n",
        "\n",
        "    with mlflow.start_run(run_name=\"manual_shapiq_demo\"):\n",
        "        # Log basic metrics\n",
        "        accuracy = model.score(X_test, y_test)\n",
        "        mlflow.log_metric(\"accuracy\", accuracy)\n",
        "\n",
        "        # Log SHAP-IQ interactions\n",
        "        X_test_df = pd.DataFrame(X_test, columns=feature_names)\n",
        "        print(\"Computing and logging SHAP-IQ interactions...\")\n",
        "\n",
        "        log_shapiq_interactions(\n",
        "            model,\n",
        "            X_test_df,\n",
        "            feature_names,\n",
        "            max_order=2,\n",
        "            top_n=5,\n",
        "            budget=64,\n",
        "            n_samples=15  # Sample for faster computation\n",
        "        )\n",
        "\n",
        "        current_run = mlflow.active_run()\n",
        "        print(f\"✓ SHAP-IQ logged to run: {current_run.info.run_id[:8]}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Run all SHAP-IQ demos.\"\"\"\n",
        "    print(\"🌟 SHAP-IQ Integration Demonstration\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"This demo shows how Shapley interactions are computed and logged\")\n",
        "    print(\"in the MLOps pipeline to understand feature interactions.\")\n",
        "    print()\n",
        "\n",
        "    try:\n",
        "        # Demo 1: Standalone computation\n",
        "        demo_standalone_shapiq()\n",
        "\n",
        "        # Demo 2: Integrated training\n",
        "        demo_integrated_training()\n",
        "\n",
        "        # Demo 3: Manual logging\n",
        "        demo_manual_shapiq_logging()\n",
        "\n",
        "        print(f\"\\n\\n🎉 SHAP-IQ Demo Complete!\")\n",
        "        print(\"=\" * 60)\n",
        "        print(\"✓ Standalone SHAP-IQ computation\")\n",
        "        print(\"✓ Integrated training with automatic SHAP-IQ logging\")\n",
        "        print(\"✓ Manual SHAP-IQ logging\")\n",
        "        print()\n",
        "        print(\"🔍 Check MLflow UI to see logged SHAP-IQ metrics and artifacts:\")\n",
        "        print(\"   - Metrics: shapiq_order1_*, shapiq_order2_*, etc.\")\n",
        "        print(\"   - Artifacts: shapiq_interactions.csv, shapiq_interactions_summary.csv\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Demo failed: {e}\")\n",
        "        print(f\"\\n❌ Demo failed: {e}\")\n",
        "        print(\"This might be due to SHAP-IQ dependency issues or data problems.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/backend/ML/examples/select_best_and_dashboard.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/backend/ML/examples/select_best_and_dashboard.py\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Select best model and launch dashboard (training is done elsewhere).\n",
        "\n",
        "Usage:\n",
        "    python src/examples/select_best_and_dashboard.py\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "from src.backend.ML.mlops.utils import add_project_root_to_sys_path\n",
        "PROJECT_ROOT = add_project_root_to_sys_path()\n",
        "\n",
        "from src.backend.ML.mlops.experiment_utils import get_best_run\n",
        "from src.backend.ML.mlops.model_registry import load_model_from_run\n",
        "from src.backend.ML.mlops.explainer import dashboard_best_run\n",
        "\n",
        "# Configuration variables\n",
        "METRIC = \"accuracy\"  # Metric to optimize (e.g., 'accuracy', 'f1')\n",
        "PORT = 8050           # Port for the dashboard\n",
        "MAXIMIZE = True       # Whether to maximize (True) or minimize (False) the metric\n",
        "\n",
        "def main() -> None:\n",
        "    print(f\"🔍 Searching MLflow runs by {METRIC}…\")\n",
        "\n",
        "    # Retrieve the best run based on the specified metric\n",
        "    best = get_best_run(metric_key=METRIC, maximize=MAXIMIZE)\n",
        "    run_id = best[\"run_id\"]\n",
        "    score = best.get(f\"metrics.{METRIC}\", \"N/A\")\n",
        "\n",
        "    print(f\"🏆 Best run: {run_id[:8]} — {METRIC}: {score}\")\n",
        "\n",
        "    # Load the model from the run registry\n",
        "    model = load_model_from_run(run_id)\n",
        "    if model is None:\n",
        "        raise RuntimeError(\"Model could not be loaded from registry\")\n",
        "\n",
        "    print(\"✓ Model loaded – launching dashboard\")\n",
        "    # Launch the explainer dashboard for the best model\n",
        "    dashboard_best_run(METRIC, maximize=MAXIMIZE, port=PORT)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/backend/ML/scripts/run_training.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/backend/ML/scripts/run_training.py\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Simple training runner script.\n",
        "\n",
        "Run with:\n",
        "    python src/backend/ML/scripts/run_training.py\n",
        "    # or inside Jupyter:\n",
        "    %run src/backend/ML/scripts/run_training.py\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import logging\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import mlflow\n",
        "from mlflow.tracking import MlflowClient\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Add project root to Python path\n",
        "project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), \"../../../..\"))\n",
        "sys.path.insert(0, project_root)\n",
        "\n",
        "from src.backend.ML.mlops.training import run_all_trainings\n",
        "from src.backend.ML.mlops.training_bayes import train_bayes_logreg\n",
        "from src.backend.ML.mlops.experiment_utils import setup_mlflow_experiment\n",
        "\n",
        "\n",
        "def _is_mount(path: Path) -> bool:\n",
        "    \"\"\"Return True if *path* is a mount point.\"\"\"\n",
        "    try:\n",
        "        return path.is_mount()\n",
        "    except AttributeError:\n",
        "        import os as _os\n",
        "        return _os.path.ismount(path)\n",
        "\n",
        "\n",
        "def clean_mlruns() -> None:\n",
        "    \"\"\"\n",
        "    Remove contents of ./mlruns when it is a mount point, otherwise remove the entire directory.\n",
        "\n",
        "    This avoids 'Device or resource busy' errors when ./mlruns is bind-mounted.\n",
        "    \"\"\"\n",
        "    mlruns_dir = Path(\"mlruns\")\n",
        "    mlruns_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    if _is_mount(mlruns_dir):\n",
        "        logger.warning(\"⚠️  '%s' is a mount-point – deleting contents only\", mlruns_dir)\n",
        "        for child in mlruns_dir.iterdir():\n",
        "            if child.name == \".trash\":\n",
        "                continue  # preserve special folder if present\n",
        "            if child.is_dir():\n",
        "                shutil.rmtree(child, ignore_errors=True)\n",
        "            else:\n",
        "                try:\n",
        "                    child.unlink(missing_ok=True)\n",
        "                except TypeError:\n",
        "                    # Python < 3.8 compatibility\n",
        "                    if child.exists():\n",
        "                        child.unlink()\n",
        "    else:\n",
        "        logger.info(\"🧹 Removing %s entirely\", mlruns_dir)\n",
        "        shutil.rmtree(mlruns_dir, ignore_errors=True)\n",
        "        mlruns_dir.mkdir(exist_ok=True)\n",
        "\n",
        "\n",
        "def promote_to_production(model_name: str) -> None:\n",
        "    \"\"\"Promote *latest* version of <model_name> to Production if it exists.\"\"\"\n",
        "    client = MlflowClient()\n",
        "    versions = client.search_model_versions(f\"name='{model_name}'\")\n",
        "\n",
        "    if not versions:\n",
        "        logger.warning(\"⚠️  No versions for %s – skipping promotion\", model_name)\n",
        "        return  # graceful exit when model not yet registered\n",
        "\n",
        "    # pick numerically highest version (latest)\n",
        "    latest = max(versions, key=lambda v: int(v.version))\n",
        "\n",
        "    # Archive current Production versions\n",
        "    for mv in versions:\n",
        "        if mv.current_stage == \"Production\":\n",
        "            client.transition_model_version_stage(\n",
        "                name=model_name,\n",
        "                version=mv.version,\n",
        "                stage=\"Archived\",\n",
        "            )\n",
        "\n",
        "    client.transition_model_version_stage(\n",
        "        name=model_name,\n",
        "        version=latest.version,\n",
        "        stage=\"Production\",\n",
        "    )\n",
        "    logger.info(\"✅ Promoted %s version %s to Production\", model_name, latest.version)\n",
        "\n",
        "\n",
        "def needs_training(model_name: str, min_acc: float = 0.9) -> bool:\n",
        "    \"\"\"Return True if <model_name> needs (re)training.\n",
        "\n",
        "    Logic:\n",
        "    1. No *Production* version exists ⇒ train.\n",
        "    2. Accuracy metric below *min_acc* ⇒ train.\n",
        "    \"\"\"\n",
        "    client = MlflowClient()\n",
        "    try:\n",
        "        prod_versions = client.get_latest_versions(model_name, stages=[\"Production\"])\n",
        "    except Exception as err:\n",
        "        logger.warning(\"⚠️  Could not query model %s – assuming training needed (%s)\", model_name, err)\n",
        "        return True\n",
        "\n",
        "    if not prod_versions:\n",
        "        logger.info(\"ℹ️  No Production version for %s\", model_name)\n",
        "        return True\n",
        "\n",
        "    run_id = prod_versions[0].run_id\n",
        "    run = client.get_run(run_id)\n",
        "    acc = run.data.metrics.get(\"accuracy\", 0.0)\n",
        "    logger.info(\"🔎 %s v%s accuracy=%.3f\", model_name, prod_versions[0].version, acc)\n",
        "    return acc < min_acc\n",
        "\n",
        "\n",
        "def main() -> None:\n",
        "    \"\"\"Run training pipelines only if required.\"\"\"\n",
        "    try:\n",
        "        logger.info(\"🚀 Training pipeline started (cwd=%s)\", Path.cwd())\n",
        "\n",
        "        # Clean MLflow runs only in CI to ensure reproducibility\n",
        "        if os.getenv(\"CI\") == \"1\":\n",
        "            clean_mlruns()\n",
        "\n",
        "        # Ensure experiment / tracking URI\n",
        "        setup_mlflow_experiment()\n",
        "\n",
        "        # 1️⃣  Iris models --------------------------------------------------\n",
        "        if needs_training(\"iris_random_forest\") or needs_training(\"iris_logreg\"):\n",
        "            logger.info(\"🛠  Training Iris models…\")\n",
        "            run_all_trainings(n_trials=5)\n",
        "            promote_to_production(\"iris_random_forest\")\n",
        "            promote_to_production(\"iris_logreg\")\n",
        "        else:\n",
        "            logger.info(\"✅ Iris models meet accuracy threshold – skipping training\")\n",
        "\n",
        "        # 2️⃣  Breast Cancer Bayesian model -------------------------------\n",
        "        if needs_training(\"breast_cancer_bayes\", min_acc=0.93):\n",
        "            logger.info(\"🛠  Training Breast Cancer Bayesian model…\")\n",
        "            train_bayes_logreg(draws=50, tune=25)\n",
        "            promote_to_production(\"breast_cancer_bayes\")\n",
        "        else:\n",
        "            logger.info(\"✅ Cancer model meets accuracy threshold – skipping training\")\n",
        "\n",
        "        logger.info(\"🏁 Training script completed successfully\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(\"❌ Training failed: %s\", e)\n",
        "        sys.exit(1)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/backend/ML/examples/iris_classification_example.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/backend/ML/examples/iris_classification_example.py\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Iris Classification Example (argparse-free, notebook-safe).\n",
        "\n",
        "Configuration:\n",
        "    • export EXPLAINER_DASHBOARD=1   # launch dashboard\n",
        "    • export EXPLAINER_PORT=8150     # optional port override\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "import os\n",
        "import logging\n",
        "\n",
        "from src.backend.ML.mlops.utils import ensure_src_on_path\n",
        "ensure_src_on_path()\n",
        "\n",
        "from src.backend.ML.mlops.training import (\n",
        "    load_and_prepare_iris_data,\n",
        "    train_logistic_regression,\n",
        "    train_random_forest_optimized,\n",
        "    compare_models,\n",
        ")\n",
        "from src.backend.ML.mlops.model_registry import load_model_from_run\n",
        "from src.backend.ML.mlops.experiment_utils import get_best_run\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "\n",
        "def _bool_env(var: str, default: bool = False) -> bool:\n",
        "    v = os.getenv(var)\n",
        "    return default if v is None else v.lower() in {\"1\", \"true\", \"yes\"}\n",
        "\n",
        "\n",
        "def main(*, dashboard: bool = False, dashboard_port: int | None = None) -> None:\n",
        "    print(\"🌸 Iris Classification with MLflow\\n\" + \"=\" * 50)\n",
        "\n",
        "    # 1 Load data ------------------------------------------------------------\n",
        "    X_train, X_test, y_train, y_test, feat_names, tgt_names, _ = (\n",
        "        load_and_prepare_iris_data()\n",
        "    )\n",
        "    print(f\"✓ Training samples: {len(X_train)} | Test: {len(X_test)}\")\n",
        "\n",
        "    # 2 Logistic Regression --------------------------------------------------\n",
        "    lr_run = train_logistic_regression(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        X_test,\n",
        "        y_test,\n",
        "        feat_names,\n",
        "        tgt_names,\n",
        "        run_name=\"lr_baseline\",\n",
        "        register=True,\n",
        "    )\n",
        "    print(f\"✓ Logistic run {lr_run[:8]}\")\n",
        "\n",
        "    # 3 Random Forest + Optuna ----------------------------------------------\n",
        "    rf_run = train_random_forest_optimized(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        X_test,\n",
        "        y_test,\n",
        "        feat_names,\n",
        "        tgt_names,\n",
        "        n_trials=20,\n",
        "        run_name=\"rf_optimized\",\n",
        "        register=True,\n",
        "    )\n",
        "    print(f\"✓ RF run {rf_run[:8]}\")\n",
        "\n",
        "    # 4 Compare & test best --------------------------------------------------\n",
        "    compare_models()\n",
        "    best = get_best_run()\n",
        "    mdl = load_model_from_run(best[\"run_id\"])\n",
        "    if mdl is not None:\n",
        "        acc = (mdl.predict(X_test) == y_test).mean()\n",
        "        print(f\"🏆 Best model accuracy: {acc:.4f}\")\n",
        "    else:\n",
        "        print(\"❌ Could not load best model\")\n",
        "\n",
        "    if dashboard:\n",
        "        port = dashboard_port or int(os.getenv(\"EXPLAINER_PORT\", \"8050\"))\n",
        "        print(f\"\\n🚀 ExplainerDashboard running on http://localhost:{port}\")\n",
        "        # Import and run dashboard for best model\n",
        "        from src.backend.ML.mlops.explainer import dashboard_best_run\n",
        "        dashboard_best_run(\"accuracy\", port=port)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main(\n",
        "        dashboard=_bool_env(\"EXPLAINER_DASHBOARD\", False),\n",
        "        dashboard_port=int(os.getenv(\"EXPLAINER_PORT\", \"8050\")),\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/backend/ML/model_api/__init__.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/backend/ML/model_api/__init__.py\n",
        "# Model API package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:MLflow tracking URI set to http://mlflow:5000\n",
            "INFO:__main__:Loading Iris Random Forest model...\n",
            "/tmp/ipykernel_32624/252283631.py:151: FutureWarning: ``mlflow.tracking.client.MlflowClient.get_latest_versions`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/latest/model-registry.html#migrating-from-stages\n",
            "  versions = client.get_latest_versions(name, stages=[\"Production\"])\n",
            "INFO:__main__:Loading Iris Logistic Regression model...\n",
            "/tmp/ipykernel_32624/252283631.py:151: FutureWarning: ``mlflow.tracking.client.MlflowClient.get_latest_versions`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/latest/model-registry.html#migrating-from-stages\n",
            "  versions = client.get_latest_versions(name, stages=[\"Production\"])\n",
            "INFO:__main__:Loading Breast Cancer Bayesian model...\n",
            "/tmp/ipykernel_32624/252283631.py:151: FutureWarning: ``mlflow.tracking.client.MlflowClient.get_latest_versions`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/latest/model-registry.html#migrating-from-stages\n",
            "  versions = client.get_latest_versions(name, stages=[\"Production\"])\n",
            "INFO:__main__:✅ All models loaded successfully\n",
            "INFO:httpx:HTTP Request: GET http://testserver/health \"HTTP/1.1 200 OK\"\n",
            "/tmp/ipykernel_32624/252283631.py:303: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
            "  X = pd.DataFrame([r.dict() for r in req.rows])\n",
            "INFO:httpx:HTTP Request: POST http://testserver/predict/iris/rf \"HTTP/1.1 200 OK\"\n",
            "/workspace/src/backend/ML/mlops/training_bayes.py:84: RuntimeWarning: overflow encountered in exp\n",
            "  return (1 / (1 + np.exp(-logits))).mean(0)\n",
            "INFO:httpx:HTTP Request: POST http://testserver/predict/cancer/bayes \"HTTP/1.1 200 OK\"\n",
            "/tmp/ipykernel_32624/252283631.py:418: FutureWarning: ``mlflow.tracking.client.MlflowClient.get_latest_versions`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/latest/model-registry.html#migrating-from-stages\n",
            "  mv = client.get_latest_versions(name, stages=[\"Production\"])[0]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🏥 Testing /health endpoint...\n",
            "Response: {'status': 'healthy', 'models': {'iris_rf': {'loaded': True, 'info': {'version': '1', 'stage': 'Production', 'run_id': '510396fab8ac4cdda4fdf2bc4841fad1', 'creation_timestamp': 1751734398406, 'metrics': {'accuracy': 1.0, 'precision_macro': 1.0, 'recall_macro': 1.0, 'f1_macro': 1.0, 'precision_0': 1.0, 'recall_0': 1.0, 'f1_0': 1.0, 'support_0': 10.0, 'precision_1': 1.0, 'recall_1': 1.0, 'f1_1': 1.0, 'support_1': 9.0, 'precision_2': 1.0, 'recall_2': 1.0, 'f1_2': 1.0, 'support_2': 11.0, 'roc_auc_ovr_weighted': 1.0, 'log_loss': 2.2204460492503136e-16, 'mcc': 1.0, 'best_accuracy': 1.0, 'shapiq_total_interactions': 330.0, 'shapiq_unique_combinations': 11.0, 'shapiq_max_order': 2.0, 'shapiq_samples_analyzed': 30.0, 'shapiq_order0_': 0.29868026676967113, 'shapiq_order1_petal_length_cm': 0.11833661278222699, 'shapiq_order1_petal_width_cm': 0.11678626826884603, 'shapiq_order2_petal_length_cm_x_petal_width_cm': 0.0674760478235455, 'shapiq_order2_sepal_length_cm_x_petal_length_cm': 0.04803468386639531, 'shapiq_order1_sepal_length_cm': 0.042406584006311296, 'shapiq_order2_sepal_length_cm_x_petal_width_cm': 0.03608703170188405, 'shapiq_order1_sepal_width_cm': 0.005651990580745765, 'shapiq_order2_sepal_width_cm_x_petal_width_cm': 0.0032007927346469093, 'shapiq_order2_sepal_length_cm_x_sepal_width_cm': 0.0026457566985656406, 'shapiq_order0_count': 30.0, 'shapiq_order0_mean_abs': 0.2986802667696712, 'shapiq_order1_count': 120.0, 'shapiq_order1_mean_abs': 0.03737265393198579, 'shapiq_order1_std': 0.1102817119540345, 'shapiq_order2_count': 180.0, 'shapiq_order2_mean_abs': 0.024915102496736174, 'shapiq_order2_std': 0.03384884689125578}}}, 'iris_logreg': {'loaded': True, 'info': {'version': '1', 'stage': 'Production', 'run_id': 'f40d38458d104a588a1865e03d102006', 'creation_timestamp': 1751734347701, 'metrics': {'training_precision_score': 0.9674588284344383, 'training_recall_score': 0.9666666666666667, 'training_f1_score': 0.9666666666666666, 'training_accuracy_score': 0.9666666666666667, 'training_log_loss': 0.1454138190998098, 'training_roc_auc': 0.9988534500729623, 'training_score': 0.9666666666666667, 'accuracy': 1.0, 'precision_macro': 1.0, 'recall_macro': 1.0, 'f1_macro': 1.0, 'precision_0': 1.0, 'recall_0': 1.0, 'f1_0': 1.0, 'support_0': 10.0, 'precision_1': 1.0, 'recall_1': 1.0, 'f1_1': 1.0, 'support_1': 9.0, 'precision_2': 1.0, 'recall_2': 1.0, 'f1_2': 1.0, 'support_2': 11.0, 'roc_auc_ovr_weighted': 1.0, 'log_loss': 2.2204460492503136e-16, 'mcc': 1.0, 'shapiq_total_interactions': 330.0, 'shapiq_unique_combinations': 11.0, 'shapiq_max_order': 2.0, 'shapiq_samples_analyzed': 30.0, 'shapiq_order0_': 0.2908711115261618, 'shapiq_order2_petal_length_cm_x_petal_width_cm': 0.24396377853550114, 'shapiq_order1_petal_width_cm': 0.19730590446039611, 'shapiq_order1_petal_length_cm': 0.1825302593935227, 'shapiq_order1_sepal_length_cm': 0.11211002111616612, 'shapiq_order2_sepal_length_cm_x_petal_length_cm': 0.08664115674366109, 'shapiq_order1_sepal_width_cm': 0.05512545866539005, 'shapiq_order2_sepal_length_cm_x_petal_width_cm': 0.04558316031764949, 'shapiq_order2_sepal_width_cm_x_petal_length_cm': 0.03807962045722758, 'shapiq_order2_sepal_width_cm_x_petal_width_cm': 0.028436414218107667, 'shapiq_order0_count': 30.0, 'shapiq_order0_mean_abs': 0.2908711115261618, 'shapiq_order1_count': 120.0, 'shapiq_order1_mean_abs': 0.1091251454889626, 'shapiq_order1_std': 0.12517039789945875, 'shapiq_order2_count': 180.0, 'shapiq_order2_mean_abs': 0.07275009661268408, 'shapiq_order2_std': 0.10848801059291543}}}, 'cancer_bayes': {'loaded': True, 'info': {'version': '1', 'stage': 'Production', 'run_id': '51f18ac345ef41f88b86edb587df6715', 'creation_timestamp': 1751734417045, 'metrics': {'accuracy': 0.9876977152899824, 'precision_macro': 0.9882708665603402, 'recall_macro': 0.985406426721632, 'f1_macro': 0.9868050103194559, 'precision_0': 0.9904306220095693, 'recall_0': 0.9764150943396226, 'f1_0': 0.9833729216152018, 'support_0': 212.0, 'precision_1': 0.9861111111111112, 'recall_1': 0.9943977591036415, 'f1_1': 0.9902370990237099, 'support_1': 357.0, 'log_loss': 0.44341928598210933, 'mcc': 0.9736730798565391, 'n_draws': 50.0}}}}, 'mlflow_tracking_uri': 'http://mlflow:5000'}\n",
            "\n",
            "🌸 Testing /predict/iris/rf endpoint with first Setosa sample...\n",
            "True class: Setosa\n",
            "Features: [5.1 3.5 1.4 0.2]\n",
            "Prediction: {'predictions': [2.0], 'model_used': 'iris_rf', 'uncertainty': None}\n",
            "\n",
            "🔬 Testing /predict/cancer/bayes endpoint with first malignant sample...\n",
            "True class: Malignant\n",
            "Features: [1.354e+01 1.436e+01 8.746e+01 5.663e+02 9.779e-02]...\n",
            "Prediction with uncertainty: {'predictions': [0.05], 'model_used': 'cancer_bayes', 'uncertainty': [{'lower': 0.05, 'upper': 0.05}]}\n",
            "\n",
            "📊 Testing /models/iris/metrics endpoint...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: GET http://testserver/models/iris/metrics \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metrics: {'iris_random_forest': {'version': '1', 'accuracy': 1.0, 'f1_macro': 1.0}, 'iris_logreg': {'version': '1', 'accuracy': 1.0, 'f1_macro': 1.0}}\n"
          ]
        }
      ],
      "source": [
        "# %%writefile src/backend/ML/model_api/main.py\n",
        "\"\"\"\n",
        "FastAPI service for Iris and Breast-Cancer predictions.\n",
        "\n",
        "This module provides a dual-dataset prediction service that:\n",
        "1. Loads/trains point-estimate models for Iris (Random Forest)\n",
        "2. Loads/trains Bayesian models for Breast Cancer (PyMC)\n",
        "3. Exposes them at /predict/iris/{algo} and /predict/cancer/bayes endpoints\n",
        "4. Includes an embedded smoke test\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "import os\n",
        "import logging\n",
        "from typing import List, Dict, Any, Optional, Sequence\n",
        "from contextlib import asynccontextmanager\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import mlflow\n",
        "from fastapi import FastAPI, HTTPException, APIRouter, Path, Body, BackgroundTasks\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel, Field, validator, field_validator\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Global service instance\n",
        "service = None\n",
        "\n",
        "@asynccontextmanager\n",
        "async def lifespan(app: FastAPI):\n",
        "    \"\"\"Load models once at startup.\"\"\"\n",
        "    global service\n",
        "    service = _ModelService()\n",
        "    try:\n",
        "        service.load_all()\n",
        "        yield\n",
        "    finally:\n",
        "        service = None\n",
        "\n",
        "# Initialize FastAPI app with metadata\n",
        "app = FastAPI(\n",
        "    title=\"Iris & Cancer Prediction Service\",\n",
        "    description=\"Dual-dataset prediction service with model switching and metrics.\",\n",
        "    version=\"2.0.0\",\n",
        "    lifespan=lifespan\n",
        ")\n",
        "\n",
        "# Add CORS middleware\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# Create routers\n",
        "iris_router = APIRouter(prefix=\"/predict/iris\", tags=[\"Iris Models\"])\n",
        "cancer_router = APIRouter(prefix=\"/predict/cancer\", tags=[\"Cancer Models\"])\n",
        "metrics_router = APIRouter(prefix=\"/models\", tags=[\"Model Metrics\"])\n",
        "\n",
        "class IrisFeatures(BaseModel):\n",
        "    \"\"\"Single Iris-flower measurement.\"\"\"\n",
        "    sepal_length: float = Field(..., ge=4.0, le=8.0, description=\"Sepal length (cm)\")\n",
        "    sepal_width: float = Field(..., ge=2.0, le=4.5, description=\"Sepal width (cm)\")\n",
        "    petal_length: float = Field(..., ge=1.0, le=7.0, description=\"Petal length (cm)\")\n",
        "    petal_width: float = Field(..., ge=0.1, le=2.5, description=\"Petal width (cm)\")\n",
        "\n",
        "    class Config:\n",
        "        json_schema_extra = {\n",
        "            \"example\": {\n",
        "                \"sepal_length\": 5.1,\n",
        "                \"sepal_width\": 3.5,\n",
        "                \"petal_length\": 1.4,\n",
        "                \"petal_width\": 0.2\n",
        "            }\n",
        "        }\n",
        "\n",
        "class CancerFeatures(BaseModel):\n",
        "    \"\"\"30-dimensional input in **exact** `load_breast_cancer().feature_names` order.\"\"\"\n",
        "    values: List[float] = Field(\n",
        "        ..., min_items=30, max_items=30,\n",
        "        description=\"All 30 features from scikit-learn breast-cancer dataset.\"\n",
        "    )\n",
        "\n",
        "    @field_validator(\"values\")\n",
        "    def _exactly_30(cls, v: List[float]) -> List[float]:\n",
        "        if len(v) != 30:\n",
        "            raise ValueError(\"Must provide exactly 30 features\")\n",
        "        return v\n",
        "\n",
        "class IrisPredictRequest(BaseModel):\n",
        "    rows: List[IrisFeatures]\n",
        "\n",
        "class CancerPredictRequest(BaseModel):\n",
        "    rows: List[CancerFeatures]\n",
        "    posterior_samples: Optional[int] = Field(None, description=\"n posterior draws\")\n",
        "\n",
        "class PredictResponse(BaseModel):\n",
        "    predictions: List[float]\n",
        "    model_used: str\n",
        "    uncertainty: Optional[List[Dict[str, float]]] = None\n",
        "\n",
        "class CancerRetrainRequest(BaseModel):\n",
        "    \"\"\"Hyper-parameters for Bayesian retraining via PyMC.\"\"\"\n",
        "    draws: int = Field(500, ge=100, le=5_000)\n",
        "    tune: int = Field(250, ge=50, le=2_000)\n",
        "    target_accept: float = Field(0.9, ge=0.5, le=0.99)\n",
        "\n",
        "class _ModelService:\n",
        "    \"\"\"Caches all Production models (iris_rf, iris_logreg, cancer_bayes).\"\"\"\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        self.iris: dict[str, mlflow.pyfunc.PyFuncModel] = {}\n",
        "        self.cancer: mlflow.pyfunc.PyFuncModel | None = None\n",
        "        self.model_info: dict[str, dict[str, Any]] = {}\n",
        "\n",
        "    def _load_prod(self, name: str) -> mlflow.pyfunc.PyFuncModel:\n",
        "        \"\"\"Load *Production* version of *name* from the MLflow registry.\n",
        "\n",
        "        Works with both HTTP and local *FileStore* back-ends because it relies\n",
        "        on ``MlflowClient.get_latest_versions`` instead of the more restrictive\n",
        "        SQL-like filter syntax that is **not** implemented for the file store.\n",
        "        When the model (or the registry itself) is missing the helper will –\n",
        "        if the *DEV_AUTOTRAIN* environment variable is set – auto-train a quick\n",
        "        baseline model and promote it to *Production* so subsequent startups\n",
        "        do not repeat the bootstrap step.\n",
        "        \"\"\"\n",
        "        from mlflow.tracking import MlflowClient  # local import to keep top clean\n",
        "        client = MlflowClient()\n",
        "\n",
        "        def _promote_latest(model_name: str) -> None:\n",
        "            \"\"\"Promote the latest *None*-stage version to *Production*.\"\"\"\n",
        "            latest = client.get_latest_versions(model_name, stages=[\"None\"])\n",
        "            if latest:\n",
        "                ver = latest[0].version\n",
        "                client.transition_model_version_stage(\n",
        "                    name=model_name,\n",
        "                    version=ver,\n",
        "                    stage=\"Production\",\n",
        "                    archive_existing_versions=True,\n",
        "                )\n",
        "                logger.info(\"🔄 Promoted %s version %s → Production\", model_name, ver)\n",
        "\n",
        "        # ------------------------------------------------------------------\n",
        "        # 1️⃣  Try to fetch an existing Production version ------------------\n",
        "        # ------------------------------------------------------------------\n",
        "        try:\n",
        "            versions = client.get_latest_versions(name, stages=[\"Production\"])\n",
        "            if versions:\n",
        "                mv = versions[0]\n",
        "                run = client.get_run(mv.run_id)\n",
        "                self.model_info[name] = {\n",
        "                    \"version\": mv.version,\n",
        "                    \"stage\": mv.current_stage,\n",
        "                    \"run_id\": mv.run_id,\n",
        "                    \"creation_timestamp\": mv.creation_timestamp,\n",
        "                    \"metrics\": run.data.metrics,\n",
        "                }\n",
        "                return mlflow.pyfunc.load_model(f\"models:/{name}/{mv.current_stage}\")\n",
        "            raise RuntimeError(\"No Production version found\")\n",
        "\n",
        "        except Exception as exc:\n",
        "            # ------------------------------------------------------------------\n",
        "            # 2️⃣  Auto-train in development mode if enabled ---------------------\n",
        "            # ------------------------------------------------------------------\n",
        "            if os.getenv(\"DEV_AUTOTRAIN\") != \"1\":\n",
        "                logger.error(\"Failed to load model %s: %s\", name, exc)\n",
        "                raise HTTPException(\n",
        "                    status_code=503,\n",
        "                    detail=f\"Model {name} missing or broken – set DEV_AUTOTRAIN=1 to bootstrap automatically: {exc}\",\n",
        "                ) from exc\n",
        "\n",
        "            logger.warning(\"⚠️  %s not found – auto-training baseline model (DEV_AUTOTRAIN)\", name)\n",
        "\n",
        "            try:\n",
        "                if name in {\"iris_random_forest\", \"iris_logreg\"}:\n",
        "                    # This helper trains **both** iris models – inexpensive (<2 s)\n",
        "                    from src.backend.ML.mlops.training import run_all_trainings\n",
        "                    run_all_trainings(n_trials=3)  # fast bootstrap\n",
        "                    _promote_latest(name)\n",
        "\n",
        "                elif name == \"breast_cancer_bayes\":\n",
        "                    # Attempt the full PyMC training – may fail if PyMC absent\n",
        "                    try:\n",
        "                        from src.backend.ML.mlops.training_bayes import train_bayes_logreg\n",
        "                        train_bayes_logreg(draws=50, tune=25)\n",
        "                        _promote_latest(name)\n",
        "                    except Exception as bayes_err:\n",
        "                        logger.warning(\"PyMC training failed (%s) – falling back to sklearn logistic regression\", bayes_err)\n",
        "                        # Lightweight sklearn fallback so the endpoint still works\n",
        "                        from sklearn.datasets import load_breast_cancer\n",
        "                        from sklearn.linear_model import LogisticRegression\n",
        "                        import pandas as pd\n",
        "                        X, y = load_breast_cancer(return_X_y=True, as_frame=True)\n",
        "                        model = LogisticRegression(max_iter=1000).fit(X, y)\n",
        "\n",
        "                        class _SklearnPyFunc(mlflow.pyfunc.PythonModel):\n",
        "                            def __init__(self, mdl):\n",
        "                                self._mdl = mdl\n",
        "\n",
        "                            def predict(self, context, model_input):  # noqa: D401\n",
        "                                import pandas as _pd\n",
        "                                if not isinstance(model_input, _pd.DataFrame):\n",
        "                                    model_input = _pd.DataFrame(model_input)\n",
        "                                proba = self._mdl.predict_proba(model_input)[:, 1]\n",
        "                                return proba\n",
        "\n",
        "                        with mlflow.start_run(run_name=\"bootstrap_breast_cancer_lr\"):\n",
        "                            mlflow.pyfunc.log_model(\n",
        "                                \"model\",\n",
        "                                python_model=_SklearnPyFunc(model),\n",
        "                                registered_model_name=name,\n",
        "                                input_example=X.iloc[:5],\n",
        "                            )\n",
        "                        _promote_latest(name)\n",
        "                else:\n",
        "                    raise RuntimeError(f\"Unknown model name '{name}' for auto-train path\")\n",
        "\n",
        "                # Retry load after successful bootstrap\n",
        "                return mlflow.pyfunc.load_model(f\"models:/{name}/Production\")\n",
        "\n",
        "            except Exception as tr_err:\n",
        "                logger.error(\"Auto-training fallback failed for %s: %s\", name, tr_err)\n",
        "                raise HTTPException(status_code=503, detail=str(tr_err)) from tr_err\n",
        "\n",
        "    def load_all(self) -> None:\n",
        "        \"\"\"Call once at startup (lifespan).\"\"\"\n",
        "        try:\n",
        "            # Ensure the service points to the same MLflow backend used during training\n",
        "            mlflow_uri = os.getenv(\"MLFLOW_TRACKING_URI\", \"file:./mlruns_local\")\n",
        "            mlflow.set_tracking_uri(mlflow_uri)\n",
        "            logger.info(\"MLflow tracking URI set to %s\", mlflow_uri)\n",
        "\n",
        "            logger.info(\"Loading Iris Random Forest model...\")\n",
        "            self.iris[\"rf\"] = self._load_prod(\"iris_random_forest\")\n",
        "\n",
        "            logger.info(\"Loading Iris Logistic Regression model...\")\n",
        "            self.iris[\"logreg\"] = self._load_prod(\"iris_logreg\")\n",
        "\n",
        "            logger.info(\"Loading Breast Cancer Bayesian model...\")\n",
        "            self.cancer = self._load_prod(\"breast_cancer_bayes\")  # Fixed model name\n",
        "\n",
        "            logger.info(\"✅ All models loaded successfully\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to load models: {e}\")\n",
        "            raise\n",
        "\n",
        "@app.get(\"/health\")\n",
        "async def health():\n",
        "    \"\"\"Health check endpoint with detailed model information.\"\"\"\n",
        "    if service is None:\n",
        "        return {\n",
        "            \"status\": \"initializing\",\n",
        "            \"message\": \"Service is starting up\"\n",
        "        }\n",
        "\n",
        "    model_status = {\n",
        "        \"iris_rf\": {\n",
        "            \"loaded\": service.iris.get(\"rf\") is not None,\n",
        "            \"info\": service.model_info.get(\"iris_random_forest\", {})\n",
        "        },\n",
        "        \"iris_logreg\": {\n",
        "            \"loaded\": service.iris.get(\"logreg\") is not None,\n",
        "            \"info\": service.model_info.get(\"iris_logreg\", {})\n",
        "        },\n",
        "        \"cancer_bayes\": {\n",
        "            \"loaded\": service.cancer is not None,\n",
        "            \"info\": service.model_info.get(\"breast_cancer_bayes\", {})\n",
        "        }\n",
        "    }\n",
        "\n",
        "    all_loaded = all(m[\"loaded\"] for m in model_status.values())\n",
        "\n",
        "    return {\n",
        "        \"status\": \"healthy\" if all_loaded else \"degraded\",\n",
        "        \"models\": model_status,\n",
        "        \"mlflow_tracking_uri\": mlflow.get_tracking_uri()\n",
        "    }\n",
        "\n",
        "@app.head(\"/health\", include_in_schema=False)\n",
        "async def health_head():\n",
        "    \"\"\"HEAD variant of /health for readiness probes (no body).\"\"\"\n",
        "    return await health()\n",
        "\n",
        "@iris_router.post(\n",
        "    \"/{algo}\",\n",
        "    summary=\"Iris ‣ Random-Forest or Logistic-Regression\",\n",
        "    response_model=PredictResponse,\n",
        "    description=\"Set **algo** to `rf` or `logreg`.\"\n",
        ")\n",
        "async def predict_iris(\n",
        "    algo: str = Path(..., pattern=\"^(rf|logreg)$\"),\n",
        "    req: IrisPredictRequest = Body(...)\n",
        "):\n",
        "    \"\"\"Make predictions using the specified Iris model.\"\"\"\n",
        "    model = service.iris.get(algo)\n",
        "    if model is None:\n",
        "        raise HTTPException(503, f\"Iris model '{algo}' not loaded\")\n",
        "    X = pd.DataFrame([r.dict() for r in req.rows])\n",
        "    preds = model.predict(X)\n",
        "    return PredictResponse(predictions=preds.tolist(), model_used=f\"iris_{algo}\")\n",
        "\n",
        "@cancer_router.post(\n",
        "    \"/bayes\",\n",
        "    summary=\"Cancer ‣ Bayesian Logistic-Regression\",\n",
        "    response_model=PredictResponse\n",
        ")\n",
        "async def predict_cancer(req: CancerPredictRequest):\n",
        "    \"\"\"Make predictions using the Bayesian Cancer model.\"\"\"\n",
        "    if service.cancer is None:\n",
        "        raise HTTPException(503, \"Cancer model not loaded\")\n",
        "\n",
        "    # Convert features to DataFrame\n",
        "    X = pd.DataFrame([r.values for r in req.rows])\n",
        "\n",
        "    # Get predictions\n",
        "    preds = service.cancer.predict(X)\n",
        "\n",
        "    # Add uncertainty estimates if requested\n",
        "    uncertainty = None\n",
        "    if req.posterior_samples:\n",
        "        # Simple bootstrap for uncertainty (illustrative)\n",
        "        samples = []\n",
        "        for _ in range(req.posterior_samples):\n",
        "            boot_idx = np.random.choice(len(X), size=len(X))\n",
        "            boot_preds = service.cancer.predict(X.iloc[boot_idx])\n",
        "            samples.append(boot_preds)\n",
        "\n",
        "        # Calculate confidence intervals\n",
        "        samples = np.array(samples)\n",
        "        uncertainty = [\n",
        "            {\n",
        "                \"lower\": float(np.percentile(samples[:, i], 2.5)),\n",
        "                \"upper\": float(np.percentile(samples[:, i], 97.5))\n",
        "            }\n",
        "            for i in range(len(X))\n",
        "        ]\n",
        "\n",
        "    return PredictResponse(\n",
        "        predictions=preds.tolist(),\n",
        "        model_used=\"cancer_bayes\",\n",
        "        uncertainty=uncertainty\n",
        "    )\n",
        "\n",
        "@app.post(\n",
        "    \"/train/cancer/bayes/retrain\",\n",
        "    summary=\"Start background Bayesian retraining and promotion\",\n",
        "    response_model=dict,\n",
        ")\n",
        "async def retrain_cancer_bayes(\n",
        "    req: CancerRetrainRequest,\n",
        "    background: BackgroundTasks,\n",
        "):\n",
        "    \"\"\"Spawn a background task that retrains the Bayesian Cancer model.\n",
        "\n",
        "    The task runs **non-blocking** so the HTTP request returns instantly.\n",
        "    It will:\n",
        "      1. call :func:`train_bayes_logreg` with given hyper-params,\n",
        "      2. register the resulting model in MLflow, and\n",
        "      3. promote the newest version to *Production*.\n",
        "    \"\"\"\n",
        "    def _job() -> None:\n",
        "        import mlflow\n",
        "        from src.backend.ML.mlops.training_bayes import train_bayes_logreg\n",
        "        from mlflow.tracking import MlflowClient\n",
        "        global service  # use the already initialised cache\n",
        "\n",
        "        # Ensure we talk to the same MLflow store as the service\n",
        "        mlflow.set_tracking_uri(os.getenv(\"MLFLOW_TRACKING_URI\", \"file:./mlruns_local\"))\n",
        "\n",
        "        # 1️⃣  Fit the model – this may take a while\n",
        "        try:\n",
        "            train_bayes_logreg(\n",
        "                draws=req.draws,\n",
        "                tune=req.tune,\n",
        "                target_accept=req.target_accept,\n",
        "                register=True,\n",
        "            )\n",
        "        except Exception as exc:  # pragma: no cover – background context\n",
        "            logger.error(\"Retrain task failed: %s\", exc)\n",
        "            return\n",
        "\n",
        "        # 2️⃣  Promote the latest *None* stage version\n",
        "        client = MlflowClient()\n",
        "        latest = client.get_latest_versions(\"breast_cancer_bayes\", stages=[\"None\"])\n",
        "        if latest:\n",
        "            mv = latest[0]\n",
        "            client.transition_model_version_stage(\n",
        "                name=\"breast_cancer_bayes\",\n",
        "                version=mv.version,\n",
        "                stage=\"Production\",\n",
        "                archive_existing_versions=True,\n",
        "            )\n",
        "            logger.info(\"✅ Promoted breast_cancer_bayes v%s → Production\", mv.version)\n",
        "\n",
        "        # 3️⃣  Hot-reload in-memory cache so /health reflects the new version\n",
        "        try:\n",
        "            service.load_all()\n",
        "        except Exception as exc:\n",
        "            logger.warning(\"Service reload after retrain failed: %s\", exc)\n",
        "\n",
        "    background.add_task(_job)\n",
        "    return {\"status\": \"started\", \"detail\": \"Bayesian retraining job running in background\"}\n",
        "\n",
        "@metrics_router.get(\n",
        "    \"/iris/metrics\",\n",
        "    summary=\"Live metrics for Iris models (Production stage)\"\n",
        ")\n",
        "def iris_metrics():\n",
        "    \"\"\"Get latest metrics for both Iris models in Production.\"\"\"\n",
        "    client = mlflow.tracking.MlflowClient()\n",
        "    out: dict[str, dict[str, Any]] = {}\n",
        "    for name in (\"iris_random_forest\", \"iris_logreg\"):\n",
        "        mv = client.get_latest_versions(name, stages=[\"Production\"])[0]\n",
        "        run = client.get_run(mv.run_id)\n",
        "        out[name] = {\n",
        "            \"version\": mv.version,\n",
        "            \"accuracy\": run.data.metrics.get(\"accuracy\"),\n",
        "            \"f1_macro\": run.data.metrics.get(\"f1_macro\")\n",
        "        }\n",
        "    return out\n",
        "\n",
        "@metrics_router.post(\"/reload\", summary=\"Reload all Production models into memory\")\n",
        "def reload_models():\n",
        "    \"\"\"Force reloading cache after external changes (e.g. new Production model).\"\"\"\n",
        "    service.load_all()\n",
        "    return {\"status\": \"reloaded\"}\n",
        "\n",
        "# Wire up routers\n",
        "app.include_router(iris_router)\n",
        "app.include_router(cancer_router)\n",
        "app.include_router(metrics_router)\n",
        "\n",
        "# ─── Smoke Test ────────────────────────────────────────────────────────────\n",
        "if __name__ == \"__main__\":\n",
        "    import asyncio\n",
        "    try:\n",
        "        import nest_asyncio  # type: ignore\n",
        "        nest_asyncio.apply()\n",
        "    except ImportError:\n",
        "        pass  # nest_asyncio is optional; continue without if unavailable\n",
        "\n",
        "    # Local import to avoid affecting prod runtime\n",
        "    from fastapi.testclient import TestClient\n",
        "    from sklearn.datasets import load_iris, load_breast_cancer\n",
        "    import numpy as np\n",
        "\n",
        "    async def test_app():\n",
        "        \"\"\"Run smoke tests with proper lifespan handling.\"\"\"\n",
        "        async with app.router.lifespan_context(app):\n",
        "            client = TestClient(app)\n",
        "\n",
        "            # Test health endpoint\n",
        "            print(\"\\n🏥 Testing /health endpoint...\")\n",
        "            health = client.get(\"/health\").json()\n",
        "            print(f\"Response: {health}\")\n",
        "\n",
        "            # Test Iris endpoint with first Setosa sample\n",
        "            print(\"\\n🌸 Testing /predict/iris/rf endpoint with first Setosa sample...\")\n",
        "            iris = load_iris()\n",
        "            setosa_sample = iris.data[0]  # First sample is Setosa\n",
        "            iris_request = {\n",
        "                \"rows\": [{\n",
        "                    \"sepal_length\": float(setosa_sample[0]),\n",
        "                    \"sepal_width\": float(setosa_sample[1]),\n",
        "                    \"petal_length\": float(setosa_sample[2]),\n",
        "                    \"petal_width\": float(setosa_sample[3])\n",
        "                }]\n",
        "            }\n",
        "            iris_response = client.post(\"/predict/iris/rf\", json=iris_request).json()\n",
        "            print(f\"True class: Setosa\")\n",
        "            print(f\"Features: {setosa_sample}\")\n",
        "            print(f\"Prediction: {iris_response}\")\n",
        "\n",
        "            # Test Cancer endpoint with first malignant sample\n",
        "            print(\"\\n🔬 Testing /predict/cancer/bayes endpoint with first malignant sample...\")\n",
        "            cancer = load_breast_cancer()\n",
        "            malignant_idx = np.where(cancer.target == 1)[0][0]\n",
        "            malignant_sample = cancer.data[malignant_idx]\n",
        "            cancer_request = {\n",
        "                \"rows\": [{\n",
        "                    \"values\": [float(x) for x in malignant_sample]\n",
        "                }],\n",
        "                \"posterior_samples\": 100\n",
        "            }\n",
        "            cancer_response = client.post(\"/predict/cancer/bayes\", json=cancer_request).json()\n",
        "            print(f\"True class: Malignant\")\n",
        "            print(f\"Features: {malignant_sample[:5]}...\")  # Show first 5 features\n",
        "            print(f\"Prediction with uncertainty: {cancer_response}\")\n",
        "\n",
        "            # Test metrics endpoint\n",
        "            print(\"\\n📊 Testing /models/iris/metrics endpoint...\")\n",
        "            metrics = client.get(\"/models/iris/metrics\").json()\n",
        "            print(f\"Metrics: {metrics}\")\n",
        "\n",
        "    # Detect running loop (e.g. Jupyter) and execute accordingly\n",
        "    try:\n",
        "        _loop = asyncio.get_running_loop()\n",
        "    except RuntimeError:\n",
        "        _loop = None\n",
        "\n",
        "    if _loop and _loop.is_running():\n",
        "        _loop.run_until_complete(test_app())\n",
        "    else:\n",
        "        asyncio.run(test_app())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
