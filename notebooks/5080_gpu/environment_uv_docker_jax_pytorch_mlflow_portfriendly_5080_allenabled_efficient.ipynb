{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensure no other containers are running for dev container, if they are stop and remove them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace\n",
      "['.devcontainer', '.env', '.git', '.github', '.gitignore', '.mypy_cache', '.pytest_cache', '.venv', 'archive', 'backend', 'data', 'Dockerfile.backend', 'frontend', 'images', 'mlflow_db', 'mlflow_shapiq_explainer_modularized_fastapi_example.ipynb', 'mlruns', 'mlruns_local', 'netlify.toml', 'node_modules', 'notebooks', 'package-lock.json', 'package.json', 'pyproject.toml', 'railway_fastapi_template copy.ipynb', 'README.md', 'render.yaml', 'scripts', 'shapiq_interactions.csv', 'shapiq_interactions_summary.csv', 'src', 'tasks.py', 'template', 'tests', 'test_iris.json', 'uv.lock', '__pycache__']\n",
      "None\n",
      "/workspace/notebooks/5080_gpu\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "print(os.getcwd())\n",
    "if os.path.exists(\"notebooks/5080_gpu\"):\n",
    "    os.chdir(\"notebooks/5080_gpu\")\n",
    "    print(os.getcwd())\n",
    "else:\n",
    "    print(\"notebooks/5080_gpu does not exist\")\n",
    "    print(os.getcwd())\n",
    "    print(\"Please run this script from the root of the repository\")\n",
    "    print(\"e.g. python environment_uv_docker_jax_pytorch_mlflow_portfriendly_5080_allenabled_efficient.ipynb\")\n",
    "    print(\"or cd to the root of the repository and run this script\")\n",
    "    print(\"e.g. cd .. && python notebooks/5080_gpu/environment_uv_docker_jax_pytorch_mlflow_portfriendly_5080_allenabled_efficient.ipynb\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../.devcontainer/.dockerignore\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../.devcontainer/.dockerignore\n",
    "# Ignore VCS and editor cruft\n",
    "**/.git\n",
    "**/.vscode\n",
    "**/.idea\n",
    ".DS_Store\n",
    "Thumbs.db\n",
    "\n",
    "# Bytecode and caches\n",
    "**/__pycache__\n",
    "**/*.pyc\n",
    "**/*.pyo\n",
    "**/*.pyd\n",
    "**/*.swp\n",
    "\n",
    "# Virtualenvs\n",
    "**/venv\n",
    "**/env\n",
    "\n",
    "# Workspace env files\n",
    ".env\n",
    "*.code-workspace\n",
    "\n",
    "# Large or generated data\n",
    "data/\n",
    "\n",
    "# Jupyter checkpoints\n",
    "notebooks/**/*.ipynb_checkpoints\n",
    "\n",
    "# Logs\n",
    "*.log\n",
    "\n",
    "# MLflow runs & artifacts (often huge, sometimes root-owned)\n",
    "mlruns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../.devcontainer/.env.template\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../.devcontainer/.env.template \n",
    "ENV_NAME=docker_dev_template\n",
    "CUDA_TAG=12.8.0\n",
    "DOCKER_BUILDKIT=1\n",
    "\n",
    "# Host ports\n",
    "HOST_JUPYTER_PORT=8890\n",
    "HOST_TENSORBOARD_PORT=6008\n",
    "HOST_EXPLAINER_PORT=8050\n",
    "HOST_STREAMLIT_PORT=8501\n",
    "HOST_MLFLOW_PORT=5000\n",
    "\n",
    "# Python & JAX\n",
    "PYTHON_VER=3.10\n",
    "\n",
    "# Leave blank to autodetect; set to 'gpu' or 'cpu' only if you must force it.\n",
    "JAX_PLATFORM_NAME=\n",
    "\n",
    "XLA_PYTHON_CLIENT_PREALLOCATE=true\n",
    "XLA_PYTHON_CLIENT_ALLOCATOR=platform\n",
    "XLA_PYTHON_CLIENT_MEM_FRACTION=0.95\n",
    "XLA_FLAGS=--xla_force_host_platform_device_count=1\n",
    "JAX_DISABLE_JIT=false\n",
    "JAX_ENABLE_X64=false\n",
    "TF_FORCE_GPU_ALLOW_GROWTH=false\n",
    "JAX_PREALLOCATION_SIZE_LIMIT_BYTES=8589934592\n",
    "\n",
    "# Runtime GPU targeting & Jupyter convenience\n",
    "JUPYTER_TOKEN=jupyter\n",
    "INSTALL_TF=0\n",
    "\n",
    "# Keep uv from targeting /workspace/.venv (a bind mount)\n",
    "UV_PROJECT_ENVIRONMENT=/app/.venv\n",
    "\n",
    "# NOTE: Torch CUDA wheel is auto-selected from CUDA_TAG at startup:\n",
    "#   12.1  -> cu121\n",
    "#   12.4  -> cu124\n",
    "#   12.6+ -> cu126 (works for 12.6 / 12.8 / 12.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../.devcontainer/devcontainer.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../.devcontainer/devcontainer.json\n",
    "{\n",
    "  \"name\": \"docker_dev_template_uv\",\n",
    "  \"dockerComposeFile\": [\"docker-compose.yml\"],\n",
    "  \"service\": \"datascience\",\n",
    "  \"workspaceFolder\": \"/workspace\",\n",
    "  \"shutdownAction\": \"stopCompose\",\n",
    "\n",
    "  \"customizations\": {\n",
    "    \"vscode\": {\n",
    "      \"extensions\": [\n",
    "        \"ms-python.python\",\n",
    "        \"ms-python.vscode-pylance\",\n",
    "        \"ms-toolsai.jupyter\",\n",
    "        \"ms-toolsai.jupyter-renderers\"\n",
    "      ],\n",
    "      \"settings\": {\n",
    "        \"telemetry.telemetryLevel\": \"off\",\n",
    "        \"python.telemetry.enabled\": false,\n",
    "        \"jupyter.telemetry.enabled\": false,\n",
    "        \"jupyter.experiments.enabled\": false,\n",
    "        \"update.mode\": \"manual\",\n",
    "        \"extensions.autoUpdate\": false,\n",
    "        \"extensions.autoCheckUpdates\": false,\n",
    "        \"remote.extensionKind\": {\n",
    "          \"ms-python.python\": [\"ui\"],\n",
    "          \"ms-python.vscode-pylance\": [\"ui\"],\n",
    "          \"ms-toolsai.jupyter\": [\"ui\"],\n",
    "          \"ms-toolsai.jupyter-renderers\": [\"ui\"]\n",
    "        },\n",
    "        \"python.defaultInterpreterPath\": \"/app/.venv/bin/python\",\n",
    "        \"jupyter.interactiveWindow.textEditor.executeSelection\": true,\n",
    "        \"jupyter.widgetScriptSources\": [\"jsdelivr.com\", \"unpkg.com\"]\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "\n",
    "  \"remoteEnv\": {\n",
    "    \"JUPYTER_ENABLE_LAB\": \"true\",\n",
    "    \"UV_PROJECT_ENVIRONMENT\": \"/app/.venv\"\n",
    "  },\n",
    "\n",
    "  \"postCreateCommand\": [\n",
    "    \"/bin/sh\",\n",
    "    \"-c\",\n",
    "    \"set -eux; \\\n",
    "    echo '## postCreate: uv bootstrap ##'; \\\n",
    "    PROJECT_DIR=/workspace; \\\n",
    "    DEV_DIR=/workspace/.devcontainer; \\\n",
    "    TARGET_DIR=$([ -f \\\"$PROJECT_DIR/pyproject.toml\\\" ] && echo \\\"$PROJECT_DIR\\\" || echo \\\"$DEV_DIR\\\"); \\\n",
    "    cd \\\"$TARGET_DIR\\\"; \\\n",
    "    uv --version; \\\n",
    "    (uv sync --frozen || uv sync); \\\n",
    "    cd \\\"$PROJECT_DIR\\\"; \\\n",
    "    python - <<'PY'\\nimport sys, subprocess\\nprint('[postCreate] Ensuring ipykernel present for', sys.executable)\\ntry:\\n    import ipykernel  # noqa\\nexcept Exception:\\n    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'ipykernel'])\\nsubprocess.check_call([sys.executable, '-m', 'ipykernel', 'install', '--name', 'uv-app-venv', '--display-name', 'Python (uv /app/.venv)', '--user'])\\nprint('[postCreate] ipykernel installed/updated as uv-app-venv')\\nPY\\n \\\n",
    "    echo '[postCreate] Installing PyJAGS (post-sync)'; \\\n",
    "    CPPFLAGS='-include cstdint' uv pip install --no-build-isolation pyjags==1.3.8 || true; \\\n",
    "    python /workspace/.devcontainer/verify_env.py\"\n",
    "  ],\n",
    "\n",
    "  \"postStartCommand\": [\n",
    "    \"/bin/sh\",\n",
    "    \"-c\",\n",
    "    \"set -eux; \\\n",
    "    /workspace/.devcontainer/gpu_bootstrap.sh; \\\n",
    "    echo \\\"[postStart] PWD=$(pwd)\\\"; \\\n",
    "    python .devcontainer/verify_env.py\"\n",
    "  ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../.devcontainer/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../.devcontainer/Dockerfile\n",
    "# .devcontainer/Dockerfile — image is built from .devcontainer context only\n",
    "ARG CUDA_TAG=12.8.0\n",
    "FROM nvidia/cuda:${CUDA_TAG}-cudnn-devel-ubuntu22.04\n",
    "\n",
    "ARG PYTHON_VER=3.10\n",
    "ARG ENV_NAME=docker_dev_template\n",
    "ARG JAX_PREALLOCATE=true\n",
    "ARG JAX_MEM_FRAC=0.95\n",
    "ARG JAX_ALLOCATOR=platform\n",
    "ARG JAX_PREALLOC_LIMIT=8589934592\n",
    "ENV DEBIAN_FRONTEND=noninteractive\n",
    "\n",
    "# OS deps\n",
    "RUN --mount=type=cache,target=/var/cache/apt \\\n",
    "    --mount=type=cache,target=/var/lib/apt \\\n",
    "    apt-get update && apt-get install -y --no-install-recommends \\\n",
    "        bash curl ca-certificates git procps htop util-linux build-essential \\\n",
    "        python3 python3-venv python3-pip python3-dev \\\n",
    "        autoconf automake libtool m4 cmake pkg-config \\\n",
    "        jags iproute2 net-tools lsof \\\n",
    "    && apt-get clean && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Node (for remote UI bits)\n",
    "RUN curl -fsSL https://deb.nodesource.com/setup_18.x | bash - \\\n",
    " && apt-get update && apt-get install -y nodejs \\\n",
    " && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# uv\n",
    "COPY --from=ghcr.io/astral-sh/uv:0.7.12 /uv /uvx /bin/\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# Create venv + base tools\n",
    "RUN --mount=type=cache,target=/root/.cache/uv \\\n",
    "    uv venv .venv --python \"${PYTHON_VER}\" --prompt \"${ENV_NAME}\" && \\\n",
    "    uv pip install --no-cache-dir jupyterlab\n",
    "\n",
    "ENV VIRTUAL_ENV=/app/.venv\n",
    "ENV PATH=\"/app/.venv/bin:${PATH}\"\n",
    "ENV UV_PROJECT_ENVIRONMENT=/app/.venv\n",
    "\n",
    "# CUDA symlink sanity\n",
    "RUN set -e; \\\n",
    "    CUDA_REAL=\"$(ls -d /usr/local/cuda-* 2>/dev/null | sort -V | tail -n1 || true)\"; \\\n",
    "    if [ -z \"$CUDA_REAL\" ] && [ -d /usr/local/cuda ]; then CUDA_REAL=\"/usr/local/cuda\"; fi; \\\n",
    "    if [ -z \"$CUDA_REAL\" ]; then echo '❌ No CUDA toolkit found.' >&2; exit 1; fi; \\\n",
    "    if [ \"$CUDA_REAL\" != \"/usr/local/cuda\" ]; then ln -sfn \"$CUDA_REAL\" /usr/local/cuda; fi; \\\n",
    "    echo \"🟢 CUDA toolkit: $CUDA_REAL\"\n",
    "\n",
    "# (no JAX or Torch installs here; bootstrap handles them)\n",
    "\n",
    "# GPU / JAX envs\n",
    "ENV XLA_PYTHON_CLIENT_PREALLOCATE=${JAX_PREALLOCATE}\n",
    "ENV XLA_PYTHON_CLIENT_MEM_FRACTION=${JAX_MEM_FRAC}\n",
    "ENV XLA_PYTHON_CLIENT_ALLOCATOR=${JAX_ALLOCATOR}\n",
    "ENV XLA_FLAGS=\"--xla_force_host_platform_device_count=1\"\n",
    "ENV JAX_DISABLE_JIT=false\n",
    "ENV JAX_ENABLE_X64=false\n",
    "ENV TF_FORCE_GPU_ALLOW_GROWTH=false\n",
    "ENV JAX_PREALLOCATION_SIZE_LIMIT_BYTES=${JAX_PREALLOC_LIMIT}\n",
    "ENV LD_LIBRARY_PATH=\"/app/.venv/lib:${LD_LIBRARY_PATH}\"\n",
    "\n",
    "WORKDIR /workspace\n",
    "RUN echo 'cd /workspace' > /etc/profile.d/99-workspace-cd.sh\n",
    "RUN mkdir -p /root/.ipython/profile_default/startup && \\\n",
    "    printf \"import os, sys\\nos.chdir('/workspace')\\nsys.path.append('/workspace')\\n\" \\\n",
    "      > /root/.ipython/profile_default/startup/00-cd-workspace.py\n",
    "RUN echo '. /app/.venv/bin/activate' > /etc/profile.d/10-uv-activate.sh\n",
    "\n",
    "CMD [\"bash\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../.devcontainer/verify_env.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../.devcontainer/verify_env.py\n",
    "#!/usr/bin/env python3\n",
    "import sys, importlib, os, textwrap\n",
    "\n",
    "CRIT = []\n",
    "WARN = []\n",
    "\n",
    "def _have(mod: str) -> bool:\n",
    "    try:\n",
    "        importlib.import_module(mod); return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def _msg_box(title: str, body: str) -> None:\n",
    "    line = \"=\" * 80\n",
    "    print(f\"\\n{line}\\n{title}\\n{line}\\n{body}\\n\")\n",
    "\n",
    "def _probe_jax() -> None:\n",
    "    \"\"\"\n",
    "    Probe JAX availability and devices without relying on private internals.\n",
    "    Adds context about import paths and provides actionable, uv-friendly hints.\n",
    "    \"\"\"\n",
    "    import importlib, os, textwrap\n",
    "\n",
    "    jp = os.environ.get(\"JAX_PLATFORM_NAME\", \"<unset>\")\n",
    "    print(f\"   JAX_PLATFORM_NAME: {jp}\")\n",
    "\n",
    "    try:\n",
    "        jax = importlib.import_module(\"jax\")\n",
    "    except Exception as e:\n",
    "        WARN.append(f\"jax not importable: {e!r}\")\n",
    "        _msg_box(\n",
    "            \"Action: JAX not importable\",\n",
    "            \"• Ensure JAX is installed into /app/.venv via: uv pip install jax\\n\"\n",
    "            \"• Avoid using bare 'pip'; prefer 'uv pip' with UV_PROJECT_ENVIRONMENT=/app/.venv .\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    # Identify import locations\n",
    "    try:\n",
    "        jaxlib = importlib.import_module(\"jaxlib\")\n",
    "        print(f\"   jax: {getattr(jax,'__version__','?')} @ {getattr(jax,'__file__','?')}\")\n",
    "        print(f\"   jaxlib @ {getattr(jaxlib,'__file__','?')}\")\n",
    "    except Exception as e:\n",
    "        WARN.append(f\"jaxlib import failed: {e!r}\")\n",
    "\n",
    "    # Device probe (no private APIs)\n",
    "    try:\n",
    "        devs = jax.devices()\n",
    "        print(f\"   jax {getattr(jax,'__version__','?')} devices: {devs}\")\n",
    "    except Exception as e:\n",
    "        WARN.append(f\"jax.devices() raised: {e!r}\")\n",
    "        _msg_box(\n",
    "            \"Action: Fix JAX GPU backend\",\n",
    "            textwrap.dedent(\"\"\"\\\n",
    "                • If you intend to use GPU with CUDA, install the PJRT CUDA plugin:\n",
    "                  uv pip install \"jax[cuda12]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
    "                • Ensure installs target /app/.venv (use 'uv pip', not bare 'pip').\n",
    "            \"\"\"),\n",
    "        )\n",
    "        return\n",
    "\n",
    "    # CPU-only hints\n",
    "    gpu = [d for d in devs if \"gpu\" in str(d).lower() or \"cuda\" in str(d).lower()]\n",
    "    if not gpu:\n",
    "        WARN.append(\"JAX imported but reports CPU-only devices.\")\n",
    "        _msg_box(\n",
    "            \"Info: JAX is CPU-only right now\",\n",
    "            textwrap.dedent(\"\"\"\\\n",
    "                Likely causes (check in order):\n",
    "                1) CUDA plugin not installed in this venv:\n",
    "                   uv pip install \"jax[cuda12]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
    "                2) Plugin/driver/CUDA version mismatch (common on very new drivers):\n",
    "                   uv pip install -U \"jax[cuda12]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
    "                3) Conflicting libraries from other frameworks in a different site-packages.\n",
    "            \"\"\"),\n",
    "        )\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"## Python & library diagnostics ##\")\n",
    "    print(\"Python:\", sys.version.split()[0])\n",
    "    print(\"sys.executable:\", sys.executable)\n",
    "    print(\"sys.prefix:\", sys.prefix)\n",
    "    print(\"VIRTUAL_ENV:\", os.environ.get(\"VIRTUAL_ENV\",\"<unset>\"))\n",
    "    print(\"PATH head:\", os.environ.get(\"PATH\",\"\").split(\":\")[:3])\n",
    "\n",
    "    if not sys.executable.startswith(\"/app/.venv/\"):\n",
    "        CRIT.append(\"Interpreter is not /app/.venv — uv env not active for this process\")\n",
    "\n",
    "    jlab_ok = _have(\"jupyterlab\")\n",
    "    print(f\" - jupyterlab: {'OK' if jlab_ok else 'MISSING'}\")\n",
    "\n",
    "    torch_ok = _have(\"torch\")\n",
    "    print(f\" - torch: {'OK' if torch_ok else 'MISSING'}\")\n",
    "    if torch_ok:\n",
    "        try:\n",
    "            import torch\n",
    "            print(\"   torch\", torch.__version__, \"CUDA available:\", torch.cuda.is_available())\n",
    "        except Exception as e:\n",
    "            WARN.append(f\"torch import ok but CUDA probe errored: {e}\")\n",
    "\n",
    "    print(f\" - jax: {'OK' if _have('jax') else 'MISSING'}\")\n",
    "    _probe_jax()\n",
    "\n",
    "    if CRIT:\n",
    "        _msg_box(\"Critical failures\", \"\\n\".join(f\"• {m}\" for m in CRIT))\n",
    "        sys.exit(1)\n",
    "\n",
    "    if WARN:\n",
    "        _msg_box(\"Warnings (non-blocking)\", \"\\n\".join(f\"• {m}\" for m in WARN))\n",
    "\n",
    "    print(\"✅ verify_env completed (warnings above are informational).\")\n",
    "    sys.exit(0)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../.devcontainer/gpu_bootstrap.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../.devcontainer/gpu_bootstrap.sh\n",
    "#!/usr/bin/env bash\n",
    "set -euxo pipefail\n",
    "\n",
    "echo \"[gpu-bootstrap] BEGIN\"\n",
    "\n",
    "PY=\"/app/.venv/bin/python\"\n",
    "export UV_PROJECT_ENVIRONMENT=\"/app/.venv\"\n",
    "\n",
    "_have_cmd() { command -v \"$1\" >/dev/null 2>&1; }\n",
    "\n",
    "seed_pip() {\n",
    "  if ! \"$PY\" -m pip --version >/dev/null 2>&1; then\n",
    "    echo \"[gpu-bootstrap] Seeding pip into /app/.venv (ensurepip)\"\n",
    "    \"$PY\" -m ensurepip --upgrade || true\n",
    "  fi\n",
    "}\n",
    "\n",
    "PIP() {\n",
    "  if _have_cmd uv; then\n",
    "    echo \"[gpu-bootstrap] Using: uv pip $*\"\n",
    "    uv pip \"$@\"\n",
    "  else\n",
    "    seed_pip\n",
    "    echo \"[gpu-bootstrap] Using: $PY -m pip $*\"\n",
    "    \"$PY\" -m pip \"$@\"\n",
    "  fi\n",
    "}\n",
    "\n",
    "PIP_SHOW() {\n",
    "  if _have_cmd uv; then uv pip show \"$@\" || true; else seed_pip; \"$PY\" -m pip show \"$@\" || true; fi\n",
    "}\n",
    "\n",
    "pick_torch_index() {\n",
    "  # Derive Torch CUDA wheel index from CUDA_TAG (defaults are safe)\n",
    "  local tag=\"${CUDA_TAG:-12.8.0}\"\n",
    "  case \"$tag\" in\n",
    "    12.1* ) echo \"cu121\" ;;\n",
    "    12.4* ) echo \"cu124\" ;;\n",
    "    12.5*|12.6*|12.7*|12.8*|12.9* ) echo \"cu126\" ;;\n",
    "    * ) echo \"cu126\" ;;\n",
    "  esac\n",
    "}\n",
    "\n",
    "echo \"[gpu-bootstrap] whoami=$(whoami)\"\n",
    "echo \"[gpu-bootstrap] PY=$PY\"\n",
    "echo \"[gpu-bootstrap] UV_PROJECT_ENVIRONMENT=${UV_PROJECT_ENVIRONMENT}\"\n",
    "$PY - <<'PY'\n",
    "import sys, os\n",
    "print(\"[gpu-bootstrap] sys.executable:\", sys.executable)\n",
    "print(\"[gpu-bootstrap] sys.prefix:\", sys.prefix)\n",
    "print(\"[gpu-bootstrap] VIRTUAL_ENV:\", os.environ.get(\"VIRTUAL_ENV\",\"<unset>\"))\n",
    "for m in (\"jax\",\"jaxlib\",\"torch\"):\n",
    "    try:\n",
    "        mod = __import__(m)\n",
    "        print(f\"[gpu-bootstrap] pre: import {m}: OK from\", getattr(mod,\"__file__\",\"?\"))\n",
    "    except Exception as e:\n",
    "        print(f\"[gpu-bootstrap] pre: import {m}: FAIL -> {e.__class__.__name__}: {e}\")\n",
    "PY\n",
    "\n",
    "if _have_cmd nvidia-smi; then\n",
    "  echo \"[gpu-bootstrap] nvidia-smi present\"\n",
    "  nvidia-smi || true\n",
    "else\n",
    "  echo \"[gpu-bootstrap] nvidia-smi NOT present\"\n",
    "fi\n",
    "\n",
    "unset JAX_PLATFORM_NAME || true\n",
    "\n",
    "# --- 1) Ensure PyTorch (GPU) -------------------------------------------------\n",
    "if $PY -c \"import torch, sys; sys.exit(0 if torch.cuda.is_available() else 1)\" 2>/dev/null; then\n",
    "  echo \"[gpu-bootstrap] PyTorch with CUDA already present\"\n",
    "else\n",
    "  if _have_cmd nvidia-smi; then\n",
    "    IDX=\"$(pick_torch_index)\"\n",
    "    echo \"[gpu-bootstrap] Installing PyTorch (${IDX}) wheels into /app/.venv\"\n",
    "    PIP install --no-cache-dir torch torchvision torchaudio --index-url \"https://download.pytorch.org/whl/${IDX}\" || true\n",
    "  else\n",
    "    echo \"[gpu-bootstrap] Skipping PyTorch GPU install (no nvidia-smi)\"\n",
    "  fi\n",
    "fi\n",
    "\n",
    "# --- 2) Ensure JAX (GPU) *after* Torch so final cuDNN >= 9.8 -----------------\n",
    "JAX_OK=1\n",
    "if ! $PY - <<'PY'\n",
    "import sys\n",
    "try:\n",
    "    import jax\n",
    "    devs=jax.devices()\n",
    "    ok=any('gpu' in str(d).lower() or 'cuda' in str(d).lower() for d in devs)\n",
    "    print(\"[gpu-bootstrap] JAX devices pre:\", devs)\n",
    "    sys.exit(0 if ok else 1)\n",
    "except Exception as e:\n",
    "    print(\"[gpu-bootstrap] JAX import/probe error (pre):\", e)\n",
    "    sys.exit(2)\n",
    "PY\n",
    "then\n",
    "  JAX_OK=0\n",
    "fi\n",
    "\n",
    "if [ \"$JAX_OK\" -ne 1 ] && _have_cmd nvidia-smi; then\n",
    "  echo \"[gpu-bootstrap] Installing/repairing JAX CUDA wheels into /app/.venv\"\n",
    "  # JAX 0.6.0 requires cuDNN >= 9.8; this will install the plugin + nvidia-* libs\n",
    "  PIP install --no-cache-dir \"jax[cuda12]==0.6.0\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html || true\n",
    "\n",
    "  # Re-probe\n",
    "  $PY - <<'PY'\n",
    "import jax\n",
    "print(\"[gpu-bootstrap] JAX:\", jax.__version__, \"devices:\", jax.devices())\n",
    "PY\n",
    "fi\n",
    "\n",
    "echo \"[gpu-bootstrap] pip/uv show (post-install)\"\n",
    "PIP_SHOW jax jaxlib jax-cuda12-plugin jax-cuda12-pjrt torch torchvision torchaudio || true\n",
    "\n",
    "# Final snapshot\n",
    "$PY - <<'PY'\n",
    "for m in (\"torch\",\"jax\",\"jaxlib\"):\n",
    "    try:\n",
    "        mod = __import__(m)\n",
    "        print(f\"[gpu-bootstrap] post: import {m}: OK from\", getattr(mod, \"__file__\", \"?\"))\n",
    "    except Exception as e:\n",
    "        print(f\"[gpu-bootstrap] post: import {m}: FAIL -> {e.__class__.__name__}: {e}\")\n",
    "try:\n",
    "    import jax\n",
    "    print(\"[gpu-bootstrap] FINAL devices:\", jax.devices())\n",
    "except Exception as e:\n",
    "    print(\"[gpu-bootstrap] FINAL devices probe error:\", e)\n",
    "PY\n",
    "\n",
    "echo \"[gpu-bootstrap] END\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../.devcontainer/setup_env.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../.devcontainer/setup_env.sh\n",
    "#!/usr/bin/env sh\n",
    "set -eu\n",
    "if [ ! -f /workspace/.env ]; then\n",
    "  echo \"📝  Generating default .env from template\"\n",
    "  cp /workspace/.devcontainer/.env.template /workspace/.env\n",
    "fi\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../.devcontainer/docker-compose.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../.devcontainer/docker-compose.yml\n",
    "# .devcontainer/docker-compose.yml\n",
    "name: ${ENV_NAME:-docker_dev_template}\n",
    "\n",
    "services:\n",
    "  datascience:\n",
    "    build:\n",
    "      context: .\n",
    "      dockerfile: Dockerfile\n",
    "      args:\n",
    "        CUDA_TAG: ${CUDA_TAG:-12.8.0}\n",
    "        PYTHON_VER: ${PYTHON_VER:-3.10}\n",
    "        ENV_NAME: ${ENV_NAME:-docker_dev_template}\n",
    "        JAX_PREALLOCATE: ${XLA_PYTHON_CLIENT_PREALLOCATE:-true}\n",
    "        JAX_MEM_FRAC: ${XLA_PYTHON_CLIENT_MEM_FRACTION:-0.95}\n",
    "        JAX_ALLOCATOR: ${XLA_PYTHON_CLIENT_ALLOCATOR:-platform}\n",
    "        JAX_PREALLOC_LIMIT: ${JAX_PREALLOCATION_SIZE_LIMIT_BYTES:-8589934592}\n",
    "\n",
    "    restart: unless-stopped\n",
    "    depends_on:\n",
    "      mlflow:\n",
    "        condition: service_healthy\n",
    "\n",
    "    gpus: all\n",
    "    init: true\n",
    "\n",
    "    environment:\n",
    "      - PYTHON_VER=${PYTHON_VER}\n",
    "      - NVIDIA_VISIBLE_DEVICES=all\n",
    "      - NVIDIA_DRIVER_CAPABILITIES=compute,utility\n",
    "      - XLA_PYTHON_CLIENT_PREALLOCATE=${XLA_PYTHON_CLIENT_PREALLOCATE}\n",
    "      - XLA_PYTHON_CLIENT_ALLOCATOR=${XLA_PYTHON_CLIENT_ALLOCATOR}\n",
    "      - XLA_PYTHON_CLIENT_MEM_FRACTION=${XLA_PYTHON_CLIENT_MEM_FRACTION}\n",
    "      - XLA_FLAGS=${XLA_FLAGS}\n",
    "      - JAX_DISABLE_JIT=${JAX_DISABLE_JIT}\n",
    "      - JAX_ENABLE_X64=${JAX_ENABLE_X64}\n",
    "      - TF_FORCE_GPU_ALLOW_GROWTH=${TF_FORCE_GPU_ALLOW_GROWTH}\n",
    "      - JAX_PREALLOCATION_SIZE_LIMIT_BYTES=${JAX_PREALLOCATION_SIZE_LIMIT_BYTES}\n",
    "      - JUPYTER_TOKEN=${JUPYTER_TOKEN:-jupyter}\n",
    "      - INSTALL_TF=${INSTALL_TF:-0}\n",
    "      - UV_PROJECT_ENVIRONMENT=/app/.venv\n",
    "\n",
    "    volumes:\n",
    "      - ..:/workspace\n",
    "      - ../mlruns:/workspace/mlruns\n",
    "\n",
    "    ports:\n",
    "      - \"${HOST_JUPYTER_PORT:-8890}:8888\"\n",
    "      - \"${HOST_TENSORBOARD_PORT:-}:6008\"\n",
    "      - \"${HOST_EXPLAINER_PORT:-8050}:8050\"\n",
    "      - \"${HOST_STREAMLIT_PORT:-}:8501\"\n",
    "\n",
    "    # Start Jupyter only; framework installs are handled by postStart gpu_bootstrap.sh\n",
    "    command: >\n",
    "      bash -lc '\n",
    "        echo \"[boot] starting jupyter lab\";\n",
    "        jupyter lab --ip=0.0.0.0 --port=8888 --allow-root --NotebookApp.token=\"${JUPYTER_TOKEN}\" --NotebookApp.allow_origin=\"*\"\n",
    "      '\n",
    "\n",
    "    healthcheck:\n",
    "      test: [\"CMD-SHELL\", \"python - <<'PY'\\nimport sys, pkgutil\\nsys.exit(0 if pkgutil.find_loader('jupyterlab') else 1)\\nPY\"]\n",
    "      interval: 30s\n",
    "      timeout: 5s\n",
    "      retries: 3\n",
    "\n",
    "    labels:\n",
    "      - \"com.docker.compose.project=${ENV_NAME:-docker_dev_template}\"\n",
    "      - \"com.docker.compose.service=datascience\"\n",
    "      - \"description=AI/ML Dev Env (GPU)\"\n",
    "\n",
    "  mlflow:\n",
    "    image: ghcr.io/mlflow/mlflow:latest\n",
    "    command: >\n",
    "      mlflow server\n",
    "      --host 0.0.0.0\n",
    "      --port 5000\n",
    "      --backend-store-uri sqlite:///mlflow.db\n",
    "      --default-artifact-root /mlflow_artifacts\n",
    "    environment:\n",
    "      MLFLOW_EXPERIMENTS_DEFAULT_ARTIFACT_LOCATION: /mlflow_artifacts\n",
    "    volumes:\n",
    "      - ../mlruns:/mlflow_artifacts\n",
    "      - ../mlflow_db:/mlflow_db\n",
    "    ports:\n",
    "      - \"${HOST_MLFLOW_PORT:-5000}:5000\"\n",
    "    restart: unless-stopped\n",
    "    healthcheck:\n",
    "      test: [\"CMD-SHELL\", \"python - <<'PY'\\nimport requests,sys; requests.get('http://localhost:5000/health').raise_for_status()\\nPY\"]\n",
    "      interval: 10s\n",
    "      timeout: 3s\n",
    "      retries: 5\n",
    "      start_period: 30s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../.devcontainer/pyproject.toml\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../.devcontainer/pyproject.toml\n",
    "[project]\n",
    "name = \"docker_dev_template\"\n",
    "version = \"0.1.0\"\n",
    "description = \"Pytorch and Jax GPU docker container\"\n",
    "authors = [\n",
    "  { name = \"Geoffrey Hadfield\" },\n",
    "]\n",
    "license = \"MIT\"\n",
    "readme = \"README.md\"\n",
    "\n",
    "# ─── Restrict to Python 3.10–3.12 ──────────────────────────────\n",
    "requires-python = \">=3.10,<3.13\"\n",
    "\n",
    "dependencies = [\n",
    "  \"pandas>=1.2.0\",\n",
    "  \"numpy>=1.20.0\",\n",
    "  \"matplotlib>=3.4.0\",\n",
    "  \"mlflow>=2.10.2\",\n",
    "  \"mlflow-skinny>=2.10.2\",\n",
    "  \"scikit-learn>=1.4.2\",\n",
    "  \"pymc>=5.0.0\",\n",
    "  \"arviz>=0.14.0\",\n",
    "  \"statsmodels>=0.13.0\",\n",
    "  \"jupyterlab>=3.0.0\",\n",
    "  \"seaborn>=0.11.0\",\n",
    "  \"tabulate>=0.9.0\",\n",
    "  \"shap>=0.40.0\",\n",
    "  \"xgboost>=1.5.0\",\n",
    "  \"lightgbm>=3.3.0\",\n",
    "  \"catboost>=1.2.8,<1.3.0\",\n",
    "  \"scipy>=1.7.0\",\n",
    "  \"shapash[report]>=2.3.0\",\n",
    "  \"shapiq>=0.1.0\",\n",
    "  \"explainerdashboard==0.5.1\",\n",
    "  \"ipywidgets>=8.0.0\",\n",
    "  \"nutpie>=0.7.1\",   # new: nutpie backend for PyMC\n",
    "  \"numpyro>=0.18.0,<1.0.0\",\n",
    "  \"pytensor>=2.18.3\",  # explicit version for CUDA support\n",
    "  \"aesara>=2.9.4\",     # alternative backend option\n",
    "  \"tqdm>=4.67.0\",\n",
    "  \"pyarrow>=12.0.0\",\n",
    "  \"optuna>=3.0.0\",\n",
    "  \"optuna-integration[mlflow]>=0.2.0\",\n",
    "  \"omegaconf>=2.3.0,<2.4.0\",\n",
    "  \"hydra-core>=1.3.2,<1.4.0\",\n",
    "  \"fastapi>=0.104.0\",\n",
    "  \"uvicorn[standard]>=0.24.0\",\n",
    "  \"pydantic>=2.0.0\",\n",
    "  \"pydantic-settings\",\n",
    "]\n",
    "\n",
    "[project.optional-dependencies]\n",
    "dev = [\n",
    "  \"pytest>=7.0.0\",\n",
    "  \"black>=23.0.0\",\n",
    "  \"isort>=5.0.0\",\n",
    "  \"flake8>=5.0.0\",\n",
    "  \"mypy>=1.0.0\",\n",
    "  \"invoke>=2.2\",\n",
    "]\n",
    "\n",
    "cuda = [\n",
    "  \"cupy-cuda12x>=12.0.0\",  # For CUDA 12.x\n",
    "]\n",
    "\n",
    "[tool.pytensor]\n",
    "# Default configuration for PyTensor\n",
    "device = \"cuda\"          # Use CUDA by default if available\n",
    "floatX = \"float32\"       # Use float32 by default for better CUDA performance\n",
    "allow_gc = true          # Allow garbage collection\n",
    "optimizer = \"fast_run\"   # Fast run optimization by default\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../.devcontainer/tasks.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../.devcontainer/tasks.py\n",
    "# tasks.py  ── invoke ≥2.2\n",
    "from invoke import task, Context  # type: ignore\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "import tempfile\n",
    "import datetime as _dt\n",
    "import atexit\n",
    "import socket\n",
    "import contextlib\n",
    "import errno\n",
    "\n",
    "\n",
    "BASE_ENV = pathlib.Path(__file__).parent\n",
    "\n",
    "\n",
    "# Track temporary env files for cleanup\n",
    "_saved_env_files: List[str] = []\n",
    "\n",
    "\n",
    "def _parse_port(port: Union[str, int, None]) -> Optional[int]:\n",
    "    \"\"\"\n",
    "    Parse and validate a port number.\n",
    "    \n",
    "    Args:\n",
    "        port: Port number as string or int, or None\n",
    "        \n",
    "    Returns:\n",
    "        Validated port number as int, or None if input was None\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If port is invalid or out of range\n",
    "    \"\"\"\n",
    "    if port is None:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        port_int = int(port)\n",
    "        if not (0 < port_int < 65536):\n",
    "            raise ValueError(f\"Port {port_int} out of valid range (1-65535)\")\n",
    "        return port_int\n",
    "    except (TypeError, ValueError) as e:\n",
    "        raise ValueError(f\"Invalid port value: {port}\") from e\n",
    "\n",
    "\n",
    "def _first_free_port(start: int = 5200) -> int:\n",
    "    \"\"\"Return the first TCP port >= *start* that is unused on localhost.\"\"\"\n",
    "    print(f\"DEBUG: Searching for free port starting at {start}\")  # Debug\n",
    "    import socket\n",
    "    import contextlib\n",
    "    for port in range(start, 65535):\n",
    "        with contextlib.closing(socket.socket()) as s:\n",
    "            if s.connect_ex((\"127.0.0.1\", port)):\n",
    "                print(f\"DEBUG: Found free port {port}\")  # Debug\n",
    "                return port\n",
    "    raise RuntimeError(\"No free port found\")\n",
    "\n",
    "\n",
    "def _free_port(start=5200) -> int:\n",
    "    \"\"\"Find a free port by letting the OS assign one.\"\"\"\n",
    "    print(f\"DEBUG: Finding free port starting at {start}\")  # Debug\n",
    "    import socket\n",
    "    import contextlib\n",
    "    with contextlib.closing(\n",
    "        socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    ) as s:\n",
    "        s.bind(('', 0))\n",
    "        port = s.getsockname()[1]\n",
    "        print(f\"DEBUG: Found free port {port}\")  # Debug\n",
    "        return port\n",
    "\n",
    "\n",
    "def _port_free(host: str, port: int, timeout: float = 0.1) -> bool:\n",
    "    \"\"\"\n",
    "    Return True iff *host:port* is NOT in use.\n",
    "\n",
    "    Uses a non-blocking TCP connect – works on Linux, macOS, Windows,\n",
    "    inside or outside WSL – and does **not** rely on lsof / netstat.\n",
    "    \"\"\"\n",
    "    print(f\"DEBUG: Checking if port {port} is free on {host}\")  # Debug\n",
    "    try:\n",
    "        with contextlib.closing(\n",
    "            socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        ) as s:\n",
    "            s.settimeout(timeout)\n",
    "            s.connect((host, port))\n",
    "            print(f\"DEBUG: Port {port} is in use\")  # Debug\n",
    "            return False      # connection succeeded ⇒ something listening\n",
    "    except (OSError, socket.timeout):\n",
    "        print(f\"DEBUG: Port {port} is free\")  # Debug\n",
    "        return True           # connection failed ⇒ port is free\n",
    "\n",
    "\n",
    "def _find_port(preferred: int, start: int = 5200) -> int:\n",
    "    \"\"\"\n",
    "    Try to use preferred port, fall back to finding first available port.\n",
    "    \n",
    "    Args:\n",
    "        preferred: The preferred port number to try first\n",
    "        start: Where to start searching if preferred port is taken\n",
    "        \n",
    "    Returns:\n",
    "        An available port number\n",
    "    \"\"\"\n",
    "    print(f\"DEBUG: Trying preferred port {preferred}\")  # Debug\n",
    "    if _port_free(\"127.0.0.1\", preferred):\n",
    "        return preferred\n",
    "    return _first_free_port(start)\n",
    "\n",
    "\n",
    "def _write_envfile(name: str, \n",
    "                   ports: Optional[dict[str, int]] = None) -> pathlib.Path:\n",
    "    \"\"\"\n",
    "    Create a throw-away .env file for the current `invoke up` run.\n",
    "    \n",
    "    Docker-compose will use this to see the chosen host-ports. We include all\n",
    "    services we know about; anything unset falls back to .env.template defaults.\n",
    "    \"\"\"\n",
    "    env_lines = [f\"ENV_NAME={name}\"]\n",
    "    mapping = {\n",
    "        \"jupyter\": \"HOST_JUPYTER_PORT\",\n",
    "        \"tensorboard\": \"HOST_TENSORBOARD_PORT\",\n",
    "        \"explainer\": \"HOST_EXPLAINER_PORT\",\n",
    "        \"streamlit\": \"HOST_STREAMLIT_PORT\",\n",
    "        \"mlflow\": \"HOST_MLFLOW_PORT\",      # NEW\n",
    "    }\n",
    "    for svc, var in mapping.items():\n",
    "        if ports and svc in ports:\n",
    "            env_lines.append(f\"{var}={ports[svc]}\")\n",
    "    env_lines.append(f\"# generated {_dt.datetime.now().isoformat()}\")\n",
    "    tmp = tempfile.NamedTemporaryFile(\n",
    "        \"w\", \n",
    "        delete=False, \n",
    "        prefix=\".env.\",\n",
    "        dir=BASE_ENV\n",
    "    )\n",
    "    tmp.write(\"\\n\".join(env_lines))\n",
    "    tmp.close()\n",
    "    _saved_env_files.append(tmp.name)\n",
    "    return pathlib.Path(tmp.name)\n",
    "\n",
    "\n",
    "# Register cleanup function\n",
    "def _cleanup_env_files() -> None:\n",
    "    \"\"\"Remove all temporary env files.\"\"\"\n",
    "    for path in _saved_env_files:\n",
    "        try:\n",
    "            os.remove(path)\n",
    "        except OSError:\n",
    "            pass\n",
    "\n",
    "\n",
    "atexit.register(_cleanup_env_files)\n",
    "\n",
    "\n",
    "def _compose(\n",
    "    c: Context,\n",
    "    cmd: str,\n",
    "    name: str,\n",
    "    rebuild: bool = False,\n",
    "    force_pty: bool = False,\n",
    "    ports: Optional[dict[str, int]] = None,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Wrapper around `docker compose` that also sanity-checks host ports.\n",
    "    \"\"\"\n",
    "    # ---------- NEW pre-flight check --------------------------------------\n",
    "    if ports:\n",
    "        for svc, port in ports.items():\n",
    "            if port is None:\n",
    "                continue\n",
    "            if not _port_free(\"127.0.0.1\", int(port)):\n",
    "                print(f\"❌  Host port {port} already bound – \"\n",
    "                      f\"{svc} cannot start. Choose another port (invoke up \"\n",
    "                      f\"--{svc}-port XXXXX) or free it first.\")\n",
    "                sys.exit(1)\n",
    "\n",
    "    env = {**os.environ, \"ENV_NAME\": name, \"COMPOSE_PROJECT_NAME\": name}\n",
    "    \n",
    "    # Add port overrides if provided\n",
    "    if ports:\n",
    "        port_mapping = {\n",
    "            \"jupyter\": \"HOST_JUPYTER_PORT\",\n",
    "            \"tensorboard\": \"HOST_TENSORBOARD_PORT\", \n",
    "            \"explainer\": \"HOST_EXPLAINER_PORT\",\n",
    "            \"streamlit\": \"HOST_STREAMLIT_PORT\",\n",
    "        }\n",
    "        for service, port in ports.items():\n",
    "            if service in port_mapping:\n",
    "                env[port_mapping[service]] = str(port)\n",
    "    \n",
    "    use_pty = force_pty or (os.name != \"nt\" and sys.stdin.isatty())\n",
    "\n",
    "    if not use_pty and not getattr(_compose, \"_warned\", False):\n",
    "        print(\"ℹ️  PTY not supported – running without TTY.\")\n",
    "        _compose._warned = True  # type: ignore[attr-defined]\n",
    "\n",
    "    if rebuild:\n",
    "        full_cmd = f\"docker compose -p {name} {cmd} --build\"\n",
    "    else:\n",
    "        full_cmd = f\"docker compose -p {name} {cmd}\"\n",
    "    c.run(full_cmd, env=env, pty=use_pty)\n",
    "\n",
    "\n",
    "@task(\n",
    "    help={\n",
    "        \"name\": \"Project/venv name (defaults to folder name)\",\n",
    "        \"use_pty\": \"Force PTY even on non-POSIX hosts\",\n",
    "        \"jupyter_port\": \"Jupyter Lab port (default: 8890)\",\n",
    "        \"tensorboard_port\": \"TensorBoard port (default: auto-assigned)\",\n",
    "        \"explainer_port\": \"Explainer Dashboard port (default: auto-assigned)\", \n",
    "        \"streamlit_port\": \"Streamlit port (default: auto-assigned)\",\n",
    "        \"mlflow_port\": \"MLflow UI port (default: 5000, auto-assigns if busy)\",\n",
    "    }\n",
    ")\n",
    "def up(\n",
    "    c,\n",
    "    name: Optional[str] = None,\n",
    "    rebuild: bool = False,\n",
    "    detach: bool = True,\n",
    "    use_pty: bool = False,\n",
    "    jupyter_port: Union[str, int, None] = None,\n",
    "    tensorboard_port: Union[str, int, None] = None,\n",
    "    explainer_port: Union[str, int, None] = None,\n",
    "    streamlit_port: Union[str, int, None] = None,\n",
    "    mlflow_port: Union[str, int, None] = None,\n",
    ") -> None:\n",
    "    \"\"\"Build (optionally --rebuild) & start the container with custom ports.\"\"\"\n",
    "    name = name or BASE_ENV.name\n",
    "\n",
    "    # ---------- Parse and validate all ports -----------------\n",
    "    try:\n",
    "        jupyter_port = _parse_port(jupyter_port)\n",
    "        tensorboard_port = _parse_port(tensorboard_port)\n",
    "        explainer_port = _parse_port(explainer_port)\n",
    "        streamlit_port = _parse_port(streamlit_port)\n",
    "        mlflow_port = _parse_port(mlflow_port)\n",
    "    except ValueError as e:\n",
    "        print(f\"❌ Port validation failed: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # ---------- build dynamic port map -----------------\n",
    "    ports = {}\n",
    "    if jupyter_port is not None:\n",
    "        ports[\"jupyter\"] = jupyter_port\n",
    "    if tensorboard_port is not None:\n",
    "        ports[\"tensorboard\"] = tensorboard_port\n",
    "    if explainer_port is not None:\n",
    "        ports[\"explainer\"] = explainer_port\n",
    "    if streamlit_port is not None:\n",
    "        ports[\"streamlit\"] = streamlit_port\n",
    "\n",
    "    # ---------- Explainer auto-assign (NEW) ------------\n",
    "    print(\"DEBUG: Starting explainer port assignment\")  # Debug\n",
    "    try:\n",
    "        # Try to use the explainer's version first\n",
    "        from src.mlops.explainer import _first_free_port  # type: ignore\n",
    "        print(\"DEBUG: Successfully imported _first_free_port from explainer\")  # Debug\n",
    "    except ModuleNotFoundError:\n",
    "        print(\"DEBUG: Failed to import _first_free_port, using local implementation\")  # Debug\n",
    "        # We'll use our local _first_free_port implementation\n",
    "        pass\n",
    "\n",
    "    if explainer_port is None:\n",
    "        print(\"DEBUG: No explainer port specified, finding one\")  # Debug\n",
    "        explainer_port = _find_port(8050, 5200)\n",
    "    elif not _port_free(\"127.0.0.1\", explainer_port):\n",
    "        print(f\"DEBUG: Specified explainer port {explainer_port} is in use\")  # Debug\n",
    "        sys.exit(1)\n",
    "    ports[\"explainer\"] = explainer_port\n",
    "    print(f\"🔌 Explainer host-port → {explainer_port}\")\n",
    "\n",
    "    # ----- MLflow auto-assign (default 5000) -----------\n",
    "    print(\"DEBUG: Starting MLflow port assignment\")  # Debug\n",
    "    if mlflow_port is None:\n",
    "        print(\"DEBUG: No MLflow port specified, finding one\")  # Debug\n",
    "        mlflow_port = _find_port(5000, 5200)\n",
    "    elif not _port_free(\"127.0.0.1\", mlflow_port):\n",
    "        print(f\"DEBUG: Specified MLflow port {mlflow_port} is in use\")  # Debug\n",
    "        sys.exit(1)\n",
    "    ports[\"mlflow\"] = mlflow_port\n",
    "    print(f\"🔌 MLflow host-port → {mlflow_port}\")\n",
    "\n",
    "    # Generate environment file\n",
    "    env_path = _write_envfile(name, ports)\n",
    "    compose_cmd = \"up -d\" if detach else \"up\"\n",
    "\n",
    "    _compose(\n",
    "        c,\n",
    "        f\"--env-file {env_path} {compose_cmd}\",\n",
    "        name,\n",
    "        rebuild=rebuild,\n",
    "        force_pty=use_pty,\n",
    "        ports=ports,\n",
    "    )\n",
    "\n",
    "\n",
    "@task(\n",
    "    help={\n",
    "        \"name\": \"Project/venv name (defaults to folder name)\",\n",
    "    }\n",
    ")\n",
    "def stop(c, name: Optional[str] = None) -> None:\n",
    "    \"\"\"Stop and remove dev container (keeps volumes).\"\"\"\n",
    "    name = name or BASE_ENV.name\n",
    "    cmd = f\"docker compose -p {name} down\"\n",
    "    try:\n",
    "        c.run(cmd)\n",
    "        print(f\"\\n🛑 Stopped and removed project '{name}'\")\n",
    "    except Exception:\n",
    "        print(f\"❌ No running containers found for project '{name}'\")\n",
    "\n",
    "\n",
    "@task\n",
    "def shell(c, name: str | None = None) -> None:\n",
    "    \"\"\"Open an interactive shell inside the running container.\"\"\"\n",
    "    name = name or BASE_ENV.name\n",
    "    cmd = f\"docker compose -p {name} ps -q datascience\"\n",
    "    cid = c.run(cmd, hide=True).stdout.strip()\n",
    "    c.run(f\"docker exec -it {cid} bash\", env={\"ENV_NAME\": name}, pty=False)\n",
    "\n",
    "\n",
    "@task\n",
    "def clean(c) -> None:\n",
    "    \"\"\"Prune stopped containers + dangling images.\"\"\"\n",
    "    c.run(\"docker system prune -f\")\n",
    "\n",
    "\n",
    "@task\n",
    "def ports(c, name: str | None = None) -> None:\n",
    "    \"\"\"Show current port mappings for the named project.\"\"\"\n",
    "    name = name or BASE_ENV.name\n",
    "    cmd = f\"docker compose -p {name} ps --format table\"\n",
    "    try:\n",
    "        c.run(cmd, hide=False)\n",
    "        print(f\"\\n📊 Port mappings for project '{name}':\")\n",
    "        print(\"=\" * 50)\n",
    "    except Exception:\n",
    "        print(f\"❌ No running containers found for project '{name}'\")\n",
    "        print(\"\\n💡 Usage examples:\")\n",
    "        print(\"  invoke up --name myproject --jupyter-port 8891\")\n",
    "        print(\"  invoke up --name myproject --jupyter-port 8892 \\\\\")\n",
    "        print(\"    --tensorboard-port 6009\")\n",
    "\n",
    "\n",
    "# --- utilities ---------------------------------------------------------------\n",
    "def _norm(path: str | pathlib.Path) -> str:\n",
    "    \"\"\"Return a lower-case, forward-slash, no-trailing-slash version of *path*.\"\"\"\n",
    "    p = str(path).replace(\"\\\\\", \"/\").rstrip(\"/\").lower()\n",
    "    return p\n",
    "\n",
    "def _docker_projects_from_this_repo() -> set[str]:\n",
    "    \"\"\"\n",
    "    Discover every Compose *project name* whose working_dir label ends with\n",
    "    the current repo path.\n",
    "\n",
    "    Works across Windows ↔ WSL ↔ macOS because we do suffix-match on a\n",
    "    normalised path.\n",
    "    \"\"\"\n",
    "    here_tail = _norm(pathlib.Path(__file__).parent.resolve())\n",
    "    cmd = (\n",
    "        \"docker container ls -a \"\n",
    "        \"--format '{{.Label \\\"com.docker.compose.project\\\"}} \"\n",
    "        \"{{.Label \\\"com.docker.compose.project.working_dir\\\"}}' \"\n",
    "        \"--filter label=com.docker.compose.project\"\n",
    "    )\n",
    "    projects: set[str] = set()\n",
    "    for line in os.popen(cmd).read().strip().splitlines():\n",
    "        try:\n",
    "            proj, wd = line.split(maxsplit=1)\n",
    "        except ValueError:\n",
    "            continue\n",
    "        if _norm(wd).endswith(here_tail):\n",
    "            projects.add(proj)\n",
    "    return projects\n",
    "\n",
    "# --- task --------------------------------------------------------------------\n",
    "@task(\n",
    "    help={\n",
    "        \"name\": \"Project name (defaults to folder). Ignored with --all.\",\n",
    "        \"all\":  \"Remove *all* projects launched from this repo.\",\n",
    "        \"rmi\":  \"Image-removal policy: all | local | none (default: local).\",\n",
    "    }\n",
    ")\n",
    "def down(c, name: str | None = None, all: bool = False, rmi: str = \"local\"):\n",
    "    \"\"\"\n",
    "    Stop containers **and** fully delete every artefact so next `invoke up`\n",
    "    starts from a clean slate.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    invoke down                  # nuke current-folder project\n",
    "    invoke down --name ml_project --rmi all   # wipe everything for ml_project\n",
    "    invoke down --all            # tear down every project from this repo\n",
    "    \"\"\"\n",
    "    if rmi not in {\"all\", \"local\", \"none\"}:\n",
    "        raise ValueError(\"--rmi must be all | local | none\")\n",
    "\n",
    "    targets = _docker_projects_from_this_repo() if all else {name or BASE_ENV.name}\n",
    "    flags = \"-v --remove-orphans\"\n",
    "    if rmi != \"none\":\n",
    "        flags += f\" --rmi {rmi}\"\n",
    "\n",
    "    for proj in targets:\n",
    "        try:\n",
    "            c.run(f\"docker compose -p {proj} down {flags}\")\n",
    "            print(f\"🗑️  Removed project '{proj}'\")\n",
    "        except Exception:\n",
    "            print(f\"⚠️  Nothing to remove for '{proj}'\")\n",
    "\n",
    "\n",
    "@task(\n",
    "    help={\n",
    "        \"yaml\": \"Path to dashboard.yaml file\",\n",
    "        \"port\": \"Port to serve on (default: 8150)\",\n",
    "        \"host\": \"Host to bind to (default: 0.0.0.0)\",\n",
    "    }\n",
    ")\n",
    "def dashboard(c, yaml: str, port: int = 8150, host: str = \"0.0.0.0\") -> None:\n",
    "    \"\"\"\n",
    "    Serve a saved ExplainerDashboard from a YAML configuration file.\n",
    "    \n",
    "    This task allows you to re-serve dashboards that were previously saved\n",
    "    with build_and_log_dashboard(save_yaml=True).\n",
    "    \n",
    "    Examples:\n",
    "        invoke dashboard --yaml dashboard.yaml\n",
    "        invoke dashboard --yaml dashboard.yaml --port 8200\n",
    "    \"\"\"\n",
    "    import sys\n",
    "    from pathlib import Path\n",
    "    from src.mlops.explainer import load_dashboard_yaml\n",
    "    \n",
    "    yaml_path = Path(yaml)\n",
    "    if not yaml_path.exists():\n",
    "        print(f\"❌ Dashboard YAML file not found: {yaml_path}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Check if port is available\n",
    "    if not _port_free(host, port):\n",
    "        print(f\"❌ Port {port} is already in use on {host}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    try:\n",
    "        print(f\"🔄 Loading dashboard from {yaml_path}\")\n",
    "        dashboard_obj = load_dashboard_yaml(yaml_path)\n",
    "        \n",
    "        print(f\"🌐 Serving ExplainerDashboard on {host}:{port}\")\n",
    "        dashboard_obj.run(port=port, host=host, use_waitress=True, open_browser=False)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load or serve dashboard: {e}\")\n",
    "        sys.exit(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../tests/diagnose_devcontainer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../tests/diagnose_devcontainer.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Comprehensive diagnostic script for dev container issues.\n",
    "Run this inside the container to diagnose Python environment and remote extension problems.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def run_command(cmd, description):\n",
    "    \"\"\"Run a command and return its output.\"\"\"\n",
    "    print(f\"\\n🔍 {description}\")\n",
    "    print(\"=\" * 60)\n",
    "    try:\n",
    "        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(f\"✅ {result.stdout.strip()}\")\n",
    "        else:\n",
    "            print(f\"❌ Error (code {result.returncode}): {result.stderr.strip()}\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Exception: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def check_paths_and_environment():\n",
    "    \"\"\"Check Python paths and environment variables.\"\"\"\n",
    "    print(\"\\n🐍 PYTHON ENVIRONMENT DIAGNOSTICS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Python executable and version\n",
    "    print(f\"Python executable: {sys.executable}\")\n",
    "    print(f\"Python version: {sys.version}\")\n",
    "    print(f\"Python path: {sys.path[:3]}...\")  # First few paths\n",
    "    \n",
    "    # Environment variables\n",
    "    print(f\"\\nVIRTUAL_ENV: {os.environ.get('VIRTUAL_ENV', 'Not set')}\")\n",
    "    print(f\"PATH (first 3): {':'.join(os.environ.get('PATH', '').split(':')[:3])}\")\n",
    "    \n",
    "    # Virtual environment validation\n",
    "    venv_path = Path('/app/.venv')\n",
    "    if venv_path.exists():\n",
    "        print(f\"✅ Virtual environment exists at {venv_path}\")\n",
    "        print(f\"   - bin directory: {list(venv_path.glob('bin/python*'))}\")\n",
    "        print(f\"   - site-packages: {(venv_path / 'lib/python3.10/site-packages').exists()}\")\n",
    "    else:\n",
    "        print(f\"❌ Virtual environment NOT found at {venv_path}\")\n",
    "\n",
    "\n",
    "def check_key_packages():\n",
    "    \"\"\"Check if key packages are importable.\"\"\"\n",
    "    print(\"\\n📦 PACKAGE IMPORT TESTS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    packages = [\n",
    "        'jax', 'torch', 'numpy', 'pandas', 'matplotlib', \n",
    "        'jupyterlab', 'streamlit', 'sklearn'\n",
    "    ]\n",
    "    \n",
    "    for package in packages:\n",
    "        try:\n",
    "            if package == 'sklearn':\n",
    "                import sklearn\n",
    "                version = sklearn.__version__\n",
    "            else:\n",
    "                module = __import__(package)\n",
    "                version = getattr(module, '__version__', 'unknown')\n",
    "            print(f\"✅ {package}: {version}\")\n",
    "        except ImportError as e:\n",
    "            print(f\"❌ {package}: Import failed - {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  {package}: {e}\")\n",
    "\n",
    "\n",
    "def check_gpu_environment():\n",
    "    \"\"\"Check GPU-related environment variables.\"\"\"\n",
    "    print(\"\\n🎮 GPU ENVIRONMENT VARIABLES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    gpu_env_vars = [\n",
    "        'XLA_PYTHON_CLIENT_PREALLOCATE',\n",
    "        'XLA_PYTHON_CLIENT_ALLOCATOR', \n",
    "        'XLA_PYTHON_CLIENT_MEM_FRACTION',\n",
    "        'JAX_PLATFORM_NAME',\n",
    "        'XLA_FLAGS',\n",
    "        'JAX_DISABLE_JIT',\n",
    "        'JAX_ENABLE_X64',\n",
    "        'JAX_PREALLOCATION_SIZE_LIMIT_BYTES',\n",
    "        'TF_FORCE_GPU_ALLOW_GROWTH',\n",
    "        'NVIDIA_VISIBLE_DEVICES',\n",
    "        'NVIDIA_DRIVER_CAPABILITIES'\n",
    "    ]\n",
    "    \n",
    "    for var in gpu_env_vars:\n",
    "        value = os.environ.get(var, 'Not set')\n",
    "        print(f\"   {var}: {value}\")\n",
    "\n",
    "\n",
    "def check_gpu_support():\n",
    "    \"\"\"Check GPU support for JAX and PyTorch with enhanced diagnostics.\"\"\"\n",
    "    print(\"\\n🎮 ENHANCED GPU SUPPORT CHECK\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # JAX GPU check with detailed info\n",
    "    try:\n",
    "        import jax\n",
    "        print(f\"JAX version: {jax.__version__}\")\n",
    "        \n",
    "        devices = jax.devices()\n",
    "        print(f\"JAX devices: {devices}\")\n",
    "        \n",
    "        if devices:\n",
    "            for i, device in enumerate(devices):\n",
    "                print(f\"   Device {i}: {device}\")\n",
    "                \n",
    "        if any('gpu' in str(device).lower() or 'cuda' in str(device).lower() for device in devices):\n",
    "            print(\"✅ JAX GPU/CUDA support detected!\")\n",
    "            \n",
    "            # Test a simple computation\n",
    "            try:\n",
    "                import jax.numpy as jnp\n",
    "                x = jnp.ones((1000, 1000))\n",
    "                result = jnp.sum(x)\n",
    "                print(f\"   ✅ JAX GPU computation test passed: sum = {result}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ⚠️  JAX GPU computation test failed: {e}\")\n",
    "        else:\n",
    "            print(\"⚠️  JAX GPU support not detected\")\n",
    "            print(\"   This might be due to GPU architecture compatibility\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ JAX GPU check failed: {e}\")\n",
    "    \n",
    "    # PyTorch GPU check with enhanced info\n",
    "    try:\n",
    "        import torch\n",
    "        print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "        print(f\"PyTorch CUDA available: {torch.cuda.is_available()}\")\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            device_count = torch.cuda.device_count()\n",
    "            print(f\"✅ PyTorch CUDA device count: {device_count}\")\n",
    "            \n",
    "            for i in range(device_count):\n",
    "                try:\n",
    "                    device_name = torch.cuda.get_device_name(i)\n",
    "                    memory_total = torch.cuda.get_device_properties(i).total_memory\n",
    "                    print(f\"   Device {i}: {device_name}\")\n",
    "                    print(f\"     Total memory: {memory_total / (1024**3):.1f} GB\")\n",
    "                except Exception as e:\n",
    "                    print(f\"   Device {i}: Error getting info - {e}\")\n",
    "            \n",
    "            # Test a simple computation\n",
    "            try:\n",
    "                device = torch.device('cuda:0')\n",
    "                x = torch.ones(1000, 1000, device=device)\n",
    "                result = torch.sum(x)\n",
    "                print(f\"   ✅ PyTorch GPU computation test passed: sum = {result}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ⚠️  PyTorch GPU computation test failed: {e}\")\n",
    "        else:\n",
    "            print(\"⚠️  PyTorch CUDA not available\")\n",
    "            print(\"   Check CUDA installation and GPU compatibility\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ PyTorch GPU check failed: {e}\")\n",
    "\n",
    "\n",
    "def check_workspace_mount():\n",
    "    \"\"\"Check if workspace is properly mounted.\"\"\"\n",
    "    print(\"\\n📁 WORKSPACE MOUNT CHECK\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    workspace_path = Path('/workspace')\n",
    "    if workspace_path.exists():\n",
    "        print(f\"✅ /workspace directory exists\")\n",
    "        try:\n",
    "            contents = list(workspace_path.iterdir())[:10]  # First 10 items\n",
    "            print(f\"   Contents (first 10): {[p.name for p in contents]}\")\n",
    "            \n",
    "            # Check for specific expected files\n",
    "            expected_files = ['.devcontainer', 'pyproject.toml', 'docker-compose.yml']\n",
    "            for file in expected_files:\n",
    "                if (workspace_path / file).exists():\n",
    "                    print(f\"   ✅ Found: {file}\")\n",
    "                else:\n",
    "                    print(f\"   ❌ Missing: {file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Error reading workspace: {e}\")\n",
    "    else:\n",
    "        print(f\"❌ /workspace directory does not exist\")\n",
    "\n",
    "\n",
    "def check_dev_container_config():\n",
    "    \"\"\"Check dev container configuration.\"\"\"\n",
    "    print(\"\\n⚙️  DEV CONTAINER CONFIG CHECK\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    config_path = Path('/workspace/.devcontainer/devcontainer.json')\n",
    "    if config_path.exists():\n",
    "        print(\"✅ devcontainer.json found\")\n",
    "        try:\n",
    "            with open(config_path) as f:\n",
    "                config = json.load(f)\n",
    "            print(f\"   Name: {config.get('name', 'Not specified')}\")\n",
    "            print(f\"   Python path: {config.get('customizations', {}).get('vscode', {}).get('settings', {}).get('python.defaultInterpreterPath', 'Not specified')}\")\n",
    "            print(f\"   Workspace folder: {config.get('workspaceFolder', 'Not specified')}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Error reading config: {e}\")\n",
    "    else:\n",
    "        print(\"❌ devcontainer.json not found\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run all diagnostic checks.\"\"\"\n",
    "    print(\"🔍 DEV CONTAINER COMPREHENSIVE DIAGNOSTICS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Running from: {os.getcwd()}\")\n",
    "    print(f\"User: {os.getenv('USER', 'unknown')}\")\n",
    "    print(f\"Container hostname: {os.getenv('HOSTNAME', 'unknown')}\")\n",
    "    \n",
    "    # System commands\n",
    "    run_command(\"uv --version\", \"UV Version\")\n",
    "    run_command(\"which python\", \"Python Location\")\n",
    "    run_command(\"ls -la /app/.venv/\", \"Virtual Environment Contents\")\n",
    "    run_command(\"mount | grep workspace\", \"Workspace Mount Status\")\n",
    "    run_command(\"nvidia-smi\", \"NVIDIA GPU Status\")\n",
    "    \n",
    "    # Python-based checks\n",
    "    check_paths_and_environment()\n",
    "    check_gpu_environment()\n",
    "    check_key_packages()\n",
    "    check_gpu_support()\n",
    "    check_workspace_mount()\n",
    "    check_dev_container_config()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"🎯 SUMMARY & RECOMMENDATIONS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"If you see issues:\")\n",
    "    print(\"1. ❌ Virtual env missing → Check Dockerfile uv sync step\")\n",
    "    print(\"2. ❌ Workspace not mounted → Check devcontainer.json mounts config\")\n",
    "    print(\"3. ❌ Packages missing → Check uv.lock and pip install steps\")\n",
    "    print(\"4. ⚠️  GPU not detected → Check docker-compose.yml gpu settings\")\n",
    "    print(\"5. 🔧 For VS Code issues → Check python.defaultInterpreterPath setting\")\n",
    "    print(\"6. 🎮 For GPU issues → Check NVIDIA drivers and CUDA compatibility\")\n",
    "    print(\"\\n✅ All checks passed = ready for development!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../tests/test_pytorch_jax_gpu.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../tests/test_pytorch_jax_gpu.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Test script to verify that PyTorch and JAX can access the GPU,\n",
    "and that PyJAGS is working correctly.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "def test_pytorch_gpu():\n",
    "    \"\"\"Test PyTorch GPU availability and basic operations.\"\"\"\n",
    "    print(\"\\n=== Testing PyTorch GPU ===\")\n",
    "    try:\n",
    "        import torch\n",
    "        print(f\"PyTorch version: {torch.__version__}\")\n",
    "        print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "        \n",
    "        if not torch.cuda.is_available():\n",
    "            print(\"❌ PyTorch CUDA not available!\")\n",
    "            return False\n",
    "        \n",
    "        print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "        print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "        print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "        \n",
    "        # Run a simple test computation\n",
    "        x = torch.rand(1000, 1000).cuda()\n",
    "        y = torch.rand(1000, 1000).cuda()\n",
    "        start = torch.cuda.Event(enable_timing=True)\n",
    "        end = torch.cuda.Event(enable_timing=True)\n",
    "        \n",
    "        start.record()\n",
    "        z = torch.matmul(x, y)\n",
    "        end.record()\n",
    "        \n",
    "        # Wait for GPU computation to finish\n",
    "        torch.cuda.synchronize()\n",
    "        print(f\"Matrix multiplication time: {start.elapsed_time(end):.2f} ms\")\n",
    "        print(f\"Result shape: {z.shape}\")\n",
    "        print(\"✅ PyTorch GPU test passed!\")\n",
    "        return True\n",
    "    \n",
    "    except ImportError:\n",
    "        print(\"❌ PyTorch not found!\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during PyTorch GPU test: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def test_jax_gpu():\n",
    "    \"\"\"Test JAX GPU availability and basic operations.\"\"\"\n",
    "    print(\"\\n=== Testing JAX GPU ===\")\n",
    "    try:\n",
    "        import jax, jax.numpy as jnp\n",
    "        print(f\"JAX version: {jax.__version__}\")\n",
    "\n",
    "        # Probe both: this won't crash if GPU plugin isn't present\n",
    "        gpu_devs = []\n",
    "        try:\n",
    "            gpu_devs = jax.devices(\"gpu\")\n",
    "        except Exception:\n",
    "            # Some builds don't register 'gpu' backend explicitly\n",
    "            pass\n",
    "\n",
    "        if not gpu_devs:\n",
    "            # Fallback: inspect all devices for cuda/gpu strings\n",
    "            devs = jax.devices()\n",
    "            gpu_devs = [d for d in devs if \"gpu\" in str(d).lower() or \"cuda\" in str(d).lower()]\n",
    "\n",
    "        if not gpu_devs:\n",
    "            print(\"❌ No GPU devices found by JAX!\")\n",
    "            return False\n",
    "\n",
    "        print(f\"Available JAX GPU devices: {gpu_devs}\")\n",
    "        @jax.jit\n",
    "        def matmul(a, b): return jnp.matmul(a, b)\n",
    "        x = jnp.ones((1024, 1024))\n",
    "        y = jnp.ones((1024, 1024))\n",
    "        result = matmul(x, y)\n",
    "        print(f\"Result shape: {result.shape}\")\n",
    "        print(\"✅ JAX GPU test passed!\")\n",
    "        return True\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"❌ JAX not found!\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during JAX GPU test: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "def test_pyjags():\n",
    "    \"\"\"Test PyJAGS installation and basic functionality.\"\"\"\n",
    "    print(\"\\n=== Testing PyJAGS ===\")\n",
    "    try:\n",
    "        import pyjags\n",
    "        print(f\"PyJAGS version: {pyjags.__version__}\")\n",
    "        \n",
    "        # Create a simple model to verify that PyJAGS works\n",
    "        code = \"\"\"\n",
    "        model {\n",
    "            # Likelihood\n",
    "            y ~ dnorm(mu, 1/sigma^2)\n",
    "            \n",
    "            # Priors\n",
    "            mu ~ dnorm(0, 0.001)\n",
    "            sigma ~ dunif(0, 100)\n",
    "        }\n",
    "        \"\"\"\n",
    "        \n",
    "        # Sample data\n",
    "        data = {'y': 0.5}\n",
    "        \n",
    "        # Initialize model with data\n",
    "        model = pyjags.Model(code, data=data, chains=1, adapt=100)\n",
    "        print(\"JAGS model initialized successfully!\")\n",
    "        \n",
    "        # Sample from the model\n",
    "        samples = model.sample(200, vars=['mu', 'sigma'])\n",
    "        print(\"JAGS sampling completed successfully!\")\n",
    "        \n",
    "        # Verify the samples\n",
    "        mu_samples = samples['mu']\n",
    "        sigma_samples = samples['sigma']\n",
    "        print(f\"mu mean: {mu_samples.mean():.4f}\")\n",
    "        print(f\"sigma mean: {sigma_samples.mean():.4f}\")\n",
    "        \n",
    "        print(\"✅ PyJAGS test passed!\")\n",
    "        return True\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"❌ PyJAGS not found!\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during PyJAGS test: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Running GPU and PyJAGS verification tests...\")\n",
    "    \n",
    "    pytorch_success = test_pytorch_gpu()\n",
    "    jax_success = test_jax_gpu()\n",
    "    pyjags_success = test_pyjags()\n",
    "    \n",
    "    print(\"\\n=== Test Summary ===\")\n",
    "    print(f\"PyTorch GPU: {'✅ PASS' if pytorch_success else '❌ FAIL'}\")\n",
    "    print(f\"JAX GPU: {'✅ PASS' if jax_success else '❌ FAIL'}\")\n",
    "    print(f\"PyJAGS: {'✅ PASS' if pyjags_success else '❌ FAIL'}\")\n",
    "    \n",
    "    if pytorch_success and jax_success and pyjags_success:\n",
    "        print(\"\\n🎉 All tests passed! The container is working correctly.\")\n",
    "        sys.exit(0)\n",
    "    else:\n",
    "        print(\"\\n❌ Some tests failed. Please check the output for details.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
