{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Streamlined Docker Development Environment Setup\n",
        "\n",
        "This notebook sets up a streamlined Docker-based development environment with:\n",
        "- PyTorch with GPU support (official base image)\n",
        "- UV package manager for fast dependency resolution\n",
        "- VS Code devcontainer integration\n",
        "- Simplified configuration and testing\n",
        "\n",
        "**Key improvements over previous setup:**\n",
        "- ~40% faster build time using official PyTorch base image\n",
        "- Simplified environment configuration\n",
        "- PyTorch/Jax-focused GPU acceleration\n",
        "\n",
        "goal: have a container that can utilize jax and pytorch on gpu on a nvidia 4090 gpu. Let's ensure that each of the requirements are set and that once in the container it have the uv environemnt set up as well for development. It should show errors and the root of the problem otherwise, so have plenty of healthchecks to ensure everything is going correctly.  \n",
        "\n",
        "├── docker-compose.yml (for docker composition)\n",
        "├── pyproject.toml ( for local development)\n",
        "└── .devcontainer/\n",
        "    ├── Dockerfile (for docker build)\n",
        "    ├── devcontainer.json\n",
        "    ├── .env.template\n",
        "    ├── .dockerignore\n",
        "    ├── validate_gpu.py\n",
        "    └── tests/\n",
        "        ├── test_summary.py\n",
        "        ├── test_pytorch.py\n",
        "        ├── test_pytorch_gpu.py\n",
        "        └── test_uv.py\n",
        "\n",
        "\n",
        "## Table of Contents\n",
        "1. [Environment Files](#environment-files)\n",
        "2. [DevContainer Configuration](#devcontainer-configuration)\n",
        "3. [Docker Setup](#docker-setup)\n",
        "4. [Testing & Diagnostics](#testing--diagnostics)\n",
        "5. [Verification](#verification)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Environment Files\n",
        "\n",
        "First, let's create the necessary environment configuration files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ../../.devcontainer/.env.template\n"
          ]
        }
      ],
      "source": [
        "%%writefile ../../.devcontainer/.env.template\n",
        "ENV_NAME=docker_dev_template\n",
        "\n",
        "# GPU Configuration for RTX 4090\n",
        "CUDA_TAG=12.4.0\n",
        "PYTHON_VER=3.10\n",
        "\n",
        "# Host Port Configuration\n",
        "HOST_JUPYTER_PORT=8891\n",
        "HOST_TENSORBOARD_PORT=6008\n",
        "HOST_EXPLAINER_PORT=8050\n",
        "HOST_STREAMLIT_PORT=8501\n",
        "HOST_MLFLOW_PORT=5000\n",
        "\n",
        "# JAX/GPU Configuration - CRITICAL: NO INLINE COMMENTS\n",
        "# These environment variables are parsed directly by JAX and must be clean\n",
        "\n",
        "# Memory fraction for GPU allocation (0.0 to 1.0)\n",
        "# For RTX 4090 24GB VRAM, 0.4 provides good balance\n",
        "XLA_PYTHON_CLIENT_MEM_FRACTION=0.4\n",
        "\n",
        "# Disable memory preallocation for better memory management\n",
        "XLA_PYTHON_CLIENT_PREALLOCATE=false\n",
        "\n",
        "# Use platform allocator for optimal GPU memory handling\n",
        "XLA_PYTHON_CLIENT_ALLOCATOR=platform\n",
        "\n",
        "# XLA compiler flags for CUDA\n",
        "XLA_FLAGS=--xla_gpu_cuda_data_dir=/usr/local/cuda\n",
        "\n",
        "# JAX memory preallocation limit in bytes\n",
        "# 16GB limit (17179869184 bytes) for RTX 4090\n",
        "JAX_PREALLOCATION_SIZE_LIMIT_BYTES=17179869184\n",
        "\n",
        "# JAX behavior configuration\n",
        "JAX_DISABLE_JIT=false\n",
        "JAX_ENABLE_X64=false\n",
        "\n",
        "# TensorFlow GPU configuration (if using TensorFlow)\n",
        "TF_FORCE_GPU_ALLOW_GROWTH=true\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ../../.devcontainer/.dockerignore\n"
          ]
        }
      ],
      "source": [
        "%%writefile ../../.devcontainer/.dockerignore\n",
        "# Reduce Docker build context\n",
        ".git\n",
        ".gitignore\n",
        ".gitattributes\n",
        ".gitmodules\n",
        ".vscode\n",
        ".idea\n",
        "*.swp\n",
        "*.swo\n",
        "*~\n",
        ".DS_Store\n",
        "Thumbs.db\n",
        "__pycache__\n",
        "*.pyc\n",
        "*.pyo\n",
        "*.pyd\n",
        ".Python\n",
        "*.so\n",
        ".coverage*\n",
        ".cache\n",
        ".pytest_cache\n",
        ".mypy_cache\n",
        ".tox\n",
        "pip-log.txt\n",
        "pip-delete-this-directory.txt\n",
        "env\n",
        "venv\n",
        "ENV\n",
        "env.bak\n",
        "venv.bak\n",
        ".ipynb_checkpoints\n",
        "# Large data (adjust as needed)\n",
        "data/raw\n",
        "data/external\n",
        "*.csv\n",
        "*.parquet\n",
        "*.h5\n",
        "*.hdf5\n",
        "# Models\n",
        "*.pt\n",
        "*.pth\n",
        "*.pkl\n",
        "*.joblib\n",
        "models/\n",
        "# Logs and temps\n",
        "*.log\n",
        "logs/\n",
        "*.tmp\n",
        "*.temp\n",
        ".tmp\n",
        "temp/\n",
        "# Build artifacts\n",
        "build/\n",
        "dist/\n",
        "*.egg-info/\n",
        ".eggs/\n",
        "# Node\n",
        "node_modules\n",
        "npm-debug.log*\n",
        "yarn-*.log*\n",
        ".npm\n",
        ".eslintcache\n",
        ".node_repl_history\n",
        "*.tgz\n",
        "*.tar.gz\n",
        "# Archives\n",
        "*.zip\n",
        "*.tar\n",
        "*.tar.bz2\n",
        "*.rar\n",
        "*.7z\n",
        "# Docs (opt‑in if needed)\n",
        "docs/\n",
        "*.md\n",
        "README*\n",
        "LICENSE*\n",
        "CHANGELOG*\n",
        "# Tests (opt‑in if needed)\n",
        "tests/\n",
        "test_*\n",
        "*_test.py\n",
        "# CI\n",
        ".github/\n",
        ".gitlab-ci.yml\n",
        ".travis.yml\n",
        ".circleci/\n",
        "azure-pipelines.yml\n",
        "# Env\n",
        ".env\n",
        ".env.local\n",
        ".env.*.local\n",
        ".editorconfig\n",
        ".prettierrc*\n",
        ".eslintrc*\n",
        "# Universal junk (de‑duped)\n",
        "*.py[cod]"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## DevContainer Configuration\n",
        "\n",
        "Setting up the VS Code devcontainer configuration files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ../../.devcontainer/devcontainer.json\n"
          ]
        }
      ],
      "source": [
        "%%writefile ../../.devcontainer/devcontainer.json\n",
        "{\n",
        "  \"name\": \"docker_dev_template_rtx4090\",\n",
        "  \"dockerComposeFile\": \"../docker-compose.yml\",\n",
        "  \"service\": \"datascience\",\n",
        "  \"workspaceFolder\": \"/workspace\",\n",
        "  \"shutdownAction\": \"stopCompose\",\n",
        "\n",
        "  \"overrideCommand\": false,\n",
        "  \"containerEnv\": {\n",
        "    \"CONTAINER_WORKSPACE_FOLDER\": \"/workspace\",\n",
        "    \"UV_PROJECT_ENVIRONMENT\": \"/app/.venv\",\n",
        "    \"VIRTUAL_ENV\": \"/app/.venv\",\n",
        "    \"PYTHONPATH\": \"/workspace\",\n",
        "    \"TERM\": \"xterm-256color\"\n",
        "  },\n",
        "\n",
        "  \"runArgs\": [\n",
        "    \"--gpus\", \"all\",\n",
        "    \"--name\", \"${localEnv:ENV_NAME:docker_dev_template}_datascience\"\n",
        "  ],\n",
        "\n",
        "  \"customizations\": {\n",
        "    \"vscode\": {\n",
        "      \"settings\": {\n",
        "        \"python.defaultInterpreterPath\": \"/app/.venv/bin/python\",\n",
        "        \"python.pythonPath\": \"/app/.venv/bin/python\",\n",
        "        \"python.terminal.activateEnvironment\": true,\n",
        "        \"python.terminal.activateEnvInCurrentTerminal\": true,\n",
        "        \"terminal.integrated.defaultProfile.linux\": \"bash\",\n",
        "        \"terminal.integrated.profiles.linux\": {\n",
        "          \"bash\": {\n",
        "            \"path\": \"/bin/bash\",\n",
        "            \"args\": [\"-l\"],\n",
        "            \"env\": {\n",
        "              \"VIRTUAL_ENV\": \"/app/.venv\",\n",
        "              \"PATH\": \"/app/.venv/bin:${env:PATH}\",\n",
        "              \"UV_PROJECT_ENVIRONMENT\": \"/app/.venv\",\n",
        "              \"PYTHONPATH\": \"/workspace\"\n",
        "            }\n",
        "          }\n",
        "        },\n",
        "        \"jupyter.notebookFileRoot\": \"/workspace\",\n",
        "        \"jupyter.kernels.filter\": [\n",
        "          {\n",
        "            \"path\": \"/app/.venv/bin/python\",\n",
        "            \"type\": \"pythonEnvironment\"\n",
        "          }\n",
        "        ]\n",
        "      },\n",
        "      \"extensions\": [\n",
        "        \"ms-python.python\",\n",
        "        \"ms-toolsai.jupyter\",\n",
        "        \"ms-azuretools.vscode-docker\",\n",
        "        \"ms-python.flake8\",\n",
        "        \"ms-python.black-formatter\"\n",
        "      ]\n",
        "    }\n",
        "  },\n",
        "\n",
        "  \"onCreateCommand\": [\n",
        "    \"bash\", \"-lc\",\n",
        "    \"echo 'onCreate: validating environment'; ls -la /app/.venv/bin/; which python || echo 'python not found in PATH'\"\n",
        "  ],\n",
        "\n",
        "  \"postCreateCommand\": [\n",
        "    \"bash\", \"-lc\",\n",
        "    \"set -e; source /app/.venv/bin/activate; python -c 'import sys; print(f\\\"python: {sys.executable}\\\")'; uv pip install -U ipykernel jupyter-client -q; python -m ipykernel install --user --name='uv_docker_dev_template' --display-name='Python (UV Environment)'; jupyter kernelspec list; python /app/tests/test_summary.py\"\n",
        "  ],\n",
        "\n",
        "  \"postStartCommand\": [\n",
        "    \"bash\", \"-lc\",\n",
        "    \"source /app/.venv/bin/activate; python --version; python -c 'import torch; print(f\\\"pytorch cuda: {torch.cuda.is_available()}\\\")' || echo 'pytorch test failed'; python /app/validate_gpu.py --quick || echo 'gpu validation completed with warnings'\"\n",
        "  ],\n",
        "\n",
        "  \"features\": {},\n",
        "  \"forwardPorts\": [8888, 6008, 8050, 8501, 5000],\n",
        "  \"portsAttributes\": {\n",
        "    \"8888\": { \"label\": \"Jupyter Lab\", \"onAutoForward\": \"notify\" },\n",
        "    \"6008\": { \"label\": \"TensorBoard\", \"onAutoForward\": \"silent\" },\n",
        "    \"8050\": { \"label\": \"Explainer Dashboard\", \"onAutoForward\": \"silent\" },\n",
        "    \"8501\": { \"label\": \"Streamlit\", \"onAutoForward\": \"silent\" },\n",
        "    \"5000\": { \"label\": \"MLflow\", \"onAutoForward\": \"silent\" }\n",
        "  },\n",
        "\n",
        "  \"mounts\": [\n",
        "    \"source=docker_dev_template_uv_cache,target=/root/.cache/uv,type=volume\"\n",
        "  ]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ../../.devcontainer/Dockerfile\n"
          ]
        }
      ],
      "source": [
        "%%writefile ../../.devcontainer/Dockerfile\n",
        "# Dockerfile: RTX 4090 devcontainer with UV, JAX, and PyTorch (CUDA 12.x)\n",
        "\n",
        "ARG CUDA_TAG=12.4.0\n",
        "FROM nvidia/cuda:${CUDA_TAG}-devel-ubuntu22.04\n",
        "\n",
        "ARG PYTHON_VER=3.10\n",
        "ARG ENV_NAME=docker_dev_template\n",
        "ENV DEBIAN_FRONTEND=noninteractive\n",
        "\n",
        "# System dependencies\n",
        "RUN --mount=type=cache,id=apt-cache,target=/var/cache/apt,sharing=locked \\\n",
        "    --mount=type=cache,id=apt-lists,target=/var/lib/apt/lists,sharing=locked \\\n",
        "    apt-get update && apt-get install -y --no-install-recommends \\\n",
        "        bash curl ca-certificates git procps htop \\\n",
        "        python3 python3-venv python3-pip python3-dev \\\n",
        "        build-essential cmake pkg-config \\\n",
        "        libjemalloc2 libjemalloc-dev \\\n",
        "        iproute2 net-tools lsof wget \\\n",
        "    && apt-get clean && rm -rf /var/lib/apt/lists/*\n",
        "\n",
        "# UV package manager\n",
        "COPY --from=ghcr.io/astral-sh/uv:0.7.12 /uv /uvx /bin/\n",
        "\n",
        "WORKDIR /app\n",
        "\n",
        "# Create venv managed by UV\n",
        "RUN uv venv .venv --python \"${PYTHON_VER}\" --prompt \"${ENV_NAME}\"\n",
        "\n",
        "ENV VIRTUAL_ENV=/app/.venv \\\n",
        "    PATH=\"/app/.venv/bin:${PATH}\" \\\n",
        "    UV_PROJECT_ENVIRONMENT=/app/.venv \\\n",
        "    PYTHONPATH=\"/workspace\"\n",
        "\n",
        "# Memory and allocator settings\n",
        "ENV LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libjemalloc.so.2 \\\n",
        "    MALLOC_ARENA_MAX=2 \\\n",
        "    MALLOC_TCACHE_MAX=0 \\\n",
        "    PYTORCH_NO_CUDA_MEMORY_CACHING=1\n",
        "\n",
        "# GPU‑relevant environment\n",
        "ENV XLA_PYTHON_CLIENT_PREALLOCATE=false \\\n",
        "    XLA_PYTHON_CLIENT_MEM_FRACTION=0.4 \\\n",
        "    XLA_PYTHON_CLIENT_ALLOCATOR=platform \\\n",
        "    PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:1024,expandable_segments:True \\\n",
        "    JAX_PREALLOCATION_SIZE_LIMIT_BYTES=17179869184\n",
        "\n",
        "# Bring in project descriptors and tests\n",
        "COPY pyproject.toml /workspace/\n",
        "COPY uv.lock* /workspace/\n",
        "COPY .devcontainer/validate_gpu.py /app/validate_gpu.py\n",
        "COPY .devcontainer/tests/ /app/tests/\n",
        "\n",
        "# Resolve project dependencies with UV\n",
        "RUN --mount=type=cache,target=/root/.cache/uv,sharing=locked \\\n",
        "    cd /workspace && (uv sync --frozen --no-dev || (uv sync --no-dev && uv lock))\n",
        "\n",
        "# CRITICAL FIX 2: Install PyTorch first to establish CUDA environment\n",
        "RUN --mount=type=cache,target=/root/.cache/uv,sharing=locked \\\n",
        "    echo \"Installing PyTorch with CUDA 12.4...\" && \\\n",
        "    uv pip install --no-cache-dir torch torchvision torchaudio \\\n",
        "        --index-url https://download.pytorch.org/whl/cu124\n",
        "\n",
        "# CRITICAL FIX 3: Install compatible CuDNN 9.8.0 to satisfy JAX requirements\n",
        "RUN --mount=type=cache,target=/root/.cache/uv,sharing=locked \\\n",
        "    echo \"Upgrading CuDNN to 9.8.0 for JAX compatibility...\" && \\\n",
        "    uv pip install --no-cache-dir --upgrade nvidia-cudnn-cu12==9.8.0.69 || \\\n",
        "    uv pip install --no-cache-dir --upgrade nvidia-cudnn-cu12>=9.8.0\n",
        "\n",
        "# CRITICAL FIX 4: Install JAX after CuDNN upgrade with proper dependency resolution\n",
        "RUN --mount=type=cache,target=/root/.cache/uv,sharing=locked \\\n",
        "    echo \"Removing any existing JAX installations...\" && \\\n",
        "    (uv pip uninstall jax jaxlib jax-cuda12-plugin jax-cuda12-pjrt || true) && \\\n",
        "    echo \"Installing JAX with CUDA 12 support...\" && \\\n",
        "    (uv pip install --no-cache-dir \"jax[cuda12-local]>=0.4.26\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html \\\n",
        "     || uv pip install --no-cache-dir \"jax[cpu]>=0.4.26\")\n",
        "\n",
        "# Jupyter kernel support\n",
        "RUN --mount=type=cache,target=/root/.cache/uv,sharing=locked \\\n",
        "    uv pip install ipykernel jupyter-client jupyterlab\n",
        "\n",
        "# CUDA libs in path - include both system and package CUDA libraries\n",
        "ENV LD_LIBRARY_PATH=\"/app/.venv/lib:/app/.venv/lib/python3.10/site-packages/nvidia/cudnn/lib:/usr/local/cuda/lib64:${LD_LIBRARY_PATH}\"\n",
        "\n",
        "# Shell activation helper with updated library paths\n",
        "RUN echo '#!/bin/bash' > /app/activate_uv.sh && \\\n",
        "    echo 'export VIRTUAL_ENV=\"/app/.venv\"' >> /app/activate_uv.sh && \\\n",
        "    echo 'export PATH=\"/app/.venv/bin:$PATH\"' >> /app/activate_uv.sh && \\\n",
        "    echo 'export UV_PROJECT_ENVIRONMENT=\"/app/.venv\"' >> /app/activate_uv.sh && \\\n",
        "    echo 'export PYTHONPATH=\"/workspace:$PYTHONPATH\"' >> /app/activate_uv.sh && \\\n",
        "    echo 'export LD_LIBRARY_PATH=\"/app/.venv/lib:/app/.venv/lib/python3.10/site-packages/nvidia/cudnn/lib:/usr/local/cuda/lib64:${LD_LIBRARY_PATH}\"' >> /app/activate_uv.sh && \\\n",
        "    echo 'cd /workspace' >> /app/activate_uv.sh && \\\n",
        "    chmod +x /app/activate_uv.sh && \\\n",
        "    echo 'source /app/activate_uv.sh' > /etc/profile.d/10-uv-activate.sh && \\\n",
        "    echo 'source /app/activate_uv.sh' >> /root/.bashrc && \\\n",
        "    chmod +x /etc/profile.d/10-uv-activate.sh\n",
        "\n",
        "# Enhanced healthcheck script with CuDNN diagnostics\n",
        "RUN echo '#!/bin/bash' > /app/healthcheck.sh && \\\n",
        "    echo 'source /app/.venv/bin/activate' >> /app/healthcheck.sh && \\\n",
        "    echo 'echo \"=== CuDNN Version Check ===\"' >> /app/healthcheck.sh && \\\n",
        "    echo 'python -c \"import torch; print(f\\\"PyTorch CuDNN: {torch.backends.cudnn.version()}\\\")\" || echo \"PyTorch CuDNN check failed\"' >> /app/healthcheck.sh && \\\n",
        "    echo 'echo \"=== JAX Device Check ===\"' >> /app/healthcheck.sh && \\\n",
        "    echo 'python -c \"import jax; print(f\\\"JAX devices: {jax.devices()}\\\")\" || echo \"JAX device check failed\"' >> /app/healthcheck.sh && \\\n",
        "    echo 'echo \"=== GPU Validation ===\"' >> /app/healthcheck.sh && \\\n",
        "    echo 'python /app/validate_gpu.py --quick' >> /app/healthcheck.sh && \\\n",
        "    chmod +x /app/healthcheck.sh\n",
        "\n",
        "WORKDIR /workspace\n",
        "CMD [\"bash\", \"-l\"]\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Docker Setup\n",
        "\n",
        "Creating the Docker Compose configuration and project files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ../../docker-compose.yml\n"
          ]
        }
      ],
      "source": [
        "%%writefile ../../docker-compose.yml\n",
        "name: ${ENV_NAME:-docker_dev_template}\n",
        "\n",
        "services:\n",
        "  datascience:\n",
        "    build:\n",
        "      context: .\n",
        "      dockerfile: .devcontainer/Dockerfile\n",
        "      args:\n",
        "        CUDA_TAG: ${CUDA_TAG:-12.4.0}\n",
        "        PYTHON_VER: ${PYTHON_VER:-3.10}\n",
        "        ENV_NAME: ${ENV_NAME:-docker_dev_template}\n",
        "      cache_from:\n",
        "        - nvidia/cuda:${CUDA_TAG:-12.4.0}-devel-ubuntu22.04\n",
        "\n",
        "    container_name: ${ENV_NAME:-docker_dev_template}_datascience\n",
        "\n",
        "    # UPDATED: Reference environment template from .devcontainer folder\n",
        "    env_file:\n",
        "      - .devcontainer/.env.template\n",
        "\n",
        "    restart: unless-stopped\n",
        "    depends_on:\n",
        "      mlflow:\n",
        "        condition: service_healthy\n",
        "\n",
        "    deploy:\n",
        "      resources:\n",
        "        reservations:\n",
        "          devices:\n",
        "            - driver: nvidia\n",
        "              count: all\n",
        "              capabilities: [gpu]\n",
        "\n",
        "    init: true\n",
        "    gpus: all\n",
        "    shm_size: 8g\n",
        "    ulimits:\n",
        "      memlock: -1\n",
        "      stack: 67108864\n",
        "\n",
        "    environment:\n",
        "      - PYTHON_VER=${PYTHON_VER:-3.10}\n",
        "      - UV_PROJECT_ENVIRONMENT=/app/.venv\n",
        "      - VIRTUAL_ENV=/app/.venv\n",
        "      - PYTHONPATH=/workspace\n",
        "      - NVIDIA_VISIBLE_DEVICES=all\n",
        "      - NVIDIA_DRIVER_CAPABILITIES=compute,utility\n",
        "      - CUDA_VISIBLE_DEVICES=0\n",
        "      - LD_LIBRARY_PATH=/app/.venv/lib:/usr/local/cuda/lib64\n",
        "      - LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libjemalloc.so.2\n",
        "      - MALLOC_ARENA_MAX=2\n",
        "      - MALLOC_TCACHE_MAX=0\n",
        "      - PYTORCH_NO_CUDA_MEMORY_CACHING=1\n",
        "      \n",
        "      # CRITICAL FIX: Removed inline comments from JAX environment variables\n",
        "      # These were causing \"could not convert string to float\" errors\n",
        "      - XLA_PYTHON_CLIENT_PREALLOCATE=false\n",
        "      - XLA_PYTHON_CLIENT_ALLOCATOR=platform\n",
        "      - XLA_PYTHON_CLIENT_MEM_FRACTION=0.4\n",
        "      - XLA_FLAGS=--xla_gpu_cuda_data_dir=/usr/local/cuda\n",
        "      - JAX_PREALLOCATION_SIZE_LIMIT_BYTES=17179869184\n",
        "      - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:1024,expandable_segments:True\n",
        "      - JUPYTER_TOKEN=${JUPYTER_TOKEN:-jupyter}\n",
        "\n",
        "    volumes:\n",
        "      - .:/workspace:delegated\n",
        "      - ./mlruns:/workspace/mlruns\n",
        "      - uv-cache:/root/.cache/uv\n",
        "\n",
        "    ports:\n",
        "      - \"${HOST_JUPYTER_PORT:-8891}:8888\"\n",
        "      - \"${HOST_TENSORBOARD_PORT:-6008}:6008\"\n",
        "      - \"${HOST_EXPLAINER_PORT:-8050}:8050\"\n",
        "      - \"${HOST_STREAMLIT_PORT:-8501}:8501\"\n",
        "\n",
        "    command: >\n",
        "      bash -lc '\n",
        "        echo \"[boot] Starting container: ${ENV_NAME:-docker_dev_template}\";\n",
        "        echo \"[boot] Activating uv environment...\";\n",
        "        source /app/.venv/bin/activate;\n",
        "        echo \"[boot] Environment activated - Python: $(which python)\";\n",
        "        echo \"[boot] UV available: $(uv --version)\";\n",
        "        echo \"[boot] Running GPU validation...\";\n",
        "        python /app/validate_gpu.py || echo \"GPU validation warning - check logs\";\n",
        "        echo \"[boot] Starting Jupyter Lab on port 8888...\";\n",
        "        jupyter lab --ip=0.0.0.0 --port=8888 --allow-root \n",
        "        --NotebookApp.token=\"${JUPYTER_TOKEN}\" \n",
        "        --NotebookApp.allow_origin=\"*\" \n",
        "        --NotebookApp.open_browser=false\n",
        "      '\n",
        "\n",
        "    healthcheck:\n",
        "      test: [\"CMD-SHELL\", \"python -c 'import torch, jax; assert torch.cuda.is_available(); assert len([d for d in jax.devices() if \\\"gpu\\\" in str(d).lower()]) > 0' 2>/dev/null || exit 1\"]\n",
        "      interval: 60s\n",
        "      timeout: 30s\n",
        "      retries: 3\n",
        "      start_period: 120s\n",
        "\n",
        "    labels:\n",
        "      - \"com.docker.compose.project=${ENV_NAME:-docker_dev_template}\"\n",
        "      - \"com.docker.compose.service=datascience\"\n",
        "      - \"description=RTX 4090 GPU Dev Environment (PyTorch+JAX) - CUDA 12.4\"\n",
        "\n",
        "  mlflow:\n",
        "    container_name: ${ENV_NAME:-docker_dev_template}_mlflow\n",
        "    image: ghcr.io/mlflow/mlflow:latest\n",
        "    command: >\n",
        "      mlflow server\n",
        "      --host 0.0.0.0\n",
        "      --port 5000\n",
        "      --backend-store-uri sqlite:///mlflow.db\n",
        "      --default-artifact-root /mlflow_artifacts\n",
        "    environment:\n",
        "      MLFLOW_EXPERIMENTS_DEFAULT_ARTIFACT_LOCATION: /mlflow_artifacts\n",
        "    volumes:\n",
        "      - ./mlruns:/mlflow_artifacts\n",
        "      - ./mlflow_db:/mlflow_db\n",
        "    ports:\n",
        "      - \"${HOST_MLFLOW_PORT:-5000}:5000\"\n",
        "    restart: unless-stopped\n",
        "    healthcheck:\n",
        "      test: [\"CMD\", \"python\", \"-c\", \"import requests; requests.get('http://localhost:5000/health').raise_for_status()\"]\n",
        "      interval: 30s\n",
        "      timeout: 10s\n",
        "      retries: 3\n",
        "      start_period: 30s\n",
        "\n",
        "volumes:\n",
        "  uv-cache:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ../../pyproject.toml\n"
          ]
        }
      ],
      "source": [
        "%%writefile ../../pyproject.toml\n",
        "[project]\n",
        "name = \"docker_dev_template\"\n",
        "version = \"0.1.0\"\n",
        "description = \"Hierarchical Bayesian modeling for baseball exit velocity data\"\n",
        "authors = [\n",
        "  { name = \"Marlins Data Science Team\" },\n",
        "]\n",
        "license = \"MIT\"\n",
        "readme = \"README.md\"\n",
        "\n",
        "# ─── Restrict to Python 3.10–3.12 ──────────────────────────────\n",
        "requires-python = \">=3.10,<3.13\"\n",
        "\n",
        "dependencies = [\n",
        "  \"pandas>=2.0\",\n",
        "  \"numpy>=1.20,<2\",\n",
        "  \"matplotlib>=3.4.0\",\n",
        "  \"scikit-learn>=1.4.2\",\n",
        "  \"pymc>=5.0.0\",\n",
        "  \"arviz>=0.14.0\",\n",
        "  \"statsmodels>=0.13.0\",\n",
        "  \"jupyterlab>=3.0.0\",\n",
        "  \"seaborn>=0.11.0\",\n",
        "  \"tabulate>=0.9.0\",\n",
        "  \"shap>=0.40.0\",\n",
        "  \"xgboost>=1.5.0\",\n",
        "  \"lightgbm>=3.3.0\",\n",
        "  \"catboost>=1.0.0\",\n",
        "  \"scipy>=1.7.0\",\n",
        "  \"shapash[report]>=2.3.0\",\n",
        "  \"shapiq>=1.3.0\",\n",
        "  \"explainerdashboard>=0.3.0\",\n",
        "  \"ipywidgets>=8.0.0\",\n",
        "  \"nutpie>=0.7.1\",\n",
        "  \"numpyro>=0.18.0,<1.0.0\",\n",
        "  \"jax>=0.4.23\",\n",
        "  \"jaxlib>=0.4.23\",\n",
        "  \"pytensor>=2.18.3\",\n",
        "  \"aesara>=2.9.4\",\n",
        "  \"tqdm>=4.67.0\",\n",
        "  \"pyarrow>=12.0.0\",\n",
        "  \"streamlit>=1.20.0\",\n",
        "  \"sqlalchemy>=1.4\",\n",
        "  \"mysql-connector-python>=8.0\",\n",
        "  \"optuna>=4.3.0\",\n",
        "  \"bayesian-optimization>=1.2.0\",\n",
        "  \"pretty_errors>=1.2.0\",\n",
        "  \"gdown>=4.0.0\",\n",
        "  \"invoke>=2.2\",\n",
        "  # ▶ Video download stack\n",
        "  #   - pytube main-branch until next PyPI release (optional fallback)\n",
        "  \"pytube @ git+https://github.com/pytube/pytube\",\n",
        "  \"yt-dlp>=2024.12.0\",\n",
        "  #   - optional convenience wrapper (does NOT install ffmpeg binary!)\n",
        "  \"ffmpeg-python >= 0.2.0\",\n",
        "\n",
        "  # Ultralytics YOLO (SOTA object detection, segmentation, etc.)\n",
        "  # ▶ Computer vision\n",
        "  \"ultralytics==8.3.158\",\n",
        "  \"opencv-python-headless>=4.10.0\",\n",
        "  \"roboflow>=1.0.0\",\n",
        "  \"mlflow>=3.1.1,<4.0.0\",\n",
        "  \"optuna-integration[mlflow]>=4.4.0,<5.0.0\",\n",
        "  \n",
        "  # PyTorch core libraries - platform specific with PEP-508 compliant syntax\n",
        "  # CUDA wheels for Windows/Linux, CPU for macOS\n",
        "  \"torch>=2.0.0\",\n",
        "  \"torchvision>=0.15.0\",\n",
        "  \"torchaudio>=2.0.0\",\n",
        "  \n",
        "  # new for basemodels\n",
        "  \"pydantic>=2.0.0\",\n",
        "  \"pydantic-settings>=2.0.0\",\n",
        "]\n",
        "\n",
        "[project.optional-dependencies]\n",
        "dev = [\n",
        "  \"pytest>=7.0.0\",\n",
        "  \"black>=23.0.0\",\n",
        "  \"isort>=5.0.0\",\n",
        "  \"flake8>=5.0.0\",\n",
        "  \"mypy>=1.0.0\",\n",
        "  \"pre-commit>=3.0.0\",\n",
        "]\n",
        "\n",
        "cuda = [\n",
        "  \"cupy-cuda12x>=12.0.0\",  # For CUDA 12.x\n",
        "]\n",
        "\n",
        "# ─── uv configuration ──────────────────────────────────────────\n",
        "[tool.uv]                   # uv reads this block\n",
        "index-strategy = \"unsafe-best-match\"\n",
        "\n",
        "# Define named indexes for PyTorch CUDA variants\n",
        "[[tool.uv.index]]\n",
        "name = \"pytorch-cu121\"\n",
        "url = \"https://download.pytorch.org/whl/cu121\"\n",
        "explicit = true\n",
        "\n",
        "[[tool.uv.index]]\n",
        "name = \"pytorch-cu118\"\n",
        "url = \"https://download.pytorch.org/whl/cu118\"\n",
        "explicit = true\n",
        "\n",
        "[[tool.uv.index]]\n",
        "name = \"pytorch-cu124\"\n",
        "url = \"https://download.pytorch.org/whl/cu124\"\n",
        "explicit = true\n",
        "\n",
        "[[tool.uv.index]]\n",
        "name = \"pytorch-cu128\"\n",
        "url = \"https://download.pytorch.org/whl/cu128\"\n",
        "explicit = true\n",
        "\n",
        "# Removed unsupported option: torch-backend requires uv ≥0.5.3\n",
        "# To re-enable, first run: pip install -U uv>=0.5.3\n",
        "[tool.uv.pip]\n",
        "# (No unsupported keys here; configure only valid pip options.)\n",
        "\n",
        "# Map PyTorch dependencies to CUDA indexes for non-macOS platforms\n",
        "# Testing with CUDA 12.8\n",
        "[tool.uv.sources]\n",
        "torch = [\n",
        "  { index = \"pytorch-cu128\", marker = \"sys_platform == 'linux' or sys_platform == 'win32'\" },\n",
        "]\n",
        "torchvision = [\n",
        "  { index = \"pytorch-cu128\", marker = \"sys_platform == 'linux' or sys_platform == 'win32'\" },\n",
        "]\n",
        "torchaudio = [\n",
        "  { index = \"pytorch-cu128\", marker = \"sys_platform == 'linux' or sys_platform == 'win32'\" },\n",
        "]\n",
        "\n",
        "[tool.pytensor]\n",
        "device    = \"cuda\"\n",
        "floatX    = \"float32\"\n",
        "allow_gc  = true\n",
        "optimizer = \"fast_run\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ../../.devcontainer/validate_gpu.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile ../../.devcontainer/validate_gpu.py\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "GPU validation and environment diagnostics for RTX 4090 devcontainer.\n",
        "Focus: verify JAX and PyTorch access to CUDA, report common misconfigurations.\n",
        "\"\"\"\n",
        "import sys\n",
        "import os\n",
        "import subprocess\n",
        "import warnings\n",
        "import textwrap\n",
        "import re\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "def print_section(title: str) -> None:\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(f\"  {title}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "\n",
        "def validate_environment_variables() -> bool:\n",
        "    \"\"\"Validate JAX‑related environment variables (no inline comments, valid types).\"\"\"\n",
        "    print_section(\"JAX ENVIRONMENT VARIABLE VALIDATION\")\n",
        "\n",
        "    jax_numeric_vars = {\n",
        "        'XLA_PYTHON_CLIENT_MEM_FRACTION': {'type': 'float', 'range': (0.0, 1.0)},\n",
        "        'JAX_PREALLOCATION_SIZE_LIMIT_BYTES': {'type': 'int', 'range': (0, None)},\n",
        "    }\n",
        "    jax_string_vars = {\n",
        "        'XLA_FLAGS', 'JAX_PLATFORM_NAME', 'XLA_PYTHON_CLIENT_ALLOCATOR', 'XLA_PYTHON_CLIENT_PREALLOCATE'\n",
        "    }\n",
        "\n",
        "    ok = True\n",
        "    problems = []\n",
        "\n",
        "    for var, cfg in jax_numeric_vars.items():\n",
        "        value = os.environ.get(var)\n",
        "        print(f\"\\nCheck {var} -> {value}\")\n",
        "        if value is None:\n",
        "            print(\"  not set; defaults apply\")\n",
        "            continue\n",
        "        if '#' in value:\n",
        "            clean = value.split('#')[0].strip()\n",
        "            print(\"  contains inline comment; use:\", clean)\n",
        "            problems.append((var, value, clean))\n",
        "            ok = False\n",
        "            continue\n",
        "        try:\n",
        "            if cfg['type'] == 'float':\n",
        "                v = float(value)\n",
        "                low, high = cfg['range']\n",
        "                if (low is not None and v < low) or (high is not None and v > high):\n",
        "                    print(\"  out of recommended range\")\n",
        "                else:\n",
        "                    print(\"  ok\")\n",
        "            else:\n",
        "                v = int(value)\n",
        "                print(\"  ok\")\n",
        "        except ValueError as e:\n",
        "            print(\"  invalid numeric value:\", e)\n",
        "            ok = False\n",
        "\n",
        "    for var in jax_string_vars:\n",
        "        value = os.environ.get(var)\n",
        "        if value and '#' in value:\n",
        "            print(f\"warn: {var} contains '#', which can break parsing\")\n",
        "\n",
        "    if problems:\n",
        "        print(\"\\nFix suggestions:\")\n",
        "        for var, bad, clean in problems:\n",
        "            print(f\"export {var}={clean}\")\n",
        "    return ok\n",
        "\n",
        "\n",
        "def check_environment() -> None:\n",
        "    print_section(\"ENVIRONMENT CHECK\")\n",
        "    print(\"python:\", sys.executable)\n",
        "    print(\"version:\", sys.version)\n",
        "    print(\"VIRTUAL_ENV:\", os.environ.get('VIRTUAL_ENV'))\n",
        "    print(\"PATH contains .venv:\", '.venv/bin' in os.environ.get('PATH', ''))\n",
        "\n",
        "    cuda_vars = ['CUDA_HOME', 'CUDA_PATH', 'CUDA_VISIBLE_DEVICES', 'LD_LIBRARY_PATH', 'NVIDIA_VISIBLE_DEVICES']\n",
        "    print(\"\\nCUDA variables:\")\n",
        "    for var in cuda_vars:\n",
        "        print(f\"  {var}:\", os.environ.get(var, 'not set'))\n",
        "\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            ['nvidia-smi', '--query-gpu=name,driver_version,memory.total', '--format=csv,noheader'],\n",
        "            capture_output=True, text=True\n",
        "        )\n",
        "        if result.returncode == 0:\n",
        "            print(\"\\nGPU:\", result.stdout.strip())\n",
        "        else:\n",
        "            print(\"\\nwarn: nvidia-smi returned non‑zero\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"\\nwarn: nvidia-smi not found in path\")\n",
        "\n",
        "\n",
        "def test_pytorch() -> bool:\n",
        "    print_section(\"PYTORCH GPU TEST\")\n",
        "    try:\n",
        "        import torch\n",
        "        print(\"version:\", torch.__version__)\n",
        "        print(\"cuda available:\", torch.cuda.is_available())\n",
        "        if torch.cuda.is_available():\n",
        "            print(\"device count:\", torch.cuda.device_count())\n",
        "            print(\"device 0:\", torch.cuda.get_device_name(0))\n",
        "            # quick matmul\n",
        "            import time\n",
        "            dev = torch.device('cuda')\n",
        "            x = torch.randn(2000, 2000, device=dev)\n",
        "            y = torch.randn(2000, 2000, device=dev)\n",
        "            _ = x @ y\n",
        "            torch.cuda.synchronize()\n",
        "            t0 = time.time()\n",
        "            r = x @ y\n",
        "            torch.cuda.synchronize()\n",
        "            print(\"matmul elapsed s:\", round(time.time() - t0, 3))\n",
        "            _ = r.sum().item()\n",
        "            return True\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(\"pytorch test error:\", e)\n",
        "        return False\n",
        "\n",
        "\n",
        "def check_cudnn_compatibility() -> bool:\n",
        "    \"\"\"Check CuDNN version compatibility between PyTorch and JAX.\"\"\"\n",
        "    print_section(\"CUDNN COMPATIBILITY CHECK\")\n",
        "    try:\n",
        "        import torch\n",
        "        import subprocess\n",
        "        import glob\n",
        "        \n",
        "        # Check PyTorch CuDNN version\n",
        "        pytorch_cudnn = torch.backends.cudnn.version()\n",
        "        print(f\"PyTorch CuDNN version: {pytorch_cudnn}\")\n",
        "        \n",
        "        # Check installed nvidia-cudnn-cu12 package version\n",
        "        try:\n",
        "            result = subprocess.run(['uv', 'pip', 'list'], capture_output=True, text=True)\n",
        "            if result.returncode == 0:\n",
        "                lines = result.stdout.split('\\n')\n",
        "                for line in lines:\n",
        "                    if 'nvidia-cudnn-cu12' in line:\n",
        "                        print(f\"Installed CuDNN package: {line.strip()}\")\n",
        "                        break\n",
        "        except Exception as e:\n",
        "            print(f\"Could not check CuDNN package version: {e}\")\n",
        "        \n",
        "        # Check CuDNN library files\n",
        "        cudnn_paths = [\n",
        "            \"/app/.venv/lib/python3.10/site-packages/nvidia/cudnn/lib\",\n",
        "            \"/usr/local/cuda/lib64\",\n",
        "            \"/usr/lib/x86_64-linux-gnu\"\n",
        "        ]\n",
        "        \n",
        "        print(\"\\nCuDNN library search:\")\n",
        "        for path in cudnn_paths:\n",
        "            if os.path.exists(path):\n",
        "                cudnn_libs = glob.glob(f\"{path}/libcudnn*\")\n",
        "                if cudnn_libs:\n",
        "                    print(f\"  {path}: {len(cudnn_libs)} CuDNN libraries found\")\n",
        "                    for lib in cudnn_libs[:3]:  # Show first 3\n",
        "                        print(f\"    - {os.path.basename(lib)}\")\n",
        "                else:\n",
        "                    print(f\"  {path}: No CuDNN libraries found\")\n",
        "            else:\n",
        "                print(f\"  {path}: Path does not exist\")\n",
        "        \n",
        "        # Check LD_LIBRARY_PATH\n",
        "        ld_path = os.environ.get('LD_LIBRARY_PATH', '')\n",
        "        print(f\"\\nLD_LIBRARY_PATH: {ld_path}\")\n",
        "        \n",
        "        # Version compatibility check\n",
        "        if pytorch_cudnn < 9000:  # Assuming version format like 9100 for 9.1.0\n",
        "            print(\"WARNING: PyTorch CuDNN version may be too old for JAX\")\n",
        "            return False\n",
        "        \n",
        "        print(\"CuDNN compatibility check passed\")\n",
        "        return True\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"CuDNN compatibility check failed: {e}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "def test_jax_initialization() -> bool:\n",
        "    print_section(\"JAX INITIALIZATION TEST\")\n",
        "    try:\n",
        "        os.environ['JAX_TRACEBACK_FILTERING'] = 'off'\n",
        "        import jax\n",
        "        import jaxlib\n",
        "        from jaxlib import xla_client\n",
        "        print(\"jax:\", jax.__version__, \"jaxlib:\", jaxlib.__version__)\n",
        "        \n",
        "        # Check for CuDNN version mismatch errors\n",
        "        try:\n",
        "            opts = xla_client.generate_pjrt_gpu_plugin_options()\n",
        "            print(\"gpu plugin options ok; memory_fraction:\", opts.get('memory_fraction', 'not set'))\n",
        "        except Exception as e:\n",
        "            print(\"gpu plugin options error:\", e)\n",
        "            if \"could not convert string to float\" in str(e):\n",
        "                print(\"hint: check XLA_PYTHON_CLIENT_MEM_FRACTION for inline comments\")\n",
        "            elif \"CuDNN\" in str(e) and \"version\" in str(e):\n",
        "                print(\"hint: CuDNN version mismatch detected - check compatibility\")\n",
        "            return False\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(\"jax init error:\", e)\n",
        "        if \"CuDNN\" in str(e):\n",
        "            print(\"hint: CuDNN-related error detected\")\n",
        "        return False\n",
        "\n",
        "\n",
        "def test_jax() -> bool:\n",
        "    print_section(\"JAX GPU TEST\")\n",
        "    try:\n",
        "        os.environ['JAX_TRACEBACK_FILTERING'] = 'off'\n",
        "        import jax, jax.numpy as jnp\n",
        "        from jax.lib import xla_bridge\n",
        "        print(\"backend:\", xla_bridge.get_backend().platform)\n",
        "        devices = jax.devices()\n",
        "        print(\"devices:\", devices)\n",
        "        gpus = [d for d in devices if 'gpu' in str(d).lower() or getattr(d, 'platform', '') == 'gpu']\n",
        "        if not gpus:\n",
        "            print(\"no gpu devices detected by jax\")\n",
        "            return False\n",
        "        # quick compute\n",
        "        import time\n",
        "        key = jax.random.PRNGKey(0)\n",
        "        x = jax.random.normal(key, (2000, 2000))\n",
        "        x = jax.device_put(x, gpus[0])\n",
        "        t0 = time.time()\n",
        "        s = jnp.sum(x @ x).block_until_ready()\n",
        "        print(\"matmul elapsed s:\", round(time.time() - t0, 3), \"sum:\", float(s))\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(\"jax test error:\", e)\n",
        "        return False\n",
        "\n",
        "\n",
        "def main() -> int:\n",
        "    import argparse\n",
        "    p = argparse.ArgumentParser()\n",
        "    p.add_argument('--quick', action='store_true')\n",
        "    p.add_argument('--fix', action='store_true', help='Run with fix recommendations')\n",
        "    args = p.parse_args()\n",
        "\n",
        "    if args.quick:\n",
        "        env_ok = validate_environment_variables()\n",
        "        pt_ok = test_pytorch()\n",
        "        return 0 if (env_ok and pt_ok) else 1\n",
        "\n",
        "    env_ok = validate_environment_variables()\n",
        "    check_environment()\n",
        "    cudnn_ok = check_cudnn_compatibility()\n",
        "    jax_init_ok = test_jax_initialization()\n",
        "    jax_ok = test_jax()\n",
        "    pt_ok = test_pytorch()\n",
        "\n",
        "    print_section(\"SUMMARY\")\n",
        "    print(\"env vars:\", \"ok\" if env_ok else \"fail\")\n",
        "    print(\"cudnn compatibility:\", \"ok\" if cudnn_ok else \"fail\")\n",
        "    print(\"jax init:\", \"ok\" if jax_init_ok else \"fail\")\n",
        "    print(\"jax compute:\", \"ok\" if jax_ok else \"fail\")\n",
        "    print(\"pytorch:\", \"ok\" if pt_ok else \"fail\")\n",
        "\n",
        "    # Provide fix recommendations if requested\n",
        "    if args.fix and not (env_ok and cudnn_ok and jax_init_ok and jax_ok and pt_ok):\n",
        "        print_section(\"FIX RECOMMENDATIONS\")\n",
        "        if not cudnn_ok:\n",
        "            print(\"1. CuDNN version mismatch detected:\")\n",
        "            print(\"   - Upgrade nvidia-cudnn-cu12 to version >= 9.8.0\")\n",
        "            print(\"   - Ensure LD_LIBRARY_PATH includes CuDNN library paths\")\n",
        "        if not jax_init_ok:\n",
        "            print(\"2. JAX initialization failed:\")\n",
        "            print(\"   - Check CuDNN compatibility\")\n",
        "            print(\"   - Verify XLA environment variables (no inline comments)\")\n",
        "        if not jax_ok:\n",
        "            print(\"3. JAX GPU computation failed:\")\n",
        "            print(\"   - Verify GPU is accessible\")\n",
        "            print(\"   - Check CUDA driver compatibility\")\n",
        "\n",
        "    return 0 if (env_ok and cudnn_ok and jax_init_ok and jax_ok and pt_ok) else 1\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    sys.exit(main())\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## BuildKit Cache Fix & Testing\n",
        "\n",
        "The Dockerfile has been updated with isolated BuildKit cache mounts to prevent \n",
        "`archive/tar: invalid tar header` errors. If you encounter build issues:\n",
        "\n",
        "**Quick Fix:**\n",
        "```bash\n",
        "python scripts/fix_build.py\n",
        "```\n",
        "\n",
        "**Manual Fix Steps:**\n",
        "1. Clear corrupted caches: `docker builder prune --all --force`\n",
        "2. Clean build: `docker-compose build --no-cache`\n",
        "3. Test cached build: `docker-compose build`\n",
        "\n",
        "**Key Improvements:**\n",
        "- Added explicit cache IDs: `apt-cache`, `apt-lists`, `uv-cache`\n",
        "- Prevents cache key collisions and corruption\n",
        "- Maintains fast build times with reliable caching\n",
        "\n",
        "## Testing & Diagnostics\n",
        "\n",
        "Creating comprehensive testing and diagnostic scripts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ../../.devcontainer/tests/test_pytorch_gpu.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile ../../.devcontainer/tests/test_pytorch_gpu.py\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"Small PyTorch GPU benchmark.\"\"\"\n",
        "import time\n",
        "\n",
        "\n",
        "def test_pytorch(force_cpu: bool = False) -> None:\n",
        "    import torch\n",
        "    cuda_ok = torch.cuda.is_available() and not force_cpu\n",
        "    if cuda_ok:\n",
        "        name = torch.cuda.get_device_name(0)\n",
        "        major, minor = torch.cuda.get_device_capability()\n",
        "        print(f\"device: {name} (sm_{major}{minor:02d})\")\n",
        "        device = torch.device(\"cuda:0\")\n",
        "    else:\n",
        "        print(\"falling back to cpu\")\n",
        "        device = torch.device(\"cpu\")\n",
        "\n",
        "    size = (1000, 1000)\n",
        "    a, b = (torch.randn(size, device=device) for _ in range(2))\n",
        "    _ = a @ b\n",
        "    t0 = time.time()\n",
        "    _ = (a @ b).sum().item()\n",
        "    if device.type == \"cuda\":\n",
        "        torch.cuda.synchronize()\n",
        "    print(f\"matmul on {device} took {(time.time()-t0)*1000:.2f} ms\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_pytorch()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Verification\n",
        "\n",
        "Now let's test that our environment setup is working correctly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ../../.devcontainer/tests/test_uv.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile ../../.devcontainer/tests/test_uv.py\n",
        "\"\"\"UV and key package presence check.\"\"\"\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "print(\"UV version:\")\n",
        "try:\n",
        "    r = subprocess.run([\"uv\", \"--version\"], capture_output=True, text=True)\n",
        "    print(r.stdout.strip() or r.stderr.strip())\n",
        "except FileNotFoundError:\n",
        "    print(\"uv not found\")\n",
        "\n",
        "print(\"\\nPython:\")\n",
        "print(sys.executable)\n",
        "print(sys.version)\n",
        "\n",
        "print(\"\\nKey packages:\")\n",
        "for pkg in [\"numpy\", \"pandas\", \"matplotlib\", \"scipy\", \"sklearn\", \"jupyterlab\", \"seaborn\", \"tqdm\"]:\n",
        "    try:\n",
        "        if pkg == \"sklearn\":\n",
        "            import sklearn as m\n",
        "        else:\n",
        "            m = __import__(pkg)\n",
        "        print(pkg, getattr(m, \"__version__\", \"unknown\"))\n",
        "    except Exception as e:\n",
        "        print(pkg, \"missing or error:\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ../../.devcontainer/tests/test_pytorch.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile ../../.devcontainer/tests/test_pytorch.py\n",
        "print(\"PyTorch quick check\")\n",
        "try:\n",
        "    import torch\n",
        "    print(\"version:\", torch.__version__)\n",
        "    print(\"cuda:\", torch.cuda.is_available())\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"devices:\", torch.cuda.device_count())\n",
        "        for i in range(torch.cuda.device_count()):\n",
        "            print(i, torch.cuda.get_device_name(i))\n",
        "        x = torch.ones(100, 100, device='cuda:0')\n",
        "        print(\"sum:\", float(torch.sum(x)))\n",
        "except Exception as e:\n",
        "    print(\"error:\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ../../.devcontainer/tests/test_uv.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile ../../.devcontainer/tests/test_uv.py\n",
        "# Test other critical packages\n",
        "print(\"\\n📦 Testing other critical packages...\")\n",
        "\n",
        "packages_to_test = [\n",
        "    'numpy', 'pandas', 'matplotlib', 'scipy', 'sklearn', \n",
        "    'jupyterlab', 'seaborn', 'tqdm'\n",
        "]\n",
        "\n",
        "for package in packages_to_test:\n",
        "    try:\n",
        "        if package == 'sklearn':\n",
        "            import sklearn\n",
        "            version = sklearn.__version__\n",
        "        else:\n",
        "            module = __import__(package)\n",
        "            version = getattr(module, '__version__', 'unknown')\n",
        "        print(f\"   ✅ {package}: {version}\")\n",
        "    except ImportError:\n",
        "        print(f\"   ❌ {package}: Not installed\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ⚠️  {package}: Error - {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ../../.devcontainer/tests/test_summary.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile ../../.devcontainer/tests/test_summary.py\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"Aggregated checks for the devcontainer layout and GPU readiness.\"\"\"\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import subprocess\n",
        "\n",
        "\n",
        "def section(t):\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(t)\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "\n",
        "def test_structure() -> bool:\n",
        "    section(\"STRUCTURE\")\n",
        "    expected = [\n",
        "        '/workspace/docker-compose.yml',\n",
        "        '/workspace/pyproject.toml',\n",
        "        '/workspace/.devcontainer/devcontainer.json',\n",
        "        '/workspace/.devcontainer/Dockerfile',\n",
        "        '/workspace/.devcontainer/.env.template',\n",
        "        '/workspace/.devcontainer/.dockerignore',\n",
        "        '/app/validate_gpu.py',\n",
        "        '/app/tests/'\n",
        "    ]\n",
        "    ok = True\n",
        "    for p in expected:\n",
        "        if os.path.exists(p):\n",
        "            print(\"ok:\", p)\n",
        "        else:\n",
        "            print(\"missing:\", p)\n",
        "            ok = False\n",
        "    return ok\n",
        "\n",
        "\n",
        "def test_uv() -> bool:\n",
        "    section(\"UV\")\n",
        "    try:\n",
        "        r = subprocess.run(['uv', '--version'], capture_output=True, text=True)\n",
        "        print(r.stdout.strip() or r.stderr.strip())\n",
        "        return r.returncode == 0\n",
        "    except FileNotFoundError:\n",
        "        print('uv not in PATH')\n",
        "        return False\n",
        "\n",
        "\n",
        "def test_pytorch() -> bool:\n",
        "    section(\"PYTORCH\")\n",
        "    try:\n",
        "        import torch\n",
        "        print(\"version:\", torch.__version__)\n",
        "        print(\"cuda:\", torch.cuda.is_available())\n",
        "        if torch.cuda.is_available():\n",
        "            d = torch.device('cuda:0')\n",
        "            x = torch.ones(512, 512, device=d)\n",
        "            y = torch.sum(x)\n",
        "            print(\"sum:\", y.item())\n",
        "            return True\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(\"error:\", e)\n",
        "        return False\n",
        "\n",
        "\n",
        "def test_jax() -> bool:\n",
        "    section(\"JAX\")\n",
        "    try:\n",
        "        import jax, jax.numpy as jnp\n",
        "\n",
        "        # Show all devices for visibility\n",
        "        devs = jax.devices()\n",
        "        print(\"devices:\", devs)\n",
        "\n",
        "        # Prefer the supported filtered query\n",
        "        gpus = jax.devices(\"gpu\")\n",
        "\n",
        "        # Fallback for older/newer renderings (e.g., \"CudaDevice(id=0)\")\n",
        "        if not gpus:\n",
        "            gpus = [\n",
        "                d for d in devs\n",
        "                if getattr(d, \"platform\", \"\").lower() in {\"gpu\", \"cuda\"} or \"cuda\" in str(d).lower()\n",
        "            ]\n",
        "\n",
        "        if not gpus:\n",
        "            print(\"no gpu devices detected by jax\")\n",
        "            return False\n",
        "\n",
        "        # Tiny compute on the first GPU to ensure execution\n",
        "        x = jnp.ones((512, 512), dtype=jnp.float32)\n",
        "        x = jax.device_put(x, gpus[0])\n",
        "        s = jnp.sum(x).block_until_ready()\n",
        "        print(\"sum:\", float(s))\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(\"error:\", e)\n",
        "        return False\n",
        "\n",
        "\n",
        "\n",
        "def main() -> int:\n",
        "    s_ok = test_structure()\n",
        "    uv_ok = test_uv()\n",
        "    pt_ok = test_pytorch()\n",
        "    j_ok = test_jax()\n",
        "\n",
        "    section(\"SUMMARY\")\n",
        "    print(\"structure:\", s_ok, \"uv:\", uv_ok, \"pytorch:\", pt_ok, \"jax:\", j_ok)\n",
        "    return 0 if all([s_ok, uv_ok, pt_ok, j_ok]) else 1\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    sys.exit(main())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
