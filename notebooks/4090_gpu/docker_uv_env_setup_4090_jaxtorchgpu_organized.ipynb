{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Streamlined Docker Development Environment Setup\n",
        "\n",
        "This notebook sets up a streamlined Docker-based development environment with:\n",
        "- PyTorch with GPU support (official base image)\n",
        "- UV package manager for fast dependency resolution\n",
        "- VS Code devcontainer integration\n",
        "- Simplified configuration and testing\n",
        "\n",
        "**Key improvements over previous setup:**\n",
        "- ~40% faster build time using official PyTorch base image\n",
        "- Simplified environment configuration\n",
        "- PyTorch/Jax-focused GPU acceleration\n",
        "\n",
        "goal: have a container that can utilize jax and pytorch on gpu on a nvidia 4090 gpu. Let's ensure that each of the requirements are set and that once in the container it have the uv environemnt set up as well for development. It should show errors and the root of the problem otherwise, so have plenty of healthchecks to ensure everything is going correctly.  \n",
        "\n",
        "├── pyproject.toml ( for local development)\n",
        "└── .devcontainer/\n",
        "    ├── docker-compose.yml\n",
        "    ├── Dockerfile (for docker build)\n",
        "    ├── devcontainer.json\n",
        "    ├── .env.template\n",
        "    ├── .dockerignore\n",
        "    ├── validate_gpu.py\n",
        "    └── tests/\n",
        "        ├── test_summary.py\n",
        "        ├── test_pytorch.py\n",
        "        ├── test_pytorch_gpu.py\n",
        "        └── test_uv.py\n",
        "\n",
        "\n",
        "## Table of Contents\n",
        "1. [Environment Files](#environment-files)\n",
        "2. [DevContainer Configuration](#devcontainer-configuration)\n",
        "3. [Docker Setup](#docker-setup)\n",
        "4. [Testing & Diagnostics](#testing--diagnostics)\n",
        "5. [Verification](#verification)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Environment Files\n",
        "\n",
        "First, let's create the necessary environment configuration files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ../../.devcontainer/.env.template\n"
          ]
        }
      ],
      "source": [
        "%%writefile ../../.devcontainer/.env.template\n",
        "ENV_NAME=docker_dev_template\n",
        "\n",
        "# GPU Configuration for RTX 4090\n",
        "CUDA_TAG=12.4.0\n",
        "PYTHON_VER=3.10\n",
        "\n",
        "# Host Port Configuration\n",
        "HOST_JUPYTER_PORT=8891\n",
        "HOST_TENSORBOARD_PORT=6008\n",
        "HOST_EXPLAINER_PORT=8050\n",
        "HOST_STREAMLIT_PORT=8501\n",
        "HOST_MLFLOW_PORT=5000\n",
        "\n",
        "# JAX/GPU Configuration - CRITICAL: NO INLINE COMMENTS\n",
        "# These environment variables are parsed directly by JAX and must be clean\n",
        "\n",
        "# Memory fraction for GPU allocation (0.0 to 1.0)\n",
        "# For RTX 4090 24GB VRAM, 0.4 provides good balance\n",
        "XLA_PYTHON_CLIENT_MEM_FRACTION=0.4\n",
        "\n",
        "# Disable memory preallocation for better memory management\n",
        "XLA_PYTHON_CLIENT_PREALLOCATE=false\n",
        "\n",
        "# Use platform allocator for optimal GPU memory handling\n",
        "XLA_PYTHON_CLIENT_ALLOCATOR=platform\n",
        "\n",
        "# XLA compiler flags for CUDA\n",
        "XLA_FLAGS=--xla_gpu_cuda_data_dir=/usr/local/cuda\n",
        "\n",
        "# JAX memory preallocation limit in bytes\n",
        "# 16GB limit (17179869184 bytes) for RTX 4090\n",
        "JAX_PREALLOCATION_SIZE_LIMIT_BYTES=17179869184\n",
        "\n",
        "# JAX behavior configuration\n",
        "JAX_DISABLE_JIT=false\n",
        "JAX_ENABLE_X64=false\n",
        "\n",
        "# TensorFlow GPU configuration (if using TensorFlow)\n",
        "TF_FORCE_GPU_ALLOW_GROWTH=true\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ../../.devcontainer/.dockerignore\n"
          ]
        }
      ],
      "source": [
        "%%writefile ../../.devcontainer/.dockerignore\n",
        "# Reduce Docker build context\n",
        ".git\n",
        ".gitignore\n",
        ".gitattributes\n",
        ".gitmodules\n",
        ".vscode\n",
        ".idea\n",
        "*.swp\n",
        "*.swo\n",
        "*~\n",
        ".DS_Store\n",
        "Thumbs.db\n",
        "__pycache__\n",
        "*.pyc\n",
        "*.pyo\n",
        "*.pyd\n",
        ".Python\n",
        "*.so\n",
        ".coverage*\n",
        ".cache\n",
        ".pytest_cache\n",
        ".mypy_cache\n",
        ".tox\n",
        "pip-log.txt\n",
        "pip-delete-this-directory.txt\n",
        "env\n",
        "venv\n",
        "ENV\n",
        "env.bak\n",
        "venv.bak\n",
        ".ipynb_checkpoints\n",
        "# Large data (adjust as needed)\n",
        "data/raw\n",
        "data/external\n",
        "*.csv\n",
        "*.parquet\n",
        "*.h5\n",
        "*.hdf5\n",
        "# Models\n",
        "*.pt\n",
        "*.pth\n",
        "*.pkl\n",
        "*.joblib\n",
        "models/\n",
        "# Logs and temps\n",
        "*.log\n",
        "logs/\n",
        "*.tmp\n",
        "*.temp\n",
        ".tmp\n",
        "temp/\n",
        "# Build artifacts\n",
        "build/\n",
        "dist/\n",
        "*.egg-info/\n",
        ".eggs/\n",
        "# Node\n",
        "node_modules\n",
        "npm-debug.log*\n",
        "yarn-*.log*\n",
        ".npm\n",
        ".eslintcache\n",
        ".node_repl_history\n",
        "*.tgz\n",
        "*.tar.gz\n",
        "# Archives\n",
        "*.zip\n",
        "*.tar\n",
        "*.tar.bz2\n",
        "*.rar\n",
        "*.7z\n",
        "# Docs (opt‑in if needed)\n",
        "docs/\n",
        "*.md\n",
        "README*\n",
        "LICENSE*\n",
        "CHANGELOG*\n",
        "# Tests (opt‑in if needed)\n",
        "tests/\n",
        "test_*\n",
        "*_test.py\n",
        "# CI\n",
        ".github/\n",
        ".gitlab-ci.yml\n",
        ".travis.yml\n",
        ".circleci/\n",
        "azure-pipelines.yml\n",
        "# Env\n",
        ".env\n",
        ".env.local\n",
        ".env.*.local\n",
        ".editorconfig\n",
        ".prettierrc*\n",
        ".eslintrc*\n",
        "# Universal junk (de‑duped)\n",
        "*.py[cod]"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## DevContainer Configuration\n",
        "\n",
        "Setting up the VS Code devcontainer configuration files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ../../.devcontainer/devcontainer.json\n"
          ]
        }
      ],
      "source": [
        "%%writefile ../../.devcontainer/devcontainer.json\n",
        "{\n",
        "  \"name\": \"docker_dev_template_rtx4090\",\n",
        "  \"dockerComposeFile\": \"docker-compose.yml\",\n",
        "  \"service\": \"datascience\",\n",
        "  \"workspaceFolder\": \"/workspace\",\n",
        "  \"shutdownAction\": \"stopCompose\",\n",
        "\n",
        "  // Disable command override to use compose command\n",
        "  \"overrideCommand\": false,\n",
        "  \n",
        "  // Container environment variables\n",
        "  \"containerEnv\": {\n",
        "    \"CONTAINER_WORKSPACE_FOLDER\": \"/workspace\",\n",
        "    \"UV_PROJECT_ENVIRONMENT\": \"/app/.venv\",\n",
        "    \"VIRTUAL_ENV\": \"/app/.venv\",\n",
        "    \"PYTHONPATH\": \"/workspace\",\n",
        "    \"TERM\": \"xterm-256color\",\n",
        "    \"DEBIAN_FRONTEND\": \"noninteractive\"\n",
        "  },\n",
        "\n",
        "  // Additional run arguments for GPU support\n",
        "  \"runArgs\": [\n",
        "    \"--gpus\", \"all\",\n",
        "    \"--name\", \"${localEnv:ENV_NAME:docker_dev_template}_datascience\",\n",
        "    \"--shm-size\", \"8g\"\n",
        "  ],\n",
        "\n",
        "  // VS Code customizations\n",
        "  \"customizations\": {\n",
        "    \"vscode\": {\n",
        "      \"settings\": {\n",
        "        // Python interpreter settings\n",
        "        \"python.defaultInterpreterPath\": \"/app/.venv/bin/python\",\n",
        "        \"python.pythonPath\": \"/app/.venv/bin/python\",\n",
        "        \"python.terminal.activateEnvironment\": true,\n",
        "        \"python.terminal.activateEnvInCurrentTerminal\": true,\n",
        "        \n",
        "        // Terminal configuration\n",
        "        \"terminal.integrated.defaultProfile.linux\": \"bash\",\n",
        "        \"terminal.integrated.profiles.linux\": {\n",
        "          \"bash\": {\n",
        "            \"path\": \"/bin/bash\",\n",
        "            \"args\": [\"-l\"],\n",
        "            \"env\": {\n",
        "              \"VIRTUAL_ENV\": \"/app/.venv\",\n",
        "              \"PATH\": \"/app/.venv/bin:${env:PATH}\",\n",
        "              \"UV_PROJECT_ENVIRONMENT\": \"/app/.venv\",\n",
        "              \"PYTHONPATH\": \"/workspace\"\n",
        "            }\n",
        "          }\n",
        "        },\n",
        "        \n",
        "        // Jupyter settings\n",
        "        \"jupyter.notebookFileRoot\": \"/workspace\",\n",
        "        \"jupyter.kernels.filter\": [\n",
        "          {\n",
        "            \"path\": \"/app/.venv/bin/python\",\n",
        "            \"type\": \"pythonEnvironment\"\n",
        "          }\n",
        "        ],\n",
        "        \"jupyter.interactiveWindow.creationMode\": \"perFile\",\n",
        "        \n",
        "        // File associations and workspace settings\n",
        "        \"files.watcherExclude\": {\n",
        "          \"**/.git/**\": true,\n",
        "          \"**/node_modules/**\": true,\n",
        "          \"**/__pycache__/**\": true,\n",
        "          \"**/.pytest_cache/**\": true,\n",
        "          \"**/.venv/**\": true\n",
        "        },\n",
        "        \n",
        "        // Docker settings\n",
        "        \"docker.showStartPage\": false\n",
        "      },\n",
        "      \n",
        "      // Essential extensions\n",
        "      \"extensions\": [\n",
        "        \"ms-python.python\",\n",
        "        \"ms-python.flake8\", \n",
        "        \"ms-python.black-formatter\",\n",
        "        \"ms-toolsai.jupyter\",\n",
        "        \"ms-azuretools.vscode-docker\",\n",
        "        \"ms-vscode.makefile-tools\"\n",
        "      ]\n",
        "    }\n",
        "  },\n",
        "\n",
        "  // Lifecycle commands with better error handling\n",
        "  \"onCreateCommand\": {\n",
        "    \"validate-environment\": [\n",
        "      \"bash\", \"-lc\", \n",
        "      \"echo 'onCreate: Validating environment setup...'; ls -la /app/.venv/bin/ || echo 'Virtual env not ready'; which python || echo 'Python not found in PATH'; echo 'onCreate validation complete.'\"\n",
        "    ]\n",
        "  },\n",
        "\n",
        "  \"postCreateCommand\": {\n",
        "    \"setup-jupyter-kernel\": [\n",
        "      \"bash\", \"-lc\",\n",
        "      \"set -e; echo 'Setting up Jupyter kernel...'; \\\n",
        "      source /app/.venv/bin/activate; \\\n",
        "      python -c \\\"import sys; print(f'Python executable: {sys.executable}')\\\"; \\\n",
        "      python - <<'PY'\\nimport importlib\\nfor m in ('ipykernel','jupyter_client','psutil','debugpy'):\\n    try:\\n        mod=importlib.import_module(m)\\n        print(f'ok: {m} {getattr(mod,\\\"__version__\\\",\\\"unknown\\\")}')\\n    except Exception as e:\\n        print(f'ERR: {m} -> {e}')\\nPY\\n; \\\n",
        "      python -m ipykernel install --user --name='uv_docker_dev_template' --display-name='Python (UV Environment)' || echo 'Kernel install failed'; \\\n",
        "      jupyter kernelspec list || echo 'Cannot list kernels'; \\\n",
        "      echo 'Running environment tests...'; \\\n",
        "      python /app/tests/test_summary.py || echo 'Tests completed with warnings'\"\n",
        "    ]\n",
        "  },\n",
        "\n",
        "  \"postStartCommand\": {\n",
        "    \"validate-gpu-quick\": [\n",
        "      \"bash\", \"-lc\",\n",
        "      \"echo 'postStart: Running quick validation...'; source /app/.venv/bin/activate; python --version; python -c 'import torch; print(f\\\"PyTorch CUDA available: {torch.cuda.is_available()}\\\")' || echo 'PyTorch validation failed'; python /app/validate_gpu.py --quick || echo 'GPU validation completed with warnings'; echo 'Container ready for development!'\"\n",
        "    ]\n",
        "  },\n",
        "\n",
        "  // Port forwarding with better labeling\n",
        "  \"forwardPorts\": [8888, 6008, 8050, 8501, 5000],\n",
        "  \"portsAttributes\": {\n",
        "    \"8888\": { \n",
        "      \"label\": \"Jupyter Lab\", \n",
        "      \"onAutoForward\": \"notify\",\n",
        "      \"protocol\": \"http\"\n",
        "    },\n",
        "    \"6008\": { \n",
        "      \"label\": \"TensorBoard\", \n",
        "      \"onAutoForward\": \"silent\",\n",
        "      \"protocol\": \"http\"\n",
        "    },\n",
        "    \"8050\": { \n",
        "      \"label\": \"Explainer Dashboard\", \n",
        "      \"onAutoForward\": \"silent\",\n",
        "      \"protocol\": \"http\"\n",
        "    },\n",
        "    \"8501\": { \n",
        "      \"label\": \"Streamlit\", \n",
        "      \"onAutoForward\": \"silent\",\n",
        "      \"protocol\": \"http\"\n",
        "    },\n",
        "    \"5000\": { \n",
        "      \"label\": \"MLflow\", \n",
        "      \"onAutoForward\": \"silent\",\n",
        "      \"protocol\": \"http\"\n",
        "    }\n",
        "  },\n",
        "\n",
        "  // Volume mounts for caching\n",
        "  \"mounts\": [\n",
        "    \"source=docker_dev_template_uv_cache,target=/root/.cache/uv,type=volume\"\n",
        "  ],\n",
        "\n",
        "  // Additional features and settings\n",
        "  \"features\": {},\n",
        "  \n",
        "  // Wait for services to be ready\n",
        "  \"waitFor\": \"postCreateCommand\",\n",
        "  \n",
        "  // Increase timeout for initial setup\n",
        "  \"postCreateCommand.timeout\": 300,\n",
        "  \"postStartCommand.timeout\": 120\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ../../.devcontainer/Dockerfile\n"
          ]
        }
      ],
      "source": [
        "%%writefile ../../.devcontainer/Dockerfile\n",
        "# Fixed Dockerfile: RTX 4090 devcontainer with UV, JAX, and PyTorch (CUDA 12.x)\n",
        "\n",
        "ARG CUDA_TAG=12.4.0\n",
        "FROM nvidia/cuda:${CUDA_TAG}-devel-ubuntu22.04\n",
        "\n",
        "ARG PYTHON_VER=3.10\n",
        "ARG ENV_NAME=docker_dev_template\n",
        "ENV DEBIAN_FRONTEND=noninteractive\n",
        "\n",
        "# System dependencies\n",
        "RUN --mount=type=cache,id=apt-cache,target=/var/cache/apt,sharing=locked \\\n",
        "    --mount=type=cache,id=apt-lists,target=/var/lib/apt/lists,sharing=locked \\\n",
        "    apt-get update && apt-get install -y --no-install-recommends \\\n",
        "        bash curl ca-certificates git procps htop \\\n",
        "        python3 python3-venv python3-pip python3-dev \\\n",
        "        build-essential cmake pkg-config \\\n",
        "        libjemalloc2 libjemalloc-dev \\\n",
        "        iproute2 net-tools lsof wget \\\n",
        "    && apt-get clean && rm -rf /var/lib/apt/lists/*\n",
        "\n",
        "# UV package manager\n",
        "COPY --from=ghcr.io/astral-sh/uv:0.7.12 /uv /uvx /bin/\n",
        "\n",
        "WORKDIR /app\n",
        "\n",
        "# Create venv managed by UV\n",
        "RUN uv venv .venv --python \"${PYTHON_VER}\" --prompt \"${ENV_NAME}\"\n",
        "\n",
        "ENV VIRTUAL_ENV=/app/.venv \\\n",
        "    PATH=\"/app/.venv/bin:${PATH}\" \\\n",
        "    UV_PROJECT_ENVIRONMENT=/app/.venv \\\n",
        "    PYTHONPATH=\"/workspace\"\n",
        "\n",
        "# Memory and allocator settings\n",
        "ENV LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libjemalloc.so.2 \\\n",
        "    MALLOC_ARENA_MAX=2 \\\n",
        "    MALLOC_TCACHE_MAX=0 \\\n",
        "    PYTORCH_NO_CUDA_MEMORY_CACHING=1\n",
        "\n",
        "# GPU-relevant environment\n",
        "ENV XLA_PYTHON_CLIENT_PREALLOCATE=false \\\n",
        "    XLA_PYTHON_CLIENT_MEM_FRACTION=0.4 \\\n",
        "    XLA_PYTHON_CLIENT_ALLOCATOR=platform \\\n",
        "    PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:1024,expandable_segments:True \\\n",
        "    JAX_PREALLOCATION_SIZE_LIMIT_BYTES=17179869184\n",
        "\n",
        "# Project files\n",
        "COPY pyproject.toml /workspace/\n",
        "\n",
        "# Devcontainer tests and validator\n",
        "COPY .devcontainer/validate_gpu.py /app/validate_gpu.py\n",
        "COPY .devcontainer/tests/ /app/tests/\n",
        "\n",
        "# (IMPORTANT) Do NOT 'uv init' if pyproject exists. Just lock/sync.\n",
        "# Optionally create a lock explicitly when it's missing (documented and safe)\n",
        "RUN --mount=type=cache,target=/root/.cache/uv,sharing=locked \\\n",
        "    cd /workspace && \\\n",
        "    if [ ! -f uv.lock ]; then \\\n",
        "      echo \"[uv] No uv.lock found; creating from existing pyproject.toml\"; \\\n",
        "      uv lock --refresh; \\\n",
        "    else \\\n",
        "      echo \"[uv] Using existing uv.lock\"; \\\n",
        "    fi\n",
        "\n",
        "# Resolve non-GPU project dependencies first (clean, deterministic)\n",
        "RUN --mount=type=cache,target=/root/.cache/uv,sharing=locked \\\n",
        "    cd /workspace && \\\n",
        "    (uv sync --frozen --no-dev 2>/dev/null || \\\n",
        "     uv sync --no-dev 2>/dev/null || \\\n",
        "     (echo \"[uv] Installing basic dependencies...\" && uv add numpy pandas matplotlib scipy))\n",
        "\n",
        "# --- GPU stack: install PyTorch (CUDA 12.4) and JAX (CUDA 12) ---\n",
        "# Keep these out of pyproject to avoid conflicts; install explicitly here.\n",
        "\n",
        "# PyTorch with CUDA 12.4 wheels\n",
        "RUN --mount=type=cache,target=/root/.cache/uv,sharing=locked \\\n",
        "    uv pip install --no-cache-dir torch torchvision torchaudio \\\n",
        "        --index-url https://download.pytorch.org/whl/cu124 && \\\n",
        "    python - <<'PY'\n",
        "import torch\n",
        "print(\"Torch version:\", torch.__version__)\n",
        "print(\"CUDA available during build:\", torch.cuda.is_available())\n",
        "print(\"Note: CUDA will be available at runtime when container has GPU access\")\n",
        "PY\n",
        "\n",
        "# JAX with CUDA 12 wheels (from official wheel index)\n",
        "RUN --mount=type=cache,target=/root/.cache/uv,sharing=locked \\\n",
        "    uv pip install --no-cache-dir --upgrade \\\n",
        "        \"jax[cuda12]\" \\\n",
        "        -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html && \\\n",
        "    python - <<'PY'\n",
        "import jax, jaxlib\n",
        "print(\"JAX:\", jax.__version__, \"JAXLIB:\", jaxlib.__version__)\n",
        "print(\"JAX devices during build:\", jax.devices())\n",
        "print(\"Note: GPU devices will be available at runtime when container has GPU access\")\n",
        "PY\n",
        "\n",
        "# Jupyter & kernel: pin versions and avoid corrupt wheels; include psutil/debugpy\n",
        "RUN --mount=type=cache,target=/root/.cache/uv,sharing=locked \\\n",
        "    uv pip install --no-cache-dir \\\n",
        "        psutil==5.9.8 \\\n",
        "        debugpy==1.8.7 \\\n",
        "        ipykernel==6.29.5 \\\n",
        "        jupyter-client==8.6.1 \\\n",
        "        jupyterlab==4.2.5\n",
        "\n",
        "# CUDA libs in path - include both system and any packaged libs\n",
        "ENV LD_LIBRARY_PATH=\"/app/.venv/lib:/usr/local/cuda/lib64:${LD_LIBRARY_PATH}\"\n",
        "\n",
        "# Shell activation helper\n",
        "RUN echo '#!/bin/bash' > /app/activate_uv.sh && \\\n",
        "    echo 'export VIRTUAL_ENV=\"/app/.venv\"' >> /app/activate_uv.sh && \\\n",
        "    echo 'export PATH=\"/app/.venv/bin:$PATH\"' >> /app/activate_uv.sh && \\\n",
        "    echo 'export UV_PROJECT_ENVIRONMENT=\"/app/.venv\"' >> /app/activate_uv.sh && \\\n",
        "    echo 'export PYTHONPATH=\"/workspace:$PYTHONPATH\"' >> /app/activate_uv.sh && \\\n",
        "    echo 'export LD_LIBRARY_PATH=\"/app/.venv/lib:/usr/local/cuda/lib64:${LD_LIBRARY_PATH}\"' >> /app/activate_uv.sh && \\\n",
        "    echo 'cd /workspace' >> /app/activate_uv.sh && \\\n",
        "    chmod +x /app/activate_uv.sh && \\\n",
        "    echo 'source /app/activate_uv.sh' > /etc/profile.d/10-uv-activate.sh && \\\n",
        "    echo 'source /app/activate_uv.sh' >> /root/.bashrc && \\\n",
        "    chmod +x /etc/profile.d/10-uv-activate.sh\n",
        "\n",
        "# Enhanced healthcheck script\n",
        "RUN echo '#!/bin/bash' > /app/healthcheck.sh && \\\n",
        "    echo 'source /app/.venv/bin/activate' >> /app/healthcheck.sh && \\\n",
        "    echo 'python /app/validate_gpu.py --quick' >> /app/healthcheck.sh && \\\n",
        "    chmod +x /app/healthcheck.sh\n",
        "\n",
        "WORKDIR /workspace\n",
        "CMD [\"bash\", \"-l\"]\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Docker Setup\n",
        "\n",
        "Creating the Docker Compose configuration and project files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ../../.devcontainer/docker-compose.yml\n"
          ]
        }
      ],
      "source": [
        "%%writefile ../../.devcontainer/docker-compose.yml\n",
        "# Fixed .devcontainer/docker-compose.yml \n",
        "name: ${ENV_NAME:-docker_dev_template}\n",
        "\n",
        "services:\n",
        "  datascience:\n",
        "    build:\n",
        "      # Build context is parent directory (project root)\n",
        "      context: ..\n",
        "      dockerfile: .devcontainer/Dockerfile\n",
        "      args:\n",
        "        CUDA_TAG: ${CUDA_TAG:-12.4.0}\n",
        "        PYTHON_VER: ${PYTHON_VER:-3.10}\n",
        "        ENV_NAME: ${ENV_NAME:-docker_dev_template}\n",
        "      cache_from:\n",
        "        - nvidia/cuda:${CUDA_TAG:-12.4.0}-devel-ubuntu22.04\n",
        "      # Additional build options for better Windows compatibility\n",
        "      extra_hosts:\n",
        "        - \"host.docker.internal:host-gateway\"\n",
        "\n",
        "    container_name: ${ENV_NAME:-docker_dev_template}_datascience\n",
        "\n",
        "    # Environment template now referenced locally with fallback\n",
        "    env_file:\n",
        "      - .env\n",
        "\n",
        "    restart: unless-stopped\n",
        "    depends_on:\n",
        "      mlflow:\n",
        "        condition: service_started  # FIXED: Changed from service_healthy to service_started\n",
        "\n",
        "    deploy:\n",
        "      resources:\n",
        "        reservations:\n",
        "          devices:\n",
        "            - driver: nvidia\n",
        "              count: all\n",
        "              capabilities: [gpu]\n",
        "\n",
        "    # Better process management\n",
        "    init: true\n",
        "    gpus: all\n",
        "    shm_size: 8g\n",
        "    ulimits:\n",
        "      memlock: -1\n",
        "      stack: 67108864\n",
        "\n",
        "    environment:\n",
        "      - PYTHON_VER=${PYTHON_VER:-3.10}\n",
        "      - UV_PROJECT_ENVIRONMENT=/app/.venv\n",
        "      - VIRTUAL_ENV=/app/.venv\n",
        "      - PYTHONPATH=/workspace\n",
        "      - NVIDIA_VISIBLE_DEVICES=all\n",
        "      - NVIDIA_DRIVER_CAPABILITIES=compute,utility\n",
        "      - CUDA_VISIBLE_DEVICES=0\n",
        "      - LD_LIBRARY_PATH=/app/.venv/lib:/usr/local/cuda/lib64\n",
        "      - LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libjemalloc.so.2\n",
        "      - MALLOC_ARENA_MAX=2\n",
        "      - MALLOC_TCACHE_MAX=0\n",
        "      - PYTORCH_NO_CUDA_MEMORY_CACHING=1\n",
        "      \n",
        "      # CRITICAL FIX: Clean JAX environment variables (no inline comments)\n",
        "      - XLA_PYTHON_CLIENT_PREALLOCATE=false\n",
        "      - XLA_PYTHON_CLIENT_ALLOCATOR=platform\n",
        "      - XLA_PYTHON_CLIENT_MEM_FRACTION=0.4\n",
        "      - XLA_FLAGS=--xla_gpu_cuda_data_dir=/usr/local/cuda\n",
        "      - JAX_PREALLOCATION_SIZE_LIMIT_BYTES=17179869184\n",
        "      - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:1024,expandable_segments:True\n",
        "      - JUPYTER_TOKEN=${JUPYTER_TOKEN:-jupyter}\n",
        "\n",
        "    volumes:\n",
        "      # Mount parent directory (project root) to workspace\n",
        "      - ..:/workspace:delegated\n",
        "      # MLflow volumes reference parent directory  \n",
        "      - ../mlruns:/workspace/mlruns:delegated\n",
        "      - ../mlflow_db:/workspace/mlflow_db:delegated\n",
        "      # UV cache volume for faster builds\n",
        "      - uv-cache:/root/.cache/uv\n",
        "\n",
        "    ports:\n",
        "      - \"${HOST_JUPYTER_PORT:-8891}:8888\"\n",
        "      - \"${HOST_TENSORBOARD_PORT:-6008}:6008\"\n",
        "      - \"${HOST_EXPLAINER_PORT:-8050}:8050\"\n",
        "      - \"${HOST_STREAMLIT_PORT:-8501}:8501\"\n",
        "\n",
        "    # Enhanced startup command with better error handling\n",
        "    command: >\n",
        "      bash -lc '\n",
        "        set -e;\n",
        "        echo \"[boot] Starting container: ${ENV_NAME:-docker_dev_template}\";\n",
        "        echo \"[boot] System info: $(uname -a)\";\n",
        "        echo \"[boot] Working directory: $(pwd)\";\n",
        "        echo \"[boot] Files in /workspace: $(ls -la /workspace/ | head -10)\";\n",
        "        echo \"[boot] Activating uv environment...\";\n",
        "        source /app/.venv/bin/activate;\n",
        "        echo \"[boot] Environment activated - Python: $(which python)\";\n",
        "        echo \"[boot] Python version: $(python --version)\";\n",
        "        echo \"[boot] UV available: $(uv --version 2>/dev/null || echo \"uv not found\")\";\n",
        "        echo \"[boot] Running GPU validation...\";\n",
        "        python /app/validate_gpu.py --quick || echo \"GPU validation completed with warnings\";\n",
        "        echo \"[boot] Starting Jupyter Lab on port 8888...\";\n",
        "        exec jupyter lab --ip=0.0.0.0 --port=8888 --allow-root \\\n",
        "        --ServerApp.token=\"${JUPYTER_TOKEN:-jupyter}\" \\\n",
        "        --ServerApp.allow_origin=\"*\" \\\n",
        "        --ServerApp.open_browser=false \\\n",
        "        --ServerApp.root_dir=\"/workspace\"\n",
        "      '\n",
        "\n",
        "    # More robust healthcheck\n",
        "    healthcheck:\n",
        "      test: |\n",
        "        bash -c '\n",
        "          source /app/.venv/bin/activate 2>/dev/null || exit 1;\n",
        "          python -c \"\n",
        "            import sys, torch, jax;\n",
        "            assert torch.cuda.is_available(), \\\"PyTorch CUDA not available\\\";\n",
        "            gpu_devs = [d for d in jax.devices() if \\\"gpu\\\" in str(d).lower()];\n",
        "            assert len(gpu_devs) > 0, \\\"JAX GPU devices not found\\\";\n",
        "            print(f\\\"Health check OK: PyTorch CUDA={torch.cuda.is_available()}, JAX GPUs={len(gpu_devs)}\\\")\n",
        "          \" 2>/dev/null || (echo \"GPU check failed\" && exit 1)\n",
        "        '\n",
        "      interval: 60s\n",
        "      timeout: 30s\n",
        "      retries: 3\n",
        "      start_period: 180s  # Longer startup time for initial build\n",
        "\n",
        "    labels:\n",
        "      - \"com.docker.compose.project=${ENV_NAME:-docker_dev_template}\"\n",
        "      - \"com.docker.compose.service=datascience\"\n",
        "      - \"description=RTX 4090 GPU Dev Environment (PyTorch+JAX) - CUDA 12.4\"\n",
        "\n",
        "  # FIXED MLflow service with robust configuration\n",
        "  mlflow:\n",
        "    container_name: ${ENV_NAME:-docker_dev_template}_mlflow\n",
        "    image: ghcr.io/mlflow/mlflow:latest\n",
        "    \n",
        "    # FIXED: Create required directories and use better startup command\n",
        "    command: >\n",
        "      bash -c '\n",
        "        set -e;\n",
        "        echo \"[MLflow] Starting MLflow server...\";\n",
        "        mkdir -p /mlflow_artifacts /mlflow_db;\n",
        "        echo \"[MLflow] Created directories\";\n",
        "        echo \"[MLflow] Database path: /mlflow_db/mlflow.db\";\n",
        "        echo \"[MLflow] Artifacts path: /mlflow_artifacts\";\n",
        "        exec mlflow server\n",
        "        --host 0.0.0.0\n",
        "        --port 5000\n",
        "        --backend-store-uri sqlite:////mlflow_db/mlflow.db\n",
        "        --default-artifact-root /mlflow_artifacts\n",
        "        --serve-artifacts\n",
        "      '\n",
        "    \n",
        "    environment:\n",
        "      MLFLOW_EXPERIMENTS_DEFAULT_ARTIFACT_LOCATION: /mlflow_artifacts\n",
        "    \n",
        "    volumes:\n",
        "      # Create host directories if they don't exist by mounting to parent directory paths\n",
        "      - ../mlruns:/mlflow_artifacts:delegated\n",
        "      - ../mlflow_db:/mlflow_db:delegated\n",
        "    \n",
        "    ports:\n",
        "      - \"${HOST_MLFLOW_PORT:-5000}:5000\"\n",
        "    \n",
        "    restart: unless-stopped\n",
        "    \n",
        "    # FIXED: Simplified and more reliable healthcheck\n",
        "    healthcheck:\n",
        "      test: |\n",
        "        timeout 10 bash -c '</dev/tcp/localhost/5000' || \n",
        "        curl -f -s http://localhost:5000 >/dev/null 2>&1 || \n",
        "        wget --quiet --tries=1 --timeout=5 --spider http://localhost:5000 || \n",
        "        exit 1\n",
        "      interval: 30s\n",
        "      timeout: 15s\n",
        "      retries: 10  # More retries for initial startup\n",
        "      start_period: 120s  # Longer startup period\n",
        "\n",
        "    labels:\n",
        "      - \"com.docker.compose.project=${ENV_NAME:-docker_dev_template}\"\n",
        "      - \"description=MLflow Experiment Tracking Server\"\n",
        "\n",
        "# Named volume for UV cache persistence\n",
        "volumes:\n",
        "  uv-cache:\n",
        "    driver: local"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ../../pyproject.toml\n"
          ]
        }
      ],
      "source": [
        "%%writefile ../../pyproject.toml\n",
        "[project]\n",
        "name = \"docker_dev_template\"\n",
        "version = \"0.1.0\"\n",
        "description = \"Hierarchical Bayesian modeling for baseball exit velocity data\"\n",
        "authors = [\n",
        "  { name = \"Marlins Data Science Team\" },\n",
        "]\n",
        "license = \"MIT\"\n",
        "readme = \"README.md\"\n",
        "\n",
        "# ─── Restrict to Python 3.10–3.12 ──────────────────────────────\n",
        "requires-python = \">=3.10,<3.13\"\n",
        "\n",
        "dependencies = [\n",
        "  \"pandas>=2.0\",\n",
        "  \"numpy>=1.20,<2\",\n",
        "  \"matplotlib>=3.4.0\",\n",
        "  \"scikit-learn>=1.4.2\",\n",
        "  \"pymc>=5.0.0\",\n",
        "  \"arviz>=0.14.0\",\n",
        "  \"statsmodels>=0.13.0\",\n",
        "  \"jupyterlab>=3.0.0\",\n",
        "  \"seaborn>=0.11.0\",\n",
        "  \"tabulate>=0.9.0\",\n",
        "  \"shap>=0.40.0\",\n",
        "  \"xgboost>=1.5.0\",\n",
        "  \"lightgbm>=3.3.0\",\n",
        "  \"catboost>=1.0.0\",\n",
        "  \"scipy>=1.7.0\",\n",
        "  \"shapash[report]>=2.3.0\",\n",
        "  \"shapiq>=1.3.0\",\n",
        "  \"explainerdashboard>=0.3.0\",\n",
        "  \"ipywidgets>=8.0.0\",\n",
        "  \"nutpie>=0.7.1\",\n",
        "  \"pytensor>=2.18.3\",\n",
        "  \"aesara>=2.9.4\",\n",
        "  \"tqdm>=4.67.0\",\n",
        "  \"pyarrow>=12.0.0\",\n",
        "  \"streamlit>=1.20.0\",\n",
        "  \"sqlalchemy>=1.4\",\n",
        "  \"mysql-connector-python>=8.0\",\n",
        "  \"optuna>=4.3.0\",\n",
        "  \"bayesian-optimization>=1.2.0\",\n",
        "  \"pretty_errors>=1.2.0\",\n",
        "  \"gdown>=4.0.0\",\n",
        "  \"invoke>=2.2\",\n",
        "  \"pytube @ git+https://github.com/pytube/pytube\",\n",
        "  \"yt-dlp>=2024.12.0\",\n",
        "  \"ffmpeg-python>=0.2.0\",\n",
        "  \"ultralytics==8.3.158\",\n",
        "  \"opencv-python-headless>=4.10.0\",\n",
        "  \"roboflow>=1.0.0\",\n",
        "  \"mlflow>=3.1.1,<4.0.0\",\n",
        "  \"optuna-integration[mlflow]>=4.4.0,<5.0.0\",\n",
        "  \"pydantic>=2.0.0\",\n",
        "  \"pydantic-settings>=2.0.0\",\n",
        "]\n",
        "\n",
        "[project.optional-dependencies]\n",
        "dev = [\n",
        "  \"pytest>=7.0.0\",\n",
        "  \"black>=23.0.0\",\n",
        "  \"isort>=5.0.0\",\n",
        "  \"flake8>=5.0.0\",\n",
        "  \"mypy>=1.0.0\",\n",
        "  \"pre-commit>=3.0.0\",\n",
        "]\n",
        "\n",
        "cuda = [\n",
        "  \"cupy-cuda12x>=12.0.0\",  # For CUDA 12.x\n",
        "]\n",
        "\n",
        "# ─── uv configuration ──────────────────────────────────────────\n",
        "[tool.uv]                   # uv reads this block\n",
        "index-strategy = \"unsafe-best-match\"\n",
        "\n",
        "# Define named indexes for PyTorch CUDA variants\n",
        "[[tool.uv.index]]\n",
        "name = \"pytorch-cu121\"\n",
        "url = \"https://download.pytorch.org/whl/cu121\"\n",
        "explicit = true\n",
        "\n",
        "[[tool.uv.index]]\n",
        "name = \"pytorch-cu118\"\n",
        "url = \"https://download.pytorch.org/whl/cu118\"\n",
        "explicit = true\n",
        "\n",
        "[[tool.uv.index]]\n",
        "name = \"pytorch-cu124\"\n",
        "url = \"https://download.pytorch.org/whl/cu124\"\n",
        "explicit = true\n",
        "\n",
        "[[tool.uv.index]]\n",
        "name = \"pytorch-cu128\"\n",
        "url = \"https://download.pytorch.org/whl/cu128\"\n",
        "explicit = true\n",
        "\n",
        "# Removed unsupported option: torch-backend requires uv ≥0.5.3\n",
        "# To re-enable, first run: pip install -U uv>=0.5.3\n",
        "[tool.uv.pip]\n",
        "# (No unsupported keys here; configure only valid pip options.)\n",
        "\n",
        "# Map only when explicitly used (not needed now; Torch installed in Dockerfile)\n",
        "[tool.uv.sources]\n",
        "\n",
        "[tool.pytensor]\n",
        "device    = \"cuda\"\n",
        "floatX    = \"float32\"\n",
        "allow_gc  = true\n",
        "optimizer = \"fast_run\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ../../.devcontainer/validate_gpu.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile ../../.devcontainer/validate_gpu.py\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Docker Build & GPU Validation Script (container-friendly)\n",
        "- Adds --quick mode to skip Docker CLI checks\n",
        "- Skips Docker checks automatically if docker is unavailable\n",
        "- Keeps strict on GPU/Torch/JAX checks\n",
        "\"\"\"\n",
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import subprocess\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple\n",
        "\n",
        "\n",
        "def print_section(title: str) -> None:\n",
        "    print(f\"\\n{'='*60}\\n  {title}\\n{'='*60}\")\n",
        "\n",
        "\n",
        "def run_command(cmd: List[str], timeout: int = 60) -> Tuple[bool, str, str]:\n",
        "    try:\n",
        "        r = subprocess.run(cmd, capture_output=True, text=True, timeout=timeout)\n",
        "        return r.returncode == 0, r.stdout, r.stderr\n",
        "    except Exception as e:\n",
        "        return False, \"\", str(e)\n",
        "\n",
        "\n",
        "def docker_available() -> bool:\n",
        "    return shutil.which(\"docker\") is not None\n",
        "\n",
        "\n",
        "def ensure_project_structure() -> bool:\n",
        "    print_section(\"ENSURING PROJECT STRUCTURE\")\n",
        "    cwd = Path.cwd()\n",
        "    print(f\"Current directory: {cwd}\")\n",
        "\n",
        "    if (cwd / \".devcontainer\").exists():\n",
        "        project_root = cwd\n",
        "    elif cwd.name == \".devcontainer\":\n",
        "        project_root = cwd.parent\n",
        "    else:\n",
        "        project_root = cwd\n",
        "        (project_root / \".devcontainer\").mkdir(exist_ok=True)\n",
        "\n",
        "    dev_dir = project_root / \".devcontainer\"\n",
        "    print(f\"Project root: {project_root}\")\n",
        "    print(f\"DevContainer directory: {dev_dir}\")\n",
        "\n",
        "    (dev_dir / \"tests\").mkdir(exist_ok=True)\n",
        "\n",
        "    pyproject = project_root / \"pyproject.toml\"\n",
        "    if not pyproject.exists():\n",
        "        print(\"Creating minimal pyproject.toml...\")\n",
        "        pyproject.write_text(\n",
        "            \"\"\"[project]\n",
        "name = \"docker_dev_template\"\n",
        "version = \"0.1.0\"\n",
        "description = \"Docker development environment\"\n",
        "requires-python = \">=3.10,<3.13\"\n",
        "\n",
        "dependencies = [\n",
        "    \"pandas>=2.0\",\n",
        "    \"numpy>=1.20,<2\",\n",
        "    \"matplotlib>=3.4.0\",\n",
        "    \"scipy>=1.7.0\",\n",
        "    \"jupyterlab>=3.0.0\",\n",
        "]\n",
        "\n",
        "[tool.uv]\n",
        "index-strategy = \"unsafe-best-match\"\n",
        "\"\"\"\n",
        "        )\n",
        "        print(\"✅ Created pyproject.toml\")\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "def create_env_file() -> bool:\n",
        "    print_section(\"CREATING ENVIRONMENT FILE\")\n",
        "    t = Path(\".devcontainer/.env.template\")\n",
        "    f = Path(\".devcontainer/.env\")\n",
        "    if t.exists() and not f.exists():\n",
        "        f.write_bytes(t.read_bytes())\n",
        "        print(\"✅ Created .env from template\")\n",
        "        return True\n",
        "    elif f.exists():\n",
        "        print(\"✅ .env file already exists\")\n",
        "        return True\n",
        "    else:\n",
        "        f.write_text(\n",
        "            \"\"\"ENV_NAME=docker_dev_template\n",
        "CUDA_TAG=12.4.0\n",
        "PYTHON_VER=3.10\n",
        "HOST_JUPYTER_PORT=8891\n",
        "HOST_TENSORBOARD_PORT=6008\n",
        "HOST_EXPLAINER_PORT=8050\n",
        "HOST_STREAMLIT_PORT=8501\n",
        "HOST_MLFLOW_PORT=5000\n",
        "\"\"\"\n",
        "        )\n",
        "        print(\"✅ Created minimal .env file\")\n",
        "        return True\n",
        "\n",
        "\n",
        "def fix_file_permissions() -> bool:\n",
        "    print_section(\"FIXING FILE PERMISSIONS\")\n",
        "    try:\n",
        "        is_wsl = \"microsoft\" in os.uname().release.lower()\n",
        "    except Exception:\n",
        "        is_wsl = False\n",
        "\n",
        "    if os.name == \"nt\" or is_wsl:\n",
        "        print(\"Detected Windows/WSL environment\")\n",
        "        for p in [\n",
        "            \".devcontainer/validate_gpu.py\",\n",
        "            \".devcontainer/tests/test_summary.py\",\n",
        "            \".devcontainer/tests/test_pytorch.py\",\n",
        "            \".devcontainer/tests/test_pytorch_gpu.py\",\n",
        "            \".devcontainer/tests/test_uv.py\",\n",
        "        ]:\n",
        "            fp = Path(p)\n",
        "            if fp.exists():\n",
        "                try:\n",
        "                    os.chmod(fp, 0o755)\n",
        "                    print(f\"✅ Fixed permissions for {p}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"⚠️ Could not fix permissions for {p}: {e}\")\n",
        "    return True\n",
        "\n",
        "\n",
        "def validate_docker_environment() -> bool:\n",
        "    print_section(\"VALIDATING DOCKER ENVIRONMENT\")\n",
        "    if not docker_available():\n",
        "        print(\"ℹ️ Docker CLI not found in this environment; skipping Docker checks.\")\n",
        "        return True  # treat as success inside containers\n",
        "    ok, out, err = run_command([\"docker\", \"info\"])\n",
        "    if not ok:\n",
        "        print(f\"❌ Docker daemon not accessible: {err}\")\n",
        "        return False\n",
        "    print(\"✅ Docker daemon is running\")\n",
        "\n",
        "    ok, out, err = run_command([\"docker\", \"compose\", \"version\"])\n",
        "    if not ok:\n",
        "        print(f\"❌ Docker Compose not available: {err}\")\n",
        "        return False\n",
        "    print(f\"✅ Docker Compose: {out.strip()}\")\n",
        "    return True\n",
        "\n",
        "\n",
        "def stop_and_remove_containers() -> bool:\n",
        "    print_section(\"CLEANING EXISTING CONTAINERS\")\n",
        "    if not docker_available():\n",
        "        print(\"ℹ️ Docker CLI not found; skipping container cleanup.\")\n",
        "        return True\n",
        "    ok, _, err = run_command(\n",
        "        [\"docker\", \"compose\", \"-f\", \".devcontainer/docker-compose.yml\", \"down\", \"--volumes\"]\n",
        "    )\n",
        "    if not ok:\n",
        "        print(f\"⚠️ Could not stop containers (may not exist): {err}\")\n",
        "    for name in [\"docker_dev_template_datascience\", \"docker_dev_template_mlflow\"]:\n",
        "        run_command([\"docker\", \"rm\", \"-f\", name])\n",
        "    print(\"✅ Container cleanup complete\")\n",
        "    return True\n",
        "\n",
        "\n",
        "def clean_docker_cache() -> bool:\n",
        "    print_section(\"CLEANING DOCKER CACHE\")\n",
        "    if not docker_available():\n",
        "        print(\"ℹ️ Docker CLI not found; skipping cache prune.\")\n",
        "        return True\n",
        "    ok, out, err = run_command([\"docker\", \"builder\", \"prune\", \"--all\", \"--force\"])\n",
        "    if ok:\n",
        "        print(\"✅ Docker build cache cleaned\")\n",
        "        if out:\n",
        "            print(out)\n",
        "        return True\n",
        "    print(f\"❌ Failed to clean Docker cache: {err}\")\n",
        "    return False\n",
        "\n",
        "\n",
        "def test_build() -> bool:\n",
        "    print_section(\"TESTING DOCKER BUILD\")\n",
        "    if not docker_available():\n",
        "        print(\"ℹ️ Docker CLI not found; skipping compose build test.\")\n",
        "        return True\n",
        "    if Path.cwd().name == \".devcontainer\":\n",
        "        os.chdir(\"..\")\n",
        "    compose_file = \".devcontainer/docker-compose.yml\"\n",
        "    print(f\"Using compose file: {Path(compose_file).absolute()}\")\n",
        "    print(f\"Build context: {Path('.').absolute()}\")\n",
        "    ok, out, err = run_command(\n",
        "        [\"docker\", \"compose\", \"-f\", compose_file, \"build\", \"--no-cache\"], timeout=600\n",
        "    )\n",
        "    if ok:\n",
        "        print(\"✅ Docker build successful!\")\n",
        "        print(\"\\n\".join(out.splitlines()[-10:]))\n",
        "        return True\n",
        "    print(\"❌ Docker build failed\")\n",
        "    print(\"STDERR:\\n\", err)\n",
        "    print(\"STDOUT (last 20 lines):\\n\", \"\\n\".join(out.splitlines()[-20:]))\n",
        "    return False\n",
        "\n",
        "\n",
        "def section_summary(struct_ok, uv_ok, pt_ok, jax_ok):\n",
        "    print_section(\"SUMMARY\")\n",
        "    print(f\"structure: {struct_ok} uv: {uv_ok} pytorch: {pt_ok} jax: {jax_ok}\")\n",
        "\n",
        "\n",
        "def test_uv() -> bool:\n",
        "    print_section(\"UV\")\n",
        "    ok, out, err = run_command([\"uv\", \"--version\"])\n",
        "    print((out or err).strip() or \"uv not in PATH\")\n",
        "    return ok\n",
        "\n",
        "\n",
        "def test_pytorch() -> bool:\n",
        "    print_section(\"PYTORCH\")\n",
        "    try:\n",
        "        import torch\n",
        "        print(\"version:\", torch.__version__)\n",
        "        print(\"cuda:\", torch.cuda.is_available())\n",
        "        if torch.cuda.is_available():\n",
        "            d = torch.device(\"cuda:0\")\n",
        "            import time\n",
        "            x = torch.randn((512, 512), device=d)\n",
        "            t0 = time.time()\n",
        "            y = (x @ x.T).sum()\n",
        "            torch.cuda.synchronize()\n",
        "            print(\"sum:\", float(y))\n",
        "            print(f\"gpu op ms: {(time.time() - t0)*1000:.2f}\")\n",
        "            return True\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(\"error:\", e)\n",
        "        return False\n",
        "\n",
        "\n",
        "def test_jax() -> bool:\n",
        "    print_section(\"JAX\")\n",
        "    try:\n",
        "        import jax\n",
        "        import jax.numpy as jnp\n",
        "\n",
        "        devs = jax.devices()\n",
        "        print(\"devices:\", devs)\n",
        "        gpus = jax.devices(\"gpu\") or [\n",
        "            d for d in devs\n",
        "            if getattr(d, \"platform\", \"\").lower() in {\"gpu\", \"cuda\"} or \"cuda\" in str(d).lower()\n",
        "        ]\n",
        "        if not gpus:\n",
        "            print(\"no gpu devices detected by jax\")\n",
        "            return False\n",
        "        x = jnp.ones((512, 512), dtype=jnp.float32)\n",
        "        x = jax.device_put(x, gpus[0])\n",
        "        s = jnp.sum(x).block_until_ready()\n",
        "        print(\"sum:\", float(s))\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(\"error:\", e)\n",
        "        return False\n",
        "\n",
        "\n",
        "def parse_args() -> argparse.Namespace:\n",
        "    p = argparse.ArgumentParser()\n",
        "    p.add_argument(\"--quick\", action=\"store_true\",\n",
        "                   help=\"Skip Docker checks; run only structure/UV/Torch/JAX\")\n",
        "    return p.parse_args()\n",
        "\n",
        "\n",
        "def main() -> int:\n",
        "    args = parse_args()\n",
        "    print(\"Docker DevContainer Build & GPU Validation\")\n",
        "    print(f\"Working directory: {os.getcwd()}\")\n",
        "\n",
        "    # Always run these\n",
        "    struct_ok = ensure_project_structure()\n",
        "    env_ok = create_env_file()\n",
        "    perm_ok = fix_file_permissions()\n",
        "\n",
        "    # Optional Docker checks\n",
        "    docker_ok = True\n",
        "    build_ok = True\n",
        "    cache_ok = True\n",
        "    stop_ok = True\n",
        "\n",
        "    if not args.quick:\n",
        "        docker_ok = validate_docker_environment()\n",
        "        stop_ok = stop_and_remove_containers()\n",
        "        cache_ok = clean_docker_cache()\n",
        "        build_ok = test_build()\n",
        "\n",
        "    uv_ok = test_uv()\n",
        "    pt_ok = test_pytorch()\n",
        "    jax_ok = test_jax()\n",
        "\n",
        "    section_summary(struct_ok, uv_ok, pt_ok, jax_ok)\n",
        "\n",
        "    # In quick mode, ignore Docker results entirely.\n",
        "    if args.quick:\n",
        "        return 0 if all([struct_ok, uv_ok, pt_ok, jax_ok]) else 1\n",
        "\n",
        "    # Otherwise include Docker outcomes.\n",
        "    ok = all([\n",
        "        struct_ok, env_ok, perm_ok,\n",
        "        docker_ok, stop_ok, cache_ok, build_ok,\n",
        "        uv_ok, pt_ok, jax_ok\n",
        "    ])\n",
        "    return 0 if ok else 1\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    sys.exit(main())\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## BuildKit Cache Fix & Testing\n",
        "\n",
        "The Dockerfile has been updated with isolated BuildKit cache mounts to prevent \n",
        "`archive/tar: invalid tar header` errors. If you encounter build issues:\n",
        "\n",
        "**Quick Fix:**\n",
        "```bash\n",
        "python scripts/fix_build.py\n",
        "```\n",
        "\n",
        "**Manual Fix Steps:**\n",
        "1. Clear corrupted caches: `docker builder prune --all --force`\n",
        "2. Clean build: `docker-compose build --no-cache`\n",
        "3. Test cached build: `docker-compose build`\n",
        "\n",
        "**Key Improvements:**\n",
        "- Added explicit cache IDs: `apt-cache`, `apt-lists`, `uv-cache`\n",
        "- Prevents cache key collisions and corruption\n",
        "- Maintains fast build times with reliable caching\n",
        "\n",
        "## Testing & Diagnostics\n",
        "\n",
        "Creating comprehensive testing and diagnostic scripts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ../../.devcontainer/tests/test_pytorch_gpu.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile ../../.devcontainer/tests/test_pytorch_gpu.py\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"Small PyTorch GPU benchmark.\"\"\"\n",
        "import time\n",
        "\n",
        "\n",
        "def test_pytorch(force_cpu: bool = False) -> None:\n",
        "    import torch\n",
        "    cuda_ok = torch.cuda.is_available() and not force_cpu\n",
        "    if cuda_ok:\n",
        "        name = torch.cuda.get_device_name(0)\n",
        "        major, minor = torch.cuda.get_device_capability()\n",
        "        print(f\"device: {name} (sm_{major}{minor:02d})\")\n",
        "        device = torch.device(\"cuda:0\")\n",
        "    else:\n",
        "        print(\"falling back to cpu\")\n",
        "        device = torch.device(\"cpu\")\n",
        "\n",
        "    size = (1000, 1000)\n",
        "    a, b = (torch.randn(size, device=device) for _ in range(2))\n",
        "    _ = a @ b\n",
        "    t0 = time.time()\n",
        "    _ = (a @ b).sum().item()\n",
        "    if device.type == \"cuda\":\n",
        "        torch.cuda.synchronize()\n",
        "    print(f\"matmul on {device} took {(time.time()-t0)*1000:.2f} ms\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_pytorch()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Verification\n",
        "\n",
        "Now let's test that our environment setup is working correctly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ../../.devcontainer/tests/test_uv.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile ../../.devcontainer/tests/test_uv.py\n",
        "\"\"\"UV and key package presence check.\"\"\"\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "print(\"UV version:\")\n",
        "try:\n",
        "    r = subprocess.run([\"uv\", \"--version\"], capture_output=True, text=True)\n",
        "    print(r.stdout.strip() or r.stderr.strip())\n",
        "except FileNotFoundError:\n",
        "    print(\"uv not found\")\n",
        "\n",
        "print(\"\\nPython:\")\n",
        "print(sys.executable)\n",
        "print(sys.version)\n",
        "\n",
        "print(\"\\nKey packages:\")\n",
        "for pkg in [\"numpy\", \"pandas\", \"matplotlib\", \"scipy\", \"sklearn\", \"jupyterlab\", \"seaborn\", \"tqdm\"]:\n",
        "    try:\n",
        "        if pkg == \"sklearn\":\n",
        "            import sklearn as m\n",
        "        else:\n",
        "            m = __import__(pkg)\n",
        "        print(pkg, getattr(m, \"__version__\", \"unknown\"))\n",
        "    except Exception as e:\n",
        "        print(pkg, \"missing or error:\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ../../.devcontainer/tests/test_pytorch.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile ../../.devcontainer/tests/test_pytorch.py\n",
        "print(\"PyTorch quick check\")\n",
        "try:\n",
        "    import torch\n",
        "    print(\"version:\", torch.__version__)\n",
        "    print(\"cuda:\", torch.cuda.is_available())\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"devices:\", torch.cuda.device_count())\n",
        "        for i in range(torch.cuda.device_count()):\n",
        "            print(i, torch.cuda.get_device_name(i))\n",
        "        x = torch.ones(100, 100, device='cuda:0')\n",
        "        print(\"sum:\", float(torch.sum(x)))\n",
        "except Exception as e:\n",
        "    print(\"error:\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ../../.devcontainer/tests/test_uv.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile ../../.devcontainer/tests/test_uv.py\n",
        "# Test other critical packages\n",
        "print(\"\\n📦 Testing other critical packages...\")\n",
        "\n",
        "packages_to_test = [\n",
        "    'numpy', 'pandas', 'matplotlib', 'scipy', 'sklearn', \n",
        "    'jupyterlab', 'seaborn', 'tqdm'\n",
        "]\n",
        "\n",
        "for package in packages_to_test:\n",
        "    try:\n",
        "        if package == 'sklearn':\n",
        "            import sklearn\n",
        "            version = sklearn.__version__\n",
        "        else:\n",
        "            module = __import__(package)\n",
        "            version = getattr(module, '__version__', 'unknown')\n",
        "        print(f\"   ✅ {package}: {version}\")\n",
        "    except ImportError:\n",
        "        print(f\"   ❌ {package}: Not installed\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ⚠️  {package}: Error - {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ../../.devcontainer/tests/test_summary.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile ../../.devcontainer/tests/test_summary.py\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"Aggregated checks for the devcontainer layout and GPU readiness.\"\"\"\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import subprocess\n",
        "\n",
        "\n",
        "def section(t):\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(t)\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "\n",
        "def test_structure() -> bool:\n",
        "    section(\"STRUCTURE\")\n",
        "    expected = [\n",
        "        '/workspace/.devcontainer/docker-compose.yml',  # MOVED FROM ROOT\n",
        "        '/workspace/pyproject.toml',\n",
        "        '/workspace/.devcontainer/devcontainer.json',\n",
        "        '/workspace/.devcontainer/Dockerfile',\n",
        "        '/workspace/.devcontainer/.env.template',\n",
        "        '/workspace/.devcontainer/.dockerignore',\n",
        "        '/app/validate_gpu.py',\n",
        "        '/app/tests/'\n",
        "    ]\n",
        "    ok = True\n",
        "    for p in expected:\n",
        "        if os.path.exists(p):\n",
        "            print(\"ok:\", p)\n",
        "        else:\n",
        "            print(\"missing:\", p)\n",
        "            ok = False\n",
        "    return ok\n",
        "\n",
        "\n",
        "def test_uv() -> bool:\n",
        "    section(\"UV\")\n",
        "    try:\n",
        "        r = subprocess.run(['uv', '--version'], capture_output=True, text=True)\n",
        "        print(r.stdout.strip() or r.stderr.strip())\n",
        "        return r.returncode == 0\n",
        "    except FileNotFoundError:\n",
        "        print('uv not in PATH')\n",
        "        return False\n",
        "\n",
        "\n",
        "def test_pytorch() -> bool:\n",
        "    section(\"PYTORCH\")\n",
        "    try:\n",
        "        import torch\n",
        "        print(\"version:\", torch.__version__)\n",
        "        print(\"cuda:\", torch.cuda.is_available())\n",
        "        if torch.cuda.is_available():\n",
        "            d = torch.device('cuda:0')\n",
        "            x = torch.ones(512, 512, device=d)\n",
        "            y = torch.sum(x)\n",
        "            print(\"sum:\", y.item())\n",
        "            return True\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(\"error:\", e)\n",
        "        return False\n",
        "\n",
        "\n",
        "def test_jax() -> bool:\n",
        "    section(\"JAX\")\n",
        "    try:\n",
        "        import jax, jax.numpy as jnp\n",
        "\n",
        "        # Show all devices for visibility\n",
        "        devs = jax.devices()\n",
        "        print(\"devices:\", devs)\n",
        "\n",
        "        # Prefer the supported filtered query\n",
        "        gpus = jax.devices(\"gpu\")\n",
        "\n",
        "        # Fallback for older/newer renderings (e.g., \"CudaDevice(id=0)\")\n",
        "        if not gpus:\n",
        "            gpus = [\n",
        "                d for d in devs\n",
        "                if getattr(d, \"platform\", \"\").lower() in {\"gpu\", \"cuda\"} or \"cuda\" in str(d).lower()\n",
        "            ]\n",
        "\n",
        "        if not gpus:\n",
        "            print(\"no gpu devices detected by jax\")\n",
        "            return False\n",
        "\n",
        "        # Tiny compute on the first GPU to ensure execution\n",
        "        x = jnp.ones((512, 512), dtype=jnp.float32)\n",
        "        x = jax.device_put(x, gpus[0])\n",
        "        s = jnp.sum(x).block_until_ready()\n",
        "        print(\"sum:\", float(s))\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(\"error:\", e)\n",
        "        return False\n",
        "\n",
        "\n",
        "\n",
        "def main() -> int:\n",
        "    s_ok = test_structure()\n",
        "    uv_ok = test_uv()\n",
        "    pt_ok = test_pytorch()\n",
        "    j_ok = test_jax()\n",
        "\n",
        "    section(\"SUMMARY\")\n",
        "    print(\"structure:\", s_ok, \"uv:\", uv_ok, \"pytorch:\", pt_ok, \"jax:\", j_ok)\n",
        "    return 0 if all([s_ok, uv_ok, pt_ok, j_ok]) else 1\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    sys.exit(main())\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
