{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de854878",
   "metadata": {},
   "source": [
    " ## Project 1\n",
    " **Question:**  \n",
    " You are tasked by the scouting department with creating a machine learning model that predicts which NCAA players will become good 3PT shooters in the NBA. How would you approach this problem? Be sure to explain what datasets you would want to use in your answer.\n",
    "\n",
    " **Answer:**\n",
    "\n",
    " ### Problem Definition\n",
    " The goal is to identify which NCAA players are most likely to become reliable NBA shooters—defined here as hitting **≥36% from three on meaningful volume (250+ attempts) in their first three NBA seasons**. This gives scouts a forward-looking tool to prioritize prospects.\n",
    "\n",
    " ### Data Sources\n",
    " - **Primary:** NCAA box scores and season splits from Sports-Reference; NBA outcomes from the NBA Stats API and Basketball-Reference.\n",
    " - **Premium (if available):** Synergy Sports for shot-type context (catch-and-shoot vs. off-dribble, contested vs. uncontested).\n",
    " - **Linking:** Draft records to connect each NCAA player’s college profile to their NBA career.\n",
    "\n",
    " ### Key Features\n",
    " - **3PA rate:** Shows willingness to take threes.\n",
    " - **FT%:** A reliable indicator of shooting touch.\n",
    " - **Age on Draft Night:** Aligns with NBA development curves; younger players with the same stats usually have more upside.\n",
    " - **EB-smoothed NCAA 3P%:** Adjusts raw makes/attempts by blending them with league averages, so small samples aren’t over- or under-weighted.\n",
    " - **Shot selection & role metrics:** Synergy adds insight into whether a player’s threes come in translatable contexts (spot-ups vs. forced off-dribble).\n",
    "\n",
    " ### Controls & Adjustments\n",
    " - Add a regime flag for NCAA 3P% (pre- vs. post-2019 line move) to account for different difficulty environments.\n",
    " - Normalize by pace and possessions so players on faster teams aren’t inflated.\n",
    " - Recognize positional context: guards are expected to take higher-volume threes, while bigs may show value even at lower volume.\n",
    "\n",
    " ### Modeling Approach\n",
    " - **Y variable:** An Empirical Bayes estimate of NBA 3P% in Years 1–3 (shrunk toward league average for stability).\n",
    " - **Features:** NCAA data flows into a model (regularized regression or gradient boosted trees). The model outputs:\n",
    "     1. A probability the player will be at or above NBA league-average from three on real volume.\n",
    "     2. A stabilized NBA 3P% projection with uncertainty bands.\n",
    "\n",
    " ### Why This Works\n",
    " Empirical Bayes on both sides (college and NBA) prevents us from overrating noisy samples. The regime flag ensures fair comparisons across eras. The features (3PA rate, FT%, role, and age) are exactly the indicators that scouts already care about—the model simply quantifies them against 10+ years of draft history. This way, we aren’t replacing the eye test, just giving scouts an evidence-based lens to spot hidden value or validate what they already see on film.\n",
    "\n",
    " ---\n",
    "\n",
    " ## Project 2\n",
    " **Question:**  \n",
    " You’ve built the machine learning model from question 1, and now a scout wants help understanding it. Explain how machine learning models would work in this specific context in one or two paragraphs, using simple language.\n",
    "\n",
    " **Answer:**\n",
    "\n",
    " Think of this like having a very experienced scout who has tracked every draft class since 2010. The model looks at hundreds of players who went from college to the NBA and learns the patterns: younger players with solid free throw touch and high three-point volume usually develop into strong NBA shooters, while older players with similar stats tend to plateau.\n",
    "\n",
    " For any new prospect, we feed in their college profile—3PA rate, FT%, age, role, and their EB-adjusted NCAA 3P% (which smooths out hot streaks or cold stretches). The model compares them to historical players and gives two outputs: (1) the probability that they’ll become an above-average NBA shooter in their first three years, and (2) a stabilized projection of their NBA 3P%. We also highlight the “why” in plain terms—for example: “19 years old, high volume, strong FT%—looks similar to Player X at the same stage.”\n",
    "\n",
    " This doesn’t replace your eye test. Instead, it adds another layer: it can uncover overlooked shooters or flag concerns about hyped players whose stats don’t usually translate. And because the outcome variable is an EB-smoothed NBA 3P% over Y1–Y3, it reflects true shooting skill instead of just early streaks or slumps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfba4ac8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 3\n",
    "\n",
    "**Prompt:**  \n",
    "Please download the NBA Dataset available for free here. Using only the data provided in that dataset, rate the 10 best, 10 most average, and 10 worst individual player seasons. Only include the regular season, use players with at least 500 minutes played in that specific season, and only go back as far as the 2010-11 season. You may define and calculate these metrics in any way you wish.\n",
    "\n",
    "List the results of your code for all 3 rankings. Only ranks, player names, and seasons are needed for each set. *This question is required.*\n",
    "\n",
    "---\n",
    "\n",
    "### Answer\n",
    "\n",
    "This analysis evaluates individual NBA player seasons from 2010-11 onwards using a dual-metric approach to capture both all-around impact and per-minute efficiency. I developed rankings using PIE (Player Impact Estimate) and Game Score per 36 minutes, then built predictive models to forecast 2025-26 season performance.\n",
    "\n",
    "**Key Findings:**\n",
    "- **Top performers:** LeBron James dominates PIE rankings (2010-2014), while modern big men lead Game Score/36 (Embiid 2023-24, Jokic 2024-25, Giannis 2019-20)\n",
    "- **Predictive models:** Achieved 85.2% R² for PIE prediction and 70.7% R² for Game Score/36\n",
    "- **2025-26 Projections:** Embiid, Jokic, and Giannis expected to lead both metrics\n",
    "\n",
    "---\n",
    "\n",
    "## Results\n",
    "\n",
    "### PIE Rankings – All-Around Impact Leaders\n",
    "\n",
    "#### Top 10 Seasons\n",
    "\n",
    "| Rank | Player            | Season   |\n",
    "|------|-------------------|----------|\n",
    "| 1    | LeBron James      | 2012-13  |\n",
    "| 2    | LeBron James      | 2011-12  |\n",
    "| 3    | Russell Westbrook | 2016-17  |\n",
    "| 4    | Kevin Durant      | 2013-14  |\n",
    "| 5    | Nikola Jokic      | 2021-22  |\n",
    "| 6    | Nikola Jokic      | 2024-25  |\n",
    "| 7    | Kevin Durant      | 2012-13  |\n",
    "| 8    | Joel Embiid       | 2023-24  |\n",
    "| 9    | LeBron James      | 2010-11  |\n",
    "| 10   | LeBron James      | 2013-14  |\n",
    "\n",
    "#### Middle 10 Seasons (Around Median)\n",
    "\n",
    "| Rank | Player             | Season   |\n",
    "|------|--------------------|----------|\n",
    "| 1    | Patrick Beverley   | 2018-19  |\n",
    "| 2    | Bub Carrington     | 2024-25  |\n",
    "| 3    | Omer Asik          | 2013-14  |\n",
    "| 4    | J.J. Barea         | 2010-11  |\n",
    "| 5    | Carmelo Anthony    | 2020-21  |\n",
    "| 6    | Earl Clark         | 2012-13  |\n",
    "| 7    | Cory Joseph        | 2015-16  |\n",
    "| 8    | Jaxson Hayes       | 2021-22  |\n",
    "| 9    | Quentin Grimes     | 2022-23  |\n",
    "| 10   | Steve Novak        | 2011-12  |\n",
    "\n",
    "#### Bottom 10 Seasons\n",
    "\n",
    "| Rank | Player             | Season   |\n",
    "|------|--------------------|----------|\n",
    "| 1    | Ronnie Price       | 2010-11  |\n",
    "| 2    | Gary Harris        | 2014-15  |\n",
    "| 3    | Terrance Ferguson  | 2019-20  |\n",
    "| 4    | Kevin Seraphin     | 2010-11  |\n",
    "| 5    | Ryan Hollins       | 2011-12  |\n",
    "| 6    | Jason Collins      | 2010-11  |\n",
    "| 7    | Doron Lamb         | 2012-13  |\n",
    "| 8    | Rashad Vaughn      | 2015-16  |\n",
    "| 9    | Terrance Ferguson  | 2017-18  |\n",
    "| 10   | John Lucas III     | 2013-14  |\n",
    "\n",
    "---\n",
    "\n",
    "### Game Score per 36 Rankings – Per-Minute Efficiency Leaders\n",
    "\n",
    "#### Top 10 Seasons\n",
    "\n",
    "| Rank | Player                  | Season   |\n",
    "|------|-------------------------|----------|\n",
    "| 1    | Joel Embiid             | 2023-24  |\n",
    "| 2    | Nikola Jokic            | 2024-25  |\n",
    "| 3    | Giannis Antetokounmpo   | 2019-20  |\n",
    "| 4    | Nikola Jokic            | 2021-22  |\n",
    "| 5    | Shai Gilgeous-Alexander | 2024-25  |\n",
    "| 6    | Giannis Antetokounmpo   | 2021-22  |\n",
    "| 7    | Joel Embiid             | 2022-23  |\n",
    "| 8    | Nikola Jokic            | 2022-23  |\n",
    "| 9    | Giannis Antetokounmpo   | 2023-24  |\n",
    "| 10   | Nikola Jokic            | 2023-24  |\n",
    "\n",
    "#### Middle 10 Seasons (Around Median)\n",
    "\n",
    "| Rank | Player                | Season   |\n",
    "|------|-----------------------|----------|\n",
    "| 1    | Jabari Walker         | 2023-24  |\n",
    "| 2    | Roy Hibbert           | 2014-15  |\n",
    "| 3    | Max Strus             | 2024-25  |\n",
    "| 4    | Sam Hauser            | 2023-24  |\n",
    "| 5    | Julian Champagnie     | 2024-25  |\n",
    "| 6    | Justise Winslow       | 2021-22  |\n",
    "| 7    | Naji Marshall         | 2023-24  |\n",
    "| 8    | Shabazz Napier        | 2017-18  |\n",
    "| 9    | Terance Mann          | 2021-22  |\n",
    "| 10   | Michael Carter-Williams | 2014-15 |\n",
    "\n",
    "#### Bottom 10 Seasons\n",
    "\n",
    "| Rank | Player             | Season   |\n",
    "|------|--------------------|----------|\n",
    "| 1    | DeShawn Stevenson  | 2011-12  |\n",
    "| 2    | Terrance Ferguson  | 2019-20  |\n",
    "| 3    | Stephen Graham     | 2010-11  |\n",
    "| 4    | Mike Miller        | 2014-15  |\n",
    "| 5    | Shawne Williams    | 2011-12  |\n",
    "| 6    | Rashad Vaughn      | 2015-16  |\n",
    "| 7    | Semi Ojeleye       | 2017-18  |\n",
    "| 8    | Jason Collins      | 2010-11  |\n",
    "| 9    | Doron Lamb         | 2012-13  |\n",
    "| 10   | Anthony Brown      | 2015-16  |\n",
    "\n",
    "---\n",
    "\n",
    "## Question 4\n",
    "\n",
    "**Prompt:**  \n",
    "Describe how you calculated these rankings and why you chose that approach. *This question is required.*\n",
    "\n",
    "---\n",
    "\n",
    "### Answer 4\n",
    "\n",
    "#### Methodology & Rationale\n",
    "\n",
    "**Why These Two Metrics?**  \n",
    "I chose a complementary pair that captures different aspects of basketball excellence:\n",
    "\n",
    "1. **PIE (Player Impact Estimate)**\n",
    "   - Measures share of team events while on court\n",
    "   - Captures all-around, high-usage seasons\n",
    "   - Formula: (Player's positive contributions) / (Total game events from both teams)\n",
    "   - For each game, PIE is calculated as a player’s contributions divided by the combined contributions of both teams, so all players’ PIE in a game sum to 1. A season PIE is then the weighted average of these game values, where weights are the game totals.\n",
    "   - Best for identifying star-level, ball-dominant impact\n",
    "\n",
    "2. **Game Score per 36 Minutes**\n",
    "   - Hollinger's efficiency metric scaled to per-36 minutes\n",
    "   - Rewards scoring, rebounds, assists; penalizes missed shots and turnovers\n",
    "   - Formula: Points + 0.4×FGM - 0.7×FGA - 0.4×(FTA-FTM) + 0.7×OREB + 0.3×DREB + STL + 0.7×AST + 0.7×BLK - 0.4×PF - TOV\n",
    "   - Best for per-minute production regardless of playing time\n",
    "\n",
    "**Data Filtering & Preprocessing**\n",
    "\n",
    "- **Filters Applied:**\n",
    "  - Seasons: 2010-11 through 2024-25\n",
    "  - Minutes: Minimum 500 minutes played\n",
    "  - Regular season only\n",
    "  - Final dataset: 4,423 rows\n",
    "\n",
    "- **Tiebreakers:**\n",
    "  1. Primary metric (PIE or Game Score/36)\n",
    "  2. season_pie DESC\n",
    "  3. total_minutes DESC\n",
    "  4. ts_pct DESC\n",
    "\n",
    "- **Side Notes:**\n",
    "  - For ‘most average,’ I defined it as the 10 player-seasons closest to the median value of the distribution, since NBA performance data is skewed by extreme outliers. This captures the truly ‘middle’ players rather than being pulled upward by superstars.\n",
    "\n",
    "---\n",
    "\n",
    "## Appendix: Technical Implementation\n",
    "\n",
    "### Predictive Analytics – 2025–26 Season Forecasts\n",
    "\n",
    "#### Machine Learning Pipeline\n",
    "\n",
    "I completed the main rankings quickly enough to use the remaining time to build forecasts for the 2025–26 season. I developed a simple prediction system using lagged features to project both PIE and Game Score per 36.\n",
    "\n",
    "- **Key Features:**\n",
    "  - 39 engineered features including advanced metrics, usage rates, and performance consistency\n",
    "  - Lag-based approach using prior season performance as primary predictors\n",
    "  - Feature importance filtering to prevent overfitting\n",
    "  - Random Forest models optimized for each target metric\n",
    "\n",
    "- **Model Performance:**\n",
    "  - PIE Prediction Model: R² = 0.852, RMSE = 0.011\n",
    "    - Top predictors: Previous PIE (1.063), Production/36 lag (0.003), Free throw attempts lag (0.002)\n",
    "  - Game Score/36 Prediction Model: R² = 0.707, RMSE = 2.161\n",
    "    - Top predictors: Production/36 lag (0.809), Offensive impact lag (0.058), Two-way impact lag (0.007)\n",
    "\n",
    "#### 2025–26 Projections\n",
    "\n",
    "**PIE Leaderboard (Projected)**\n",
    "\n",
    "| Rank | Player                  | Predicted PIE |\n",
    "|------|-------------------------|--------------|\n",
    "| 1    | Joel Embiid             | 0.1482       |\n",
    "| 2    | Nikola Jokic            | 0.1474       |\n",
    "| 3    | Giannis Antetokounmpo   | 0.1432       |\n",
    "| 4    | Shai Gilgeous-Alexander | 0.1399       |\n",
    "| 5    | Luka Doncic             | 0.1396       |\n",
    "\n",
    "**Game Score/36 Leaderboard (Projected)**\n",
    "\n",
    "| Rank | Player                | Predicted GS/36 |\n",
    "|------|-----------------------|-----------------|\n",
    "| 1    | Giannis Antetokounmpo | 27.68           |\n",
    "| 2    | Nikola Jokic          | 27.36           |\n",
    "| 3    | Joel Embiid           | 25.37           |\n",
    "| 4    | Anthony Davis         | 24.91           |\n",
    "| 5    | Luka Doncic           | 24.02           |\n",
    "\n",
    "#### Key Insights\n",
    "\n",
    "- **Historical Patterns:**\n",
    "  - PIE leaders = high-usage stars in prime (LeBron 2009–2013, Westbrook’s triple-double season).\n",
    "  - Game Score/36 leaders = modern bigs (Jokic, Embiid, Giannis).\n",
    "  - Divergence shows two routes to excellence: all-around dominance vs. efficient per-minute scoring.\n",
    "- **Predictive Insights:**\n",
    "  - Previous season performance is the strongest predictor.\n",
    "  - PIE persistence is higher (R² = 0.85) → stable measure.\n",
    "  - Game Score/36 is more volatile due to injuries and role changes.\n",
    "\n",
    "#### Future Production Enhancements\n",
    "\n",
    "- **Infrastructure & Orchestration**\n",
    "  - Apache Airflow for automated retraining\n",
    "  - Apache Kafka for real-time streaming\n",
    "  - MLflow for experiment tracking\n",
    "  - FastAPI server for real-time predictions\n",
    "\n",
    "- **Advanced Analytics & Features**\n",
    "  - Integrate PER, VORP, EWA\n",
    "  - SHAP/LIME for explainability\n",
    "  - Recursive Feature Elimination (RFE)\n",
    "  - Bayesian hierarchical models for pooling\n",
    "\n",
    "- **Model Enhancement**\n",
    "  - Position/team/league-level predictions\n",
    "  - Uncertainty quantification with intervals\n",
    "  - Real-time updating\n",
    "  - Ensembles combining multiple models\n",
    "\n",
    "- **User Experience**\n",
    "  - React/Vite dashboards\n",
    "  - Predicted vs. historical side-by-side\n",
    "  - Custom metric builders for scouts\n",
    "  - Mobile-friendly delivery\n",
    "\n",
    "- **Production Monitoring**\n",
    "  - Model drift detection\n",
    "  - A/B testing framework\n",
    "  - Performance dashboards\n",
    "  - Automated alerts on performance changes\n",
    "\n",
    "- **Code Structure**\n",
    "  1. `data_loader.py` – preprocessing and filtering\n",
    "  2. `feature_engineering.py` – advanced + lagged metrics\n",
    "  3. `ml_pipeline.py` – Random Forest training, feature filtering\n",
    "  4. `prediction_engine.py` – 2025–26 forecasting\n",
    "  5. `ranking_system.py` – Top/Middle/Bottom 10 with tiebreakers\n",
    "\n",
    "- **Feature Engineering:**\n",
    "  - Advanced metrics: TS%, eFG%, usage, ratings\n",
    "  - Consistency: variance of game scores\n",
    "  - Context: minutes tier, team win%, season experience\n",
    "  - Lag features: prior-season performance\n",
    "\n",
    "- **Model Architecture:**\n",
    "  - Random Forest Regressor, 100 estimators\n",
    "  - Permutation importance, 10-fold CV\n",
    "  - Minimum threshold = 0.001\n",
    "  - Separate models per metric\n",
    "\n",
    "- **Data Quality Assurance:**\n",
    "  - Lag validation across seasons\n",
    "  - Missing data handled with complete cases\n",
    "  - Outlier validation\n",
    "  - Reproducible with fixed seeds/versioning\n",
    "\n",
    "---\n",
    "\n",
    "## Detailed Rankings (With Context)\n",
    "\n",
    "### PIE\n",
    "\n",
    "#### Top 10 Seasons by PIE (with context)\n",
    "\n",
    "| Rank | Player Name      | Season   | PIE      | PIE%  | PIE Num | PIE Den   | Points | FGM | FTM | FGA | FTA | DREB | OREB | AST | STL | BLK | PF  | TOV | Minutes  |\n",
    "|------|------------------|----------|----------|-------|---------|-----------|--------|-----|-----|-----|-----|------|------|-----|-----|-----|-----|-----|----------|\n",
    "| 1    | LeBron James     | 2012-13  | 0.174857 | 17.49 | 2254.0  | 12890.5   | 2036.0 | 765 | 403 |1354 | 535 | 513  | 97   | 551 |129  | 67  |110  |226  | 2835.00  |\n",
    "| 2    | LeBron James     | 2011-12  | 0.174495 | 17.45 | 1683.0  | 9645.0    | 1683.0 | 621 | 387 |1169 | 502 | 398  | 94   | 387 |115  | 50  | 96  |213  | 2297.00  |\n",
    "| 3    | Russell Westbrook| 2016-17  | 0.171554 | 17.16 | 2466.0  | 14374.5   | 2558.0 | 824 | 710 |1941 | 840 | 727  |137   | 840 |132  | 31  |190  |438  | 2761.00  |\n",
    "| 4    | Kevin Durant     | 2013-14  | 0.169186 | 16.92 | 2339.5  | 13828.0   | 2593.0 | 849 | 703 |1688 | 805 | 540  | 58   | 445 |103  | 59  |174  |285  | 3087.00  |\n",
    "| 5    | Nikola Jokic     | 2021-22  | 0.166170 | 16.62 | 2536.5  | 15264.5   | 2004.0 | 764 | 379 |1311 | 468 | 813  |206   | 584 |109  | 63  |191  |281  | 2440.00  |\n",
    "| 6    | Nikola Jokic     | 2024-25  | 0.163343 | 16.33 | 2670.5  | 16349.0   | 2071.0 | 786 | 361 |1364 | 451 | 692  |200   | 716 |127  | 45  |160  |230  | 2557.28  |\n",
    "| 7    | Kevin Durant     | 2012-13  | 0.162572 | 16.26 | 2243.5  | 13800.0   | 2280.0 | 731 | 679 |1433 | 750 | 594  | 46   | 374 |116  |105  |143  |280  | 3076.00  |\n",
    "| 8    | Joel Embiid      | 2023-24  | 0.162201 | 16.22 | 1195.5  | 7370.5    | 1217.0 | 412 | 342 | 768 | 389 | 302  | 80   | 197 | 39  | 59  | 96  |130  | 1140.00  |\n",
    "| 9    | LeBron James     | 2010-11  | 0.161252 | 16.13 | 2030.0  | 12589.0   | 2111.0 | 758 | 503 |1485 | 663 | 510  | 80   | 554 |124  | 50  |163  |284  | 3027.00  |\n",
    "| 10   | LeBron James     | 2013-14  | 0.158951 | 15.90 | 2075.5  | 13057.5   | 2089.0 | 767 | 439 |1353 | 585 | 452  | 81   | 488 |121  | 26  |126  |270  | 2863.00  |\n",
    "\n",
    "#### Middle 10 Seasons by PIE (with context)\n",
    "\n",
    "| Rank | Player Name        | Season   | PIE      | PIE%  | PIE Num | PIE Den   | Points | FGM | FTM | FGA | FTA | DREB | OREB | AST | STL | BLK | PF  | TOV | Minutes  |\n",
    "|------|--------------------|----------|----------|-------|---------|-----------|--------|-----|-----|-----|-----|------|------|-----|-----|-----|-----|-----|----------|\n",
    "| 1    | Patrick Beverley   | 2018-19  | 0.044128 | 4.41  | 674.5   | 15285.0   | 596.0  | 194 | 96  | 477 | 123 | 312  | 76   | 300 | 67  | 43  |265  | 85  | 2097.00  |\n",
    "| 2    | Bub Carrington     | 2024-25  | 0.044140 | 4.41  | 751.5   | 17025.5   | 801.0  | 297 | 69  | 743 | 85  | 303  | 33   | 360 | 53  | 20  |190  |140  | 2419.13  |\n",
    "| 3    | Omer Asik          | 2013-14  | 0.044143 | 4.41  | 377.0   | 8540.5    | 280.0  | 101 | 78  | 190 |126  | 277  |101   | 25  | 14  | 37  | 92  | 59  | 947.00   |\n",
    "| 4    | J.J. Barea         | 2010-11  | 0.044121 | 4.41  | 601.0   | 13621.5   | 769.0  | 285 |133  | 649 |157  | 130  | 29   | 317 | 30  |  1  |136  |136  | 1631.00  |\n",
    "| 5    | Carmelo Anthony    | 2020-21  | 0.044149 | 4.41  | 699.5   | 15844.0   |1035.0  | 367 |155  | 870 |176  | 221  | 41   | 110 | 53  | 42  |168  | 69  | 1894.00  |\n",
    "| 6    | Earl Clark         | 2012-13  | 0.044116 | 4.41  | 430.0   | 9747.0    | 426.0  | 170 | 51  | 386 | 74  | 242  | 82   | 65  | 36  | 44  |102  | 61  | 1332.00  |\n",
    "| 7    | Cory Joseph        | 2015-16  | 0.044110 | 4.41  | 588.5   | 13341.5   | 677.0  | 257 |133  | 585 |174  | 171  | 39   | 250 | 63  | 20  |131  |102  | 2003.00  |\n",
    "| 8    | Jaxson Hayes       | 2021-22  | 0.044106 | 4.41  | 609.0   | 13807.5   | 654.0  | 245 |144  | 398 |188  | 200  |115   | 43  | 33  | 55  |155  | 54  | 1363.00  |\n",
    "| 9    | Quentin Grimes     | 2022-23  | 0.044102 | 4.41  | 627.5   | 14228.5   | 799.0  | 282 | 78  | 602 | 98  | 180  | 49   | 150 | 47  | 26  |177  | 69  | 2086.00  |\n",
    "| 10   | Steve Novak        | 2011-12  | 0.044089 | 4.41  | 350.0   | 7938.5    | 477.0  | 161 | 22  | 336 | 26  | 95   | 9    | 12  | 16  |  9  | 59  | 21  | 998.00   |\n",
    "\n",
    "#### Bottom 10 Seasons by PIE (with context)\n",
    "\n",
    "| Rank | Player Name        | Season   | PIE      | PIE%  | PIE Num | PIE Den   | Points | FGM | FTM | FGA | FTA | DREB | OREB | AST | STL | BLK | PF  | TOV | Minutes  |\n",
    "|------|--------------------|----------|----------|-------|---------|-----------|--------|-----|-----|-----|-----|------|------|-----|-----|-----|-----|-----|----------|\n",
    "| 1    | Ronnie Price       | 2010-11  | 0.004207 | 0.42  | 40.5    | 9627.5    | 197.0  | 74  | 29  | 210 | 39  | 39   | 22   | 56  | 42  |  5  |106  | 55  | 689.00   |\n",
    "| 2    | Gary Harris        | 2014-15  | 0.004483 | 0.45  | 41.0    | 9146.5    | 188.0  | 66  | 35  | 217 | 47  | 43   | 21   | 29  | 39  |  7  | 71  | 38  | 692.00   |\n",
    "| 3    | Terrance Ferguson  | 2019-20  | 0.004869 | 0.49  | 46.5    | 9549.5    | 209.0  | 74  | 15  | 199 | 19  | 50   | 23   | 45  | 26  | 16  |144  | 30  |1146.00   |\n",
    "| 4    | Kevin Seraphin     | 2010-11  | 0.005166 | 0.52  | 49.0    | 9486.0    | 154.0  | 66  | 22  | 147 | 31  | 72   | 80   | 10  | 17  | 28  |126  | 42  | 604.00   |\n",
    "| 5    | Ryan Hollins       | 2011-12  | 0.005233 | 0.52  | 31.5    | 6020.0    | 131.0  | 46  | 39  | 84  | 75  | 48   | 34   |  9  |  5  | 17  | 78  | 35  | 505.00   |\n",
    "| 6    | Jason Collins      | 2010-11  | 0.005796 | 0.58  | 44.5    | 7677.5    | 96.0   | 34  | 27  | 71  | 41  | 72   | 30   | 22  |  9  |  9  | 97  | 26  | 570.00   |\n",
    "| 7    | Doron Lamb         | 2012-13  | 0.005845 | 0.58  | 46.0    | 7870.0    | 154.0  | 60  | 20  |163  | 34  | 38   |  8   | 32  | 13  |  0  | 52  | 26  | 560.00   |\n",
    "| 8    | Rashad Vaughn      | 2015-16  | 0.007004 | 0.70  | 86.5    |12350.5    | 217.0  | 81  | 12  |266  | 15  | 77   | 11   | 39  | 29  | 16  | 73  | 28  | 965.00   |\n",
    "| 9    | Terrance Ferguson  | 2017-18  | 0.007394 | 0.74  | 80.5    |10887.0    | 189.0  | 70  |  9  |169  | 10  | 28   | 19   | 19  | 24  | 10  | 83  | 11  | 730.00   |\n",
    "| 10   | John Lucas III     | 2013-14  | 0.007493 | 0.75  | 51.0    | 6806.0    | 159.0  | 62  | 10  |190  | 16  | 27   | 12   | 42  | 14  |  0  | 41  | 22  | 572.00   |\n",
    "\n",
    "---\n",
    "\n",
    "### Game Score\n",
    "\n",
    "#### Top 10 Seasons by Game Score per 36 (with context)\n",
    "\n",
    "| Rank | Player Name              | Season   | GS/36   | PTS/36 | FGM/36 | FGA/36 | FTM/36 | FTA/36 | OREB/36 | DREB/36 | STL/36 | AST/36 | BLK/36 | PF/36 | TOV/36 |\n",
    "|------|--------------------------|----------|---------|--------|--------|--------|--------|--------|---------|---------|--------|--------|--------|--------|--------|\n",
    "| 1    | Joel Embiid              | 2023-24  | 32.2674 | 38.432 | 13.011 | 24.253 | 10.800 | 12.284 | 2.526   | 9.537   | 1.232  | 6.221  | 1.863  | 3.032  | 4.105  |\n",
    "| 2    | Nikola Jokic             | 2024-25  | 29.6739 | 29.154 | 11.065 | 19.202 | 5.082  | 6.349  | 2.815   | 9.742   | 1.788  |10.079  | 0.633  | 2.252  | 3.238  |\n",
    "| 3    | Giannis Antetokounmpo    | 2019-20  | 29.1506 | 35.185 | 12.985 | 23.626 | 7.502  |11.864  | 2.670   |13.597   | 1.223  | 6.849  | 1.203  | 3.629  | 4.301  |\n",
    "| 4    | Nikola Jokic             | 2021-22  | 28.7543 | 29.567 | 11.272 | 19.343 | 5.592  | 6.905  | 3.039   |11.995   | 1.608  | 8.616  | 0.930  | 2.818  | 4.146  |\n",
    "| 5    | Shai Gilgeous-Alexander  | 2024-25  | 28.3994 | 34.434 | 11.932 | 23.093 | 8.303  | 9.251  | 0.921   | 4.344   | 1.828  | 6.708  | 1.058  | 2.296  | 2.557  |\n",
    "| 6    | Giannis Antetokounmpo    | 2021-22  | 28.3919 | 33.213 | 11.430 | 20.654 | 9.174  |12.708  | 2.223   |10.684   | 1.194  | 6.437  | 1.510  | 3.517  | 3.633  |\n",
    "| 7    | Joel Embiid              | 2022-23  | 28.3874 | 34.912 | 11.643 | 21.239 |10.571  |12.331  | 1.807   | 8.908   | 1.056  | 4.382  | 1.791  | 3.279  | 3.614  |\n",
    "| 8    | Nikola Jokic             | 2022-23  | 28.2147 | 26.591 | 10.164 | 16.080 | 5.365  | 6.530  | 2.628   |10.227   | 1.369  |10.668  | 0.740  | 2.738  | 3.886  |\n",
    "| 9    | Giannis Antetokounmpo    | 2023-24  | 28.2119 | 31.862 | 12.057 | 19.805 | 7.296  |11.124  | 2.894   | 9.366   | 1.198  | 6.861  | 1.151  | 2.971  | 3.532  |\n",
    "| 10   | Nikola Jokic             | 2023-24  | 27.9987 | 27.592 | 10.947 | 18.635 | 4.641  | 5.685  | 2.892   | 9.959   | 1.439  | 9.367  | 0.931  | 2.596  | 3.132  |\n",
    "\n",
    "#### Middle 10 Seasons by Game Score per 36 (with context)\n",
    "\n",
    "| Rank | Player Name            | Season   | GS/36   | PTS/36 | FGM/36 | FGA/36 | FTM/36 | FTA/36 | OREB/36 | DREB/36 | STL/36 | AST/36 | BLK/36 | PF/36 | TOV/36 |\n",
    "|------|------------------------|----------|---------|--------|--------|--------|--------|--------|---------|---------|--------|--------|--------|--------|--------|\n",
    "| 1    | Jabari Walker          | 2023-24  | 11.6792 | 13.739 | 5.037  |10.973  | 2.788  | 3.688  | 3.283   | 7.690   | 0.899  | 1.574  | 0.450  | 3.733  | 1.462  |\n",
    "| 2    | Roy Hibbert            | 2014-15  | 11.6767 | 15.284 | 6.041  |13.531  | 3.202  | 3.888  | 2.973   | 7.318   | 0.343  | 1.601  | 2.382  | 4.116  | 2.039  |\n",
    "| 3    | Max Strus              | 2024-25  | 11.6758 | 13.439 | 4.708  |10.643  | 0.799  | 0.970  | 1.512   | 4.679   | 0.742  | 4.508  | 0.342  | 2.967  | 1.541  |\n",
    "| 4    | Sam Hauser             | 2023-24  | 11.6745 | 14.772 | 5.166  |11.645  | 0.362  | 0.385  | 0.816   | 4.916   | 0.884  | 1.699  | 0.566  | 2.152  | 0.657  |\n",
    "| 5    | Julian Champagnie      | 2024-25  | 11.6744 | 15.330 | 5.181  |12.497  | 1.649  | 1.824  | 1.261   | 4.754   | 1.164  | 2.018  | 0.660  | 2.154  | 1.397  |\n",
    "| 6    | Justise Winslow        | 2021-22  | 11.6832 | 13.152 | 5.280  |12.336  | 1.872  | 3.168  | 2.400   | 7.296   | 1.680  | 4.032  | 1.200  | 3.216  | 2.352  |\n",
    "| 7    | Naji Marshall          | 2023-24  | 11.6723 | 13.401 | 5.025  |10.746  | 1.707  | 2.244  | 1.422   | 5.373   | 1.454  | 3.856  | 0.316  | 2.718  | 1.896  |\n",
    "| 8    | Shabazz Napier         | 2017-18  | 11.6712 | 15.456 | 5.352  |12.744  | 2.784  | 3.312  | 0.624   | 3.456   | 1.944  | 3.600  | 0.336  | 2.016  | 2.160  |\n",
    "| 9    | Terance Mann           | 2021-22  | 11.6696 | 13.787 | 5.281  |10.909  | 2.024  | 2.593  | 1.644   | 5.075   | 0.870  | 3.304  | 0.332  | 2.814  | 1.328  |\n",
    "| 10   | Michael Carter-Williams| 2014-15  | 11.6677 | 16.435 | 6.193  |15.635  | 3.437  | 4.951  | 1.089   | 4.917   | 1.888  | 7.520  | 0.510  | 2.841  | 4.304  |\n",
    "\n",
    "#### Bottom 10 Seasons by Game Score per 36 (with context)\n",
    "\n",
    "| Rank | Player Name        | Season   | GS/36   | PTS/36 | FGM/36 | FGA/36 | FTM/36 | FTA/36 | OREB/36 | DREB/36 | STL/36 | AST/36 | BLK/36 | PF/36 | TOV/36 |\n",
    "|------|--------------------|----------|---------|--------|--------|--------|--------|--------|---------|---------|--------|--------|--------|--------|--------|\n",
    "| 1    | DeShawn Stevenson  | 2011-12  | 3.2811  | 5.686  | 1.883  | 6.608  | 0.346  | 0.615  | 0.269   | 3.612   | 0.730  | 1.575  | 0.154  | 2.267  | 0.730  |\n",
    "| 2    | Terrance Ferguson  | 2019-20  | 3.4524  | 6.565  | 2.325  | 6.251  | 0.471  | 0.597  | 0.723   | 1.571   | 0.817  | 1.414  | 0.503  | 4.524  | 0.942  |\n",
    "| 3    | Stephen Graham     | 2010-11  | 3.5149  | 7.540  | 3.093  | 7.695  | 1.199  | 1.469  | 0.657   | 4.099   | 0.541  | 1.547  | 0.039  | 4.099  | 1.469  |\n",
    "| 4    | Mike Miller        | 2014-15  | 3.6801  | 5.822  | 1.976  | 6.089  | 0.160  | 0.214  | 0.214   | 4.647   | 0.748  | 2.457  | 0.214  | 3.953  | 1.228  |\n",
    "| 5    | Shawne Williams    | 2011-12  | 3.7728  | 8.136  | 3.024  |10.584  | 0.576  | 0.792  | 1.440   | 3.456   | 0.720  | 1.152  | 0.792  | 3.168  | 0.936  |\n",
    "| 6    | Rashad Vaughn      | 2015-16  | 3.8462  | 8.095  | 3.022  | 9.923  | 0.448  | 0.560  | 0.410   | 2.873   | 1.082  | 1.455  | 0.597  | 2.723  | 1.045  |\n",
    "| 7    | Semi Ojeleye       | 2017-18  | 3.9658  | 6.313  | 2.104  | 6.086  | 0.809  | 1.327  | 1.198   | 4.014   | 0.680  | 0.647  | 0.129  | 2.946  | 0.809  |\n",
    "| 8    | Jason Collins      | 2010-11  | 3.9663  | 6.063  | 2.147  | 4.484  | 1.705  | 2.589  | 1.895   | 4.547   | 0.568  | 1.389  | 0.568  | 6.126  | 1.642  |\n",
    "| 9    | Doron Lamb         | 2012-13  | 4.1079  | 9.900  | 3.857  |10.479  | 1.286  | 2.186  | 0.514   | 2.443   | 0.836  | 2.057  | 0.000  | 3.343  | 1.671  |\n",
    "| 10   | Anthony Brown      | 2015-16  | 4.1775  | 7.065  | 2.396  | 7.741  | 1.044  | 1.229  | 0.553   | 3.747   | 0.860  | 1.167  | 0.307  | 2.089  | 0.922  |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0da26de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/heat_data_scientist_2025/utils/config.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/heat_data_scientist_2025/utils/config.py\n",
    "# src/heat_data_scientist_2025/utils/config.py\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Iterable, Tuple, List, Dict, Optional\n",
    "import difflib\n",
    "from pathlib import Path\n",
    "from pydantic_settings import BaseSettings, SettingsConfigDict\n",
    "from pydantic import Field\n",
    "\n",
    "class Settings(BaseSettings):\n",
    "    # allow overriding via .env or real env; keep your robust repo_root default\n",
    "    repo_root: Path = Path(__file__).resolve().parents[3]\n",
    "    heat_data_root: Path = Field(default=Path(\"data\"), alias=\"HEAT_DATA_ROOT\")\n",
    "\n",
    "    model_config = SettingsConfigDict(\n",
    "        env_file=\".env\",          # loads .env if present\n",
    "        env_prefix=\"\",            # we already use explicit aliases\n",
    "        case_sensitive=False,     # friendlier on Windows\n",
    "        extra=\"ignore\",           # ignore unknown envs\n",
    "    )\n",
    "\n",
    "    @property\n",
    "    def data_root(self) -> Path:\n",
    "        return (self.repo_root / self.heat_data_root\n",
    "                if not self.heat_data_root.is_absolute()\n",
    "                else self.heat_data_root)\n",
    "\n",
    "\n",
    "S = Settings()\n",
    "\n",
    "_REPO_ROOT = S.repo_root\n",
    "DATA_ROOT  = S.data_root\n",
    "\n",
    "RAW_DIR =       DATA_ROOT / \"raw\" / \"heat_data_scientist_2025\"\n",
    "PROCESSED_DIR = DATA_ROOT / \"processed\" / \"heat_data_scientist_2025\"\n",
    "QUALITY_DIR =   DATA_ROOT / \"quality\" / \"quality_reports\"\n",
    "ML_DATASET_PATH = PROCESSED_DIR / \"nba_ml_dataset.parquet\"\n",
    "NBA_CATALOG_PATH = PROCESSED_DIR / \"nba_data_catalog.md\"\n",
    "SQLITE_PATH = PROCESSED_DIR / \"nba.sqlite\"\n",
    "RANKINGS_PATH = PROCESSED_DIR / \"nba_rankings_results.txt\"\n",
    "\n",
    "# EDA needs\n",
    "EDA_OUT_DIR = PROCESSED_DIR / \"eda\"\n",
    "\n",
    "# ML Pipeline paths\n",
    "ML_MODELS_DIR = PROCESSED_DIR / \"models\"\n",
    "ML_PREDICTIONS_DIR = PROCESSED_DIR / \"predictions\"\n",
    "ML_EVALUATION_DIR = PROCESSED_DIR / \"evaluation\"\n",
    "\n",
    "# production yaml\n",
    "PROJECT_ROOT: Path = Path(\"src\")\n",
    "COLUMN_SCHEMA_PATH: Path = PROJECT_ROOT / \"heat_data_scientist_2025\" / \"data\" / \"column_schema.yaml\"\n",
    "\n",
    "# Project Settings:\n",
    "start_season = 2009\n",
    "end_season = 2024\n",
    "final_top_data_amt = 10\n",
    "season_type = 'Regular Season'\n",
    "minutes_total_minimum_per_season = 500\n",
    "\n",
    "# --- ML Pipeline Configuration ---\n",
    "class MLPipelineConfig:\n",
    "    \"\"\"Configuration for ML Pipeline automation\"\"\"\n",
    "    \n",
    "    # Target and prediction settings\n",
    "    TARGET_COLUMN = \"season_pie\"\n",
    "    PREDICTION_YEAR = 2025\n",
    "    SOURCE_YEAR = 2024  # Use 2024 data to predict 2025\n",
    "    \n",
    "    # Training settings\n",
    "    TRAIN_START_YEAR = 2011  # First year with reliable lag features\n",
    "    TEST_YEARS = [2023, 2024]  # Hold out for validation\n",
    "    \n",
    "    # Model settings\n",
    "    DEFAULT_STRATEGY = \"filter_complete\"  # \"filter_complete\", \"two_stage\", \"auto\"\n",
    "    RANDOM_STATE = 42\n",
    "    \n",
    "    # Required lag features for complete cases\n",
    "    REQUIRED_LAG_FEATURES = [\n",
    "        \"season_pie\",\n",
    "        \"pts_per36\", \"ast_per36\", \"reb_per36\",\n",
    "        \"ts_pct\", \"efg_pct\",\n",
    "        \"usage_events_per_min\",\n",
    "        \"games_played\", \"total_minutes\",\n",
    "        \"defensive_per36\", \"production_per36\",\n",
    "        \"win_pct\", \"team_win_pct_final\"\n",
    "    ]\n",
    "    \n",
    "    # Feature engineering settings\n",
    "    NULL_STRATEGY = \"diagnose_only\"\n",
    "    CREATE_LAG_YEARS = [1]  # Create lag1 features\n",
    "    \n",
    "    # Model parameters\n",
    "    MODEL_PARAMS = {\n",
    "        'random_forest': {\n",
    "            'n_estimators': 200,\n",
    "            'max_depth': 12,\n",
    "            'min_samples_split': 5,\n",
    "            'min_samples_leaf': 2,\n",
    "            'random_state': RANDOM_STATE,\n",
    "            'n_jobs': -1\n",
    "        },\n",
    "        'xgboost': {\n",
    "            'n_estimators': 200,\n",
    "            'max_depth': 8,\n",
    "            'learning_rate': 0.05,\n",
    "            'subsample': 0.8,\n",
    "            'random_state': RANDOM_STATE,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Evaluation settings\n",
    "    EVALUATION_METRICS = ['r2', 'rmse', 'mae', 'mape']\n",
    "    TOP_N_PREDICTIONS = 50  # Top N players for leaderboard\n",
    "    \n",
    "    # Feature importance settings\n",
    "    MIN_FEATURE_IMPORTANCE = 0.001\n",
    "    TOP_FEATURES_COUNT = 20\n",
    "    \n",
    "    # Output settings\n",
    "    SAVE_ENGINEERED_DATA = True\n",
    "    SAVE_MODELS = True\n",
    "    SAVE_PREDICTIONS = True\n",
    "    SAVE_EVALUATION = True\n",
    "    \n",
    "    # Automation settings\n",
    "    AUTO_FEATURE_SELECTION = True\n",
    "    AUTO_HYPERPARAMETER_TUNING = False  # Set to True for automated tuning\n",
    "    CROSS_VALIDATION_FOLDS = 5\n",
    "\n",
    "# Initialize ML config\n",
    "ML_CONFIG = MLPipelineConfig()\n",
    "\n",
    "# --- Kaggle dataset handle ---\n",
    "KAGGLE_DATASET = \"eoinamoore/historical-nba-data-and-player-box-scores\"\n",
    "\n",
    "# --- Tables we actually care about right now ---\n",
    "IMPORTANT_TABLES = [\"PlayerStatistics\", \"TeamStatistics\"]\n",
    "\n",
    "# --- Table -> CSV filename mapping (simple & explicit) ---\n",
    "KAGGLE_TABLE_TO_CSV = {\n",
    "    \"Players\": \"Players.csv\",\n",
    "    \"PlayerStatistics\": \"PlayerStatistics.csv\",\n",
    "    \"TeamStatistics\": \"TeamStatistics.csv\",\n",
    "}\n",
    "PARQUET_DIR = Path(DATA_ROOT) / \"parquet_cache\"\n",
    "\n",
    "# Canonical ML export list (final parquet column order)\n",
    "ML_EXPORT_COLUMNS = [\n",
    "    # IDs & core season\n",
    "    \"personId\", \"player_name\", \"season\",\n",
    "\n",
    "    # playing time & outcomes\n",
    "    \"games_played\", \"total_minutes\",\n",
    "    \"win_pct\", \"home_games_pct\", \"avg_plus_minus\", \"total_plus_minus\",\n",
    "\n",
    "    # efficiency & per-36\n",
    "    \"season_pie\", \"ts_pct\", \"fg_pct\", \"fg3_pct\", \"ft_pct\",\n",
    "    \"pts_per36\", \"ast_per36\", \"reb_per36\",\n",
    "    \"usage_per_min\", \"efficiency_per_game\",\n",
    "\n",
    "    # raw season totals\n",
    "    \"total_points\", \"total_assists\", \"total_rebounds\",\n",
    "    \"total_steals\", \"total_blocks\", \"total_turnovers\",\n",
    "    \"total_fgm\", \"total_fga\", \"total_ftm\", \"total_fta\", \"total_3pm\", \"total_3pa\",\n",
    "\n",
    "    # player bio & role (expanded to include everything you listed)\n",
    "    \"height\", \"bodyWeight\", \"draftYear\", \"draftRound\", \"draftNumber\",\n",
    "    \"birthdate\", \"country\", \"position\",\n",
    "\n",
    "    # share-of-team season metrics\n",
    "    \"share_pts\", \"share_ast\", \"share_reb\",\n",
    "    \"share_stl\", \"share_blk\",\n",
    "    \"share_fga\", \"share_fgm\",\n",
    "    \"share_3pa\", \"share_3pm\",\n",
    "    \"share_fta\", \"share_ftm\",\n",
    "    \"share_tov\", \"share_reb_off\", \"share_reb_def\", \"share_pf\",\n",
    "    \"season_game_score_total\", \"game_score_per36\",\n",
    "]\n",
    "\n",
    "def _first_existing(candidates: Iterable[Path]) -> Path:\n",
    "    for p in candidates:\n",
    "        if p.exists():\n",
    "            return p\n",
    "    raise FileNotFoundError(\n",
    "        \"None of the candidate files exist:\\n\" + \"\\n\".join(str(c) for c in candidates)\n",
    "    )\n",
    "\n",
    "\n",
    "# training features\n",
    "season_pie_numerical_features = [\n",
    "    \"season_start_year\",\n",
    "    # lagged features\n",
    "    \"season_pie_lag1\", \"ts_pct_lag1\", \"efg_pct_lag1\", \"fg_pct_lag1\", \"fg3_pct_lag1\", \"ft_pct_lag1\",\n",
    "    \"pts_per36_lag1\", \"ast_per36_lag1\", \"reb_per36_lag1\", \"defensive_per36_lag1\",\n",
    "    \"production_per36_lag1\", \"stocks_per36_lag1\", \"three_point_rate_lag1\", \"ft_rate_lag1\",\n",
    "    \"pts_per_shot_lag1\", \"ast_to_tov_lag1\", \"usage_events_per_min_lag1\", \"usage_per_min_lag1\",\n",
    "    \"games_played_lag1\", \"total_minutes_lag1\", \"total_points_lag1\", \"total_assists_lag1\",\n",
    "    \"total_rebounds_lag1\", \"total_steals_lag1\", \"total_blocks_lag1\", \"total_fga_lag1\",\n",
    "    \"total_fta_lag1\", \"total_3pa_lag1\", \"total_3pm_lag1\", \"total_tov_lag1\", \"win_pct_lag1\",\n",
    "    \"avg_plus_minus_lag1\", \"team_win_pct_final_lag1\",\n",
    "    \"offensive_impact_lag1\", \"two_way_impact_lag1\", \"efficiency_volume_score_lag1\",\n",
    "    \"versatility_score_lag1\", \"shooting_score_lag1\"\n",
    "]\n",
    "game_score_per36_numerical_features = [\n",
    "    \"season_start_year\",\n",
    "    # lagged features\n",
    "    \"season_pie_lag1\", \"ts_pct_lag1\", \"efg_pct_lag1\", \"fg_pct_lag1\", \"fg3_pct_lag1\", \"ft_pct_lag1\",\n",
    "    \"pts_per36_lag1\", \"ast_per36_lag1\", \"reb_per36_lag1\", \"defensive_per36_lag1\",\n",
    "    \"production_per36_lag1\", \"stocks_per36_lag1\", \"three_point_rate_lag1\", \"ft_rate_lag1\",\n",
    "    \"pts_per_shot_lag1\", \"ast_to_tov_lag1\", \"usage_events_per_min_lag1\", \"usage_per_min_lag1\",\n",
    "    \"games_played_lag1\", \"total_minutes_lag1\", \"total_points_lag1\", \"total_assists_lag1\",\n",
    "    \"total_rebounds_lag1\", \"total_steals_lag1\", \"total_blocks_lag1\", \"total_fga_lag1\",\n",
    "    \"total_fta_lag1\", \"total_3pa_lag1\", \"total_3pm_lag1\", \"total_tov_lag1\", \"win_pct_lag1\",\n",
    "    \"avg_plus_minus_lag1\", \"team_win_pct_final_lag1\",\n",
    "    \"offensive_impact_lag1\", \"two_way_impact_lag1\", \"efficiency_volume_score_lag1\",\n",
    "    \"versatility_score_lag1\", \"shooting_score_lag1\"\n",
    "]\n",
    "\n",
    "nominal_categoricals = []\n",
    "ordinal_categoricals = [\"minutes_tier\"] \n",
    "y_variables = [\"season_pie\", \"game_score_per36\"]\n",
    "\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Paths:\n",
    "    repo_root: Path = _REPO_ROOT\n",
    "    data_root: Path = DATA_ROOT\n",
    "    raw_dir: Path = RAW_DIR\n",
    "    processed_dir: Path = PROCESSED_DIR\n",
    "    quality_dir: Path = QUALITY_DIR\n",
    "    ml_dataset_path: Path = ML_DATASET_PATH\n",
    "    nba_catalog_path: Path = NBA_CATALOG_PATH\n",
    "    sqlite_path: Path = SQLITE_PATH\n",
    "    rankings_path: Path = RANKINGS_PATH\n",
    "    eda_out_dir: Path = EDA_OUT_DIR\n",
    "    column_schema_path: Path = COLUMN_SCHEMA_PATH\n",
    "    \n",
    "    # ML Pipeline paths\n",
    "    ml_models_dir: Path = ML_MODELS_DIR\n",
    "    ml_predictions_dir: Path = ML_PREDICTIONS_DIR\n",
    "    ml_evaluation_dir: Path = ML_EVALUATION_DIR\n",
    "\n",
    "    def ensure_ml_dirs(self) -> None:\n",
    "        \"\"\"Create ML pipeline directories if they don't exist.\"\"\"\n",
    "        self.ml_models_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.ml_predictions_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.ml_evaluation_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def csv(self, name: str) -> Path:\n",
    "        \"\"\"Return a path under RAW_DIR for an exact filename, with rich diagnostics on failure.\"\"\"\n",
    "        p = (self.raw_dir / name).resolve()\n",
    "        if p.exists():\n",
    "            return p\n",
    "\n",
    "        raw_exists = self.raw_dir.exists()\n",
    "        all_csvs = []\n",
    "        if raw_exists:\n",
    "            try:\n",
    "                all_csvs = sorted([q.name for q in self.raw_dir.glob(\"*.csv\")])\n",
    "            except Exception:\n",
    "                all_csvs = []\n",
    "\n",
    "        suggestions = difflib.get_close_matches(name, all_csvs, n=5, cutoff=0.5) if all_csvs else []\n",
    "\n",
    "        lines = [\n",
    "            f\"Expected CSV not found: {p}\",\n",
    "            f\"RAW_DIR: {self.raw_dir}  (exists={raw_exists})\",\n",
    "            f\"DATA_ROOT (override with HEAT_DATA_ROOT): {self.data_root}\",\n",
    "            f\"CSV files found in RAW_DIR ({len(all_csvs)}): {all_csvs[:25]}{' ...' if len(all_csvs) > 25 else ''}\",\n",
    "        ]\n",
    "        if suggestions:\n",
    "            lines.append(f\"Closest names: {suggestions}\")\n",
    "        lines.append(\"If your data lives elsewhere, set the environment variable HEAT_DATA_ROOT to that base folder.\")\n",
    "        raise FileNotFoundError(\"\\n\".join(lines))\n",
    "\n",
    "    def csv_any(self, *names: str) -> Path:\n",
    "        return _first_existing([(self.raw_dir / n).resolve() for n in names])\n",
    "\n",
    "    # canonical asset getters (7 kaggle tables + optional league schedule)\n",
    "    def players_csv(self) -> Path:           return self.csv(\"Players.csv\")\n",
    "    def playerstats_csv(self) -> Path:       return self.csv(\"PlayerStatistics.csv\")\n",
    "    def teamstats_csv(self) -> Path:         return self.csv(\"TeamStatistics.csv\")\n",
    "    def games_csv(self) -> Path:             return self.csv(\"Games.csv\")\n",
    "    def teams_csv(self) -> Path:             return self.csv(\"Teams.csv\")\n",
    "    def team_histories_csv(self) -> Path:    return self.csv_any(\"CoachHistory.csv\", \"Coaches.csv\")\n",
    "    def league_schedule_csv(self) -> Path:   return (self.raw_dir / \"LeagueSchedule24_25.csv\")\n",
    "\n",
    "    # ML Pipeline file getters\n",
    "    def model_path(self, model_name: str, year: int) -> Path:\n",
    "        \"\"\"Get path for saving/loading trained models.\"\"\"\n",
    "        return self.ml_models_dir / f\"{model_name}_{year}_model.pkl\"\n",
    "    \n",
    "    def predictions_path(self, target: str, year: int | None = None) -> Path:\n",
    "        \"\"\"\n",
    "        Flexible getter for per-target predictions path.\n",
    "\n",
    "        Accepts either:\n",
    "        - (target, year) e.g., (\"season_pie\", 2025)\n",
    "        - single string with trailing year, e.g., \"season_pie_2025\"\n",
    "        - single string that already includes a *_predictions_YYYY tag, we'll normalize it\n",
    "\n",
    "        Produces: .../predictions/{target}_predictions_{year}.parquet\n",
    "        \"\"\"\n",
    "        self.ensure_ml_dirs()\n",
    "\n",
    "        safe_target = str(target).strip().lower()\n",
    "\n",
    "        # If year isn't separately provided, try to parse it from the 'target' string\n",
    "        if year is None:\n",
    "            # tolerate patterns like \"season_pie_2025\" or \"season_pie_predictions_2025\"\n",
    "            import re\n",
    "            m = re.search(r'(\\d{4})$', safe_target)\n",
    "            if m:\n",
    "                year = int(m.group(1))\n",
    "                # strip known suffixes to get the target core\n",
    "                safe_target = re.sub(r'(_predictions)?_\\d{4}$', '', safe_target)\n",
    "            else:\n",
    "                raise TypeError(\"predictions_path() requires 'year' or a target string ending with a 4-digit year.\")\n",
    "\n",
    "        return self.ml_predictions_dir / f\"{safe_target}_predictions_{int(year)}.parquet\"\n",
    "\n",
    "\n",
    "    def leaderboard_path(self, target: str, year: int | None = None) -> Path:\n",
    "        \"\"\"\n",
    "        Flexible getter for per-target leaderboard path.\n",
    "\n",
    "        Accepts either:\n",
    "        - (target, year) e.g., (\"game_score_per36\", 2025)\n",
    "        - single string with trailing year, e.g., \"game_score_per36_2025\"\n",
    "        - single string that already includes a *_leaderboard_YYYY tag, we'll normalize it\n",
    "\n",
    "        Produces: .../predictions/{target}_leaderboard_{year}.csv\n",
    "        \"\"\"\n",
    "        self.ensure_ml_dirs()\n",
    "\n",
    "        safe_target = str(target).strip().lower()\n",
    "\n",
    "        if year is None:\n",
    "            import re\n",
    "            m = re.search(r'(\\d{4})$', safe_target)\n",
    "            if m:\n",
    "                year = int(m.group(1))\n",
    "                safe_target = re.sub(r'(_leaderboard)?_\\d{4}$', '', safe_target)\n",
    "            else:\n",
    "                raise TypeError(\"leaderboard_path() requires 'year' or a target string ending with a 4-digit year.\")\n",
    "\n",
    "        return self.ml_predictions_dir / f\"{safe_target}_leaderboard_{int(year)}.csv\"\n",
    "\n",
    "    \n",
    "    def evaluation_path(self, model_name: str, year: int) -> Path:\n",
    "        \"\"\"Get path for saving/loading evaluation results.\"\"\"\n",
    "        return self.ml_evaluation_dir / f\"{model_name}_{year}_evaluation.json\"\n",
    "    \n",
    "    def feature_importance_path(self, model_name: str, year: int) -> Path:\n",
    "        \"\"\"Get path for saving/loading feature importance.\"\"\"\n",
    "        return self.ml_evaluation_dir / f\"{model_name}_{year}_feature_importance.csv\"\n",
    "\n",
    "CFG = Paths()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Config paths:\")\n",
    "    print(f\"Raw dir: {CFG.raw_dir}\")\n",
    "    print(f\"Processed dir: {CFG.processed_dir}\")\n",
    "    print(f\"ML models dir: {CFG.ml_models_dir}\")\n",
    "    print(f\"ML predictions dir: {CFG.ml_predictions_dir}\")\n",
    "    print(f\"ML evaluation dir: {CFG.ml_evaluation_dir}\")\n",
    "    \n",
    "    print(\"\\nML Config:\")\n",
    "    print(f\"Target column: {ML_CONFIG.TARGET_COLUMN}\")\n",
    "    print(f\"Prediction year: {ML_CONFIG.PREDICTION_YEAR}\")\n",
    "    print(f\"Required lag features: {len(ML_CONFIG.REQUIRED_LAG_FEATURES)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8df1f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/heat_data_scientist_2025/data/kaggle_pull.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/heat_data_scientist_2025/data/kaggle_pull.py\n",
    "\"\"\"\n",
    "quick pass:\n",
    "- pull kaggle tables, compute per-game PIE -> roll to season -> rank top/mid/bottom\n",
    "- rules: 2010+ seasons, >=500 minutes gate (applied at the *end*), tie-breakers as noted\n",
    "- also spit out Game Score per 36 (gs36) companions\n",
    "- keeping this all python-first; no duckdb materialization here\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Dict, Iterable\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import kagglehub as kh\n",
    "from kagglehub import KaggleDatasetAdapter as KDA\n",
    "\n",
    "from src.heat_data_scientist_2025.utils.config import (\n",
    "    CFG,\n",
    "    KAGGLE_DATASET,\n",
    "    IMPORTANT_TABLES,\n",
    "    KAGGLE_TABLE_TO_CSV,\n",
    "    start_season as CFG_START_SEASON,\n",
    "    season_type as CFG_SEASON_TYPE,\n",
    "    minutes_total_minimum_per_season as CFG_MIN_SEASON_MINUTES,\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# Utilities\n",
    "# ---------------------------\n",
    "\n",
    "# note: season strings like 2010-11 based on aug–jul boundary\n",
    "# kinda obvious: aug+ is next season start; else previous year\n",
    "def _season_from_timestamp(ts: pd.Series) -> pd.Series:\n",
    "    \"\"\"Create season strings like '2010-11' (Aug–Jul season boundary).\"\"\"\n",
    "    dt = pd.to_datetime(ts, errors=\"coerce\", utc=False)\n",
    "    start_year = np.where(dt.dt.month >= 8, dt.dt.year, dt.dt.year - 1)\n",
    "    end_year = start_year + 1\n",
    "    start_s = pd.Series(start_year, index=ts.index).astype(str)\n",
    "    end_s = (pd.Series(end_year, index=ts.index) % 100).astype(str).str.zfill(2)\n",
    "    return start_s + \"-\" + end_s\n",
    "\n",
    "# tiny helper: divide but don't explode on zeros/NaNs\n",
    "# returns 0 when denom <= 0; good enough for rate features here\n",
    "def _safe_div(num: pd.Series, den: pd.Series) -> pd.Series:\n",
    "    \"\"\"Safe division: returns 0 where denom <= 0 or NaN.\"\"\"\n",
    "    den = den.fillna(0)\n",
    "    out = pd.Series(np.zeros(len(num)), index=num.index, dtype=\"float64\")\n",
    "    mask = den > 0\n",
    "    out.loc[mask] = (num[mask] / den[mask]).astype(\"float64\")\n",
    "    return out\n",
    "\n",
    "# bulk numeric coercion; keep errors as NaN and move on\n",
    "def _to_numeric(df: pd.DataFrame, cols: list[str]) -> pd.DataFrame:\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "# quick join QA: sizes + unmatched keys sample; prints only, no exceptions\n",
    "# i use this when debugging merges; safe to leave in\n",
    "def _join_diagnostics(\n",
    "    left: pd.DataFrame,\n",
    "    right: pd.DataFrame,\n",
    "    merged: pd.DataFrame,\n",
    "    on: list[str] | str,\n",
    "    how: str,\n",
    "    left_name: str = \"left\",\n",
    "    right_name: str = \"right\",\n",
    "    key_sample_n: int = 5,\n",
    ") -> None:\n",
    "    \"\"\"Compact report about a merge: size deltas and unmatched keys (QA only).\"\"\"\n",
    "    on_cols = [on] if isinstance(on, str) else list(on)\n",
    "    print(f\"[JOIN] '{left_name}' x '{right_name}' on {on_cols} (how='{how}')\")\n",
    "    print(f\"       {left_name} rows:  {len(left):,}\")\n",
    "    print(f\"       {right_name} rows: {len(right):,}\")\n",
    "    print(f\"       merged rows:       {len(merged):,}\")\n",
    "\n",
    "    if how in (\"left\", \"inner\"):\n",
    "        left_keys = left[on_cols].drop_duplicates()\n",
    "        right_keys = right[on_cols].drop_duplicates()\n",
    "        merged_keys = merged[on_cols].drop_duplicates()\n",
    "        left_only = (\n",
    "            left_keys.merge(merged_keys, on=on_cols, how=\"left\", indicator=True)\n",
    "            .query(\"_merge == 'left_only'\")\n",
    "            .drop(columns=[\"_merge\"])\n",
    "        )\n",
    "        right_only = (\n",
    "            right_keys.merge(merged_keys, on=on_cols, how=\"left\", indicator=True)\n",
    "            .query(\"_merge == 'left_only'\")\n",
    "            .drop(columns=[\"_merge\"])\n",
    "        )\n",
    "        if len(left_only):\n",
    "            print(f\"       ⚠ Unmatched in {right_name}: {len(left_only):,} key(s) from {left_name}\")\n",
    "            if key_sample_n > 0:\n",
    "                print(left_only.head(key_sample_n))\n",
    "        if len(right_only):\n",
    "            print(f\"       ℹ Extra keys in {right_name} not used: {len(right_only):,}\")\n",
    "            if key_sample_n > 0:\n",
    "                print(right_only.head(key_sample_n))\n",
    "\n",
    "# ---------------------------\n",
    "# Load + filters\n",
    "# ---------------------------\n",
    "\n",
    "# rough filter pass:\n",
    "# - tag seasons on both tables\n",
    "# - keep only {season_type} + seasons >= start\n",
    "# - compute season minutes, but *don’t* gate yet (see PIE bias note)\n",
    "def enforce_criteria_python(\n",
    "    players_df: pd.DataFrame | None,\n",
    "    player_stats_df: pd.DataFrame,\n",
    "    team_stats_df: pd.DataFrame,\n",
    "    start_season: int = CFG_START_SEASON,\n",
    "    season_type: str = CFG_SEASON_TYPE,\n",
    "    minutes_total_minimum_per_season: int = CFG_MIN_SEASON_MINUTES,\n",
    "    defer_minutes_gate: bool = True,\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Adds 'season' to player/team stats; filters by gameType and start season.\n",
    "    Computes per (personId, season) minutes_total but defers gating by default.\n",
    "    \"\"\"\n",
    "    ps = player_stats_df.copy()\n",
    "    ps[\"season\"] = _season_from_timestamp(ps[\"gameDate\"])\n",
    "    ts = team_stats_df.copy()\n",
    "    ts[\"season\"] = _season_from_timestamp(ts[\"gameDate\"])\n",
    "\n",
    "    ps = _to_numeric(ps, [\n",
    "        \"numMinutes\",\"points\",\"assists\",\"blocks\",\"steals\",\n",
    "        \"fieldGoalsAttempted\",\"fieldGoalsMade\",\"freeThrowsAttempted\",\"freeThrowsMade\",\n",
    "        \"threePointersAttempted\",\"threePointersMade\",\n",
    "        \"reboundsDefensive\",\"reboundsOffensive\",\"reboundsTotal\",\n",
    "        \"foulsPersonal\",\"turnovers\",\"plusMinusPoints\",\"home\",\"win\"\n",
    "    ])\n",
    "    for bcol in [\"home\",\"win\"]:\n",
    "        if bcol in ps.columns:\n",
    "            ps[bcol] = ps[bcol].fillna(0).astype(int)\n",
    "\n",
    "    ps = ps.loc[\n",
    "        (ps[\"gameType\"] == season_type) &\n",
    "        (ps[\"season\"].str.slice(0, 4).astype(int) >= start_season)\n",
    "    ].copy()\n",
    "\n",
    "    fn = ps.get(\"firstName\", \"\").fillna(\"\")\n",
    "    ln = ps.get(\"lastName\", \"\").fillna(\"\")\n",
    "    ps[\"player_name\"] = (fn + \" \" + ln).str.strip()\n",
    "\n",
    "    season_minutes = (\n",
    "        ps.groupby([\"personId\",\"season\"], as_index=False)[\"numMinutes\"]\n",
    "          .sum().rename(columns={\"numMinutes\": \"minutes_total\"})\n",
    "    )\n",
    "    ps = ps.merge(season_minutes, on=[\"personId\",\"season\"], how=\"left\")\n",
    "\n",
    "    if not defer_minutes_gate:\n",
    "        ps = ps.loc[ps[\"minutes_total\"] >= minutes_total_minimum_per_season].copy()\n",
    "\n",
    "    players_out = players_df.copy() if players_df is not None else None\n",
    "    return players_out, ps, ts\n",
    "\n",
    "# ---------------------------\n",
    "# PIE (per game) + season aggregation\n",
    "# ---------------------------\n",
    "\n",
    "# core math: build per-game PIE exactly once, *then* roll up\n",
    "# heads-up: drop 0-minute rows; also sanity check that sum(pie) ~ 1 per game\n",
    "def compute_player_game_pie(\n",
    "    filtered_player_stats: pd.DataFrame,\n",
    "    drop_zero_minute_games: bool = True,\n",
    "    validate_game_sums: bool = True,\n",
    "    atol: float = 1e-9,\n",
    "    rtol: float = 1e-6,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Compute per-game PIE; denominator is sum of numerators across ALL players in that game.\"\"\"\n",
    "    required = [\n",
    "        \"points\",\"fieldGoalsMade\",\"freeThrowsMade\",\"fieldGoalsAttempted\",\"freeThrowsAttempted\",\n",
    "        \"reboundsDefensive\",\"reboundsOffensive\",\"assists\",\"steals\",\"blocks\",\"foulsPersonal\",\"turnovers\",\n",
    "        \"gameId\",\"numMinutes\"\n",
    "    ]\n",
    "    missing = [c for c in required if c not in filtered_player_stats.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing columns for PIE calculation: {missing}\")\n",
    "\n",
    "    df = filtered_player_stats.copy()\n",
    "    df = _to_numeric(df, required)\n",
    "\n",
    "    if drop_zero_minute_games:\n",
    "        before = len(df)\n",
    "        df = df.loc[df[\"numMinutes\"] > 0].copy()\n",
    "        if before != len(df):\n",
    "            print(f\"[PIE] Dropped zero-minute rows: {before - len(df):,} (kept {len(df):,})\")\n",
    "\n",
    "    df[\"pie_numerator\"] = (\n",
    "        df[\"points\"]\n",
    "        + df[\"fieldGoalsMade\"]\n",
    "        + df[\"freeThrowsMade\"]\n",
    "        - df[\"fieldGoalsAttempted\"]\n",
    "        - df[\"freeThrowsAttempted\"]\n",
    "        + df[\"reboundsDefensive\"]\n",
    "        + 0.5 * df[\"reboundsOffensive\"]\n",
    "        + df[\"assists\"]\n",
    "        + df[\"steals\"]\n",
    "        + 0.5 * df[\"blocks\"]\n",
    "        - df[\"foulsPersonal\"]\n",
    "        - df[\"turnovers\"]\n",
    "    )\n",
    "\n",
    "    den = (\n",
    "        df.groupby(\"gameId\", as_index=False)[\"pie_numerator\"]\n",
    "          .sum().rename(columns={\"pie_numerator\": \"pie_denominator\"})\n",
    "    )\n",
    "\n",
    "    before_n = len(df)\n",
    "    merged = df.merge(den, on=\"gameId\", how=\"left\", validate=\"many_to_one\")\n",
    "    _join_diagnostics(df, den, merged, on=\"gameId\", how=\"left\",\n",
    "                      left_name=\"player_game\", right_name=\"game_denominators\")\n",
    "\n",
    "    merged[\"pie_denominator\"] = pd.to_numeric(merged[\"pie_denominator\"], errors=\"coerce\")\n",
    "    merged = merged.loc[merged[\"pie_denominator\"] > 0].copy()\n",
    "    merged[\"game_pie\"] = _safe_div(merged[\"pie_numerator\"], merged[\"pie_denominator\"])\n",
    "\n",
    "    if validate_game_sums:\n",
    "        sums = merged.groupby(\"gameId\", as_index=False)[\"game_pie\"].sum()\n",
    "        bad = sums.loc[~np.isclose(sums[\"game_pie\"], 1.0, rtol=rtol, atol=atol)]\n",
    "        if not bad.empty:\n",
    "            print(f\"[PIE][QA] Σ(game_pie) != 1 within tol for {len(bad):,} game(s). \"\n",
    "                  f\"Max deviation: {float((bad['game_pie'] - 1.0).abs().max()):.3e}\")\n",
    "            print(bad.head(5))\n",
    "\n",
    "    after_n = len(merged)\n",
    "    if after_n > before_n:\n",
    "        raise ValueError(f\"[PIE] Unexpected row gain after merge/filter: before={before_n}, after={after_n}\")\n",
    "\n",
    "    if merged[[\"pie_denominator\",\"game_pie\"]].isna().any().any():\n",
    "        raise ValueError(\"[PIE] Nulls detected after join.\")\n",
    "\n",
    "    return merged\n",
    "\n",
    "# season rollup:\n",
    "# - totals, %s, PIE season-level, per-36s, usage/eff\n",
    "# - pick primary team by minutes and attach final team win%\n",
    "# - minutes gate at the end to avoid denominator bias\n",
    "def build_player_season_table_python(\n",
    "    player_game_with_pie: pd.DataFrame,\n",
    "    team_stats_df: pd.DataFrame,\n",
    "    minutes_total_minimum_per_season: int = CFG_MIN_SEASON_MINUTES,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Aggregate player-game rows to player-season (PIE, TS%, per-36, GS36, team ctx).\"\"\"\n",
    "    pg = player_game_with_pie.copy()\n",
    "\n",
    "    pg = _to_numeric(pg, [\n",
    "        \"numMinutes\",\"points\",\"assists\",\"blocks\",\"steals\",\n",
    "        \"reboundsDefensive\",\"reboundsOffensive\",\"reboundsTotal\",\n",
    "        \"foulsPersonal\",\"turnovers\",\"fieldGoalsAttempted\",\"fieldGoalsMade\",\n",
    "        \"freeThrowsAttempted\",\"freeThrowsMade\",\"threePointersAttempted\",\"threePointersMade\",\n",
    "        \"plusMinusPoints\",\"game_pie\",\"home\",\"win\"\n",
    "    ])\n",
    "\n",
    "    g = pg.groupby([\"personId\",\"player_name\",\"season\"], as_index=False).agg(\n",
    "        games_played=(\"gameId\",\"nunique\"),\n",
    "        total_minutes=(\"numMinutes\",\"sum\"),\n",
    "        total_points=(\"points\",\"sum\"),\n",
    "        total_assists=(\"assists\",\"sum\"),\n",
    "        total_rebounds=(\"reboundsTotal\",\"sum\"),\n",
    "        total_oreb=(\"reboundsOffensive\",\"sum\"),\n",
    "        total_dreb=(\"reboundsDefensive\",\"sum\"),\n",
    "        total_blocks=(\"blocks\",\"sum\"),\n",
    "        total_stls=(\"steals\",\"sum\"),\n",
    "        total_pf=(\"foulsPersonal\",\"sum\"),\n",
    "        total_tov=(\"turnovers\",\"sum\"),\n",
    "        total_fga=(\"fieldGoalsAttempted\",\"sum\"),\n",
    "        total_fgm=(\"fieldGoalsMade\",\"sum\"),\n",
    "        total_fta=(\"freeThrowsAttempted\",\"sum\"),\n",
    "        total_ftm=(\"freeThrowsMade\",\"sum\"),\n",
    "        total_3pa=(\"threePointersAttempted\",\"sum\"),\n",
    "        total_3pm=(\"threePointersMade\",\"sum\"),\n",
    "        total_plus_minus=(\"plusMinusPoints\",\"sum\"),\n",
    "        avg_plus_minus=(\"plusMinusPoints\",\"mean\"),\n",
    "        wins=(\"win\",\"sum\"),\n",
    "        home_games=(\"home\",\"sum\"),\n",
    "        season_pie_num=(\"pie_numerator\",\"sum\"),\n",
    "        season_pie_den=(\"pie_denominator\",\"sum\"),\n",
    "    )\n",
    "\n",
    "    # Shooting\n",
    "    g[\"fg_pct\"]  = _safe_div(g[\"total_fgm\"], g[\"total_fga\"])\n",
    "    g[\"fg3_pct\"] = _safe_div(g[\"total_3pm\"], g[\"total_3pa\"])\n",
    "    g[\"ft_pct\"]  = _safe_div(g[\"total_ftm\"], g[\"total_fta\"])\n",
    "    g[\"ts_pct\"]  = _safe_div(g[\"total_points\"], 2.0 * (g[\"total_fga\"] + 0.44 * g[\"total_fta\"]))\n",
    "\n",
    "    # PIE season (weighted) + percent-style display\n",
    "    g[\"season_pie\"] = _safe_div(g[\"season_pie_num\"], g[\"season_pie_den\"])\n",
    "    g[\"season_pie_pct\"] = 100.0 * g[\"season_pie\"]\n",
    "\n",
    "    # Optional: simple per-game average of game_pie (diagnostic only)\n",
    "    game_mean = (pg.groupby([\"personId\",\"season\"], as_index=False)[\"game_pie\"]\n",
    "                   .mean()\n",
    "                   .rename(columns={\"game_pie\":\"season_pie_avg_game\"}))\n",
    "    g = g.merge(game_mean, on=[\"personId\",\"season\"], how=\"left\")\n",
    "\n",
    "    # Per-36 (incl. GS inputs)\n",
    "    g[\"pts_per36\"] = _safe_div(g[\"total_points\"] * 36.0, g[\"total_minutes\"])\n",
    "    g[\"ast_per36\"] = _safe_div(g[\"total_assists\"] * 36.0, g[\"total_minutes\"])\n",
    "    g[\"reb_per36\"] = _safe_div(g[\"total_rebounds\"] * 36.0, g[\"total_minutes\"])\n",
    "    g[\"fgm_per36\"]  = _safe_div(g[\"total_fgm\"]  * 36.0, g[\"total_minutes\"])\n",
    "    g[\"fga_per36\"]  = _safe_div(g[\"total_fga\"]  * 36.0, g[\"total_minutes\"])\n",
    "    g[\"ftm_per36\"]  = _safe_div(g[\"total_ftm\"]  * 36.0, g[\"total_minutes\"])\n",
    "    g[\"fta_per36\"]  = _safe_div(g[\"total_fta\"]  * 36.0, g[\"total_minutes\"])\n",
    "    g[\"oreb_per36\"] = _safe_div(g[\"total_oreb\"] * 36.0, g[\"total_minutes\"])\n",
    "    g[\"dreb_per36\"] = _safe_div(g[\"total_dreb\"] * 36.0, g[\"total_minutes\"])\n",
    "    g[\"stl_per36\"]  = _safe_div(g[\"total_stls\"] * 36.0, g[\"total_minutes\"])\n",
    "    g[\"blk_per36\"]  = _safe_div(g[\"total_blocks\"]* 36.0, g[\"total_minutes\"])\n",
    "    g[\"pf_per36\"]   = _safe_div(g[\"total_pf\"]   * 36.0, g[\"total_minutes\"])\n",
    "    g[\"tov_per36\"]  = _safe_div(g[\"total_tov\"]  * 36.0, g[\"total_minutes\"])\n",
    "\n",
    "    # Usage/Efficiency\n",
    "    g[\"usage_per_min\"] = _safe_div(\n",
    "        (g[\"total_fga\"] + 0.44 * g[\"total_fta\"] + g[\"total_tov\"]), g[\"total_minutes\"]\n",
    "    )\n",
    "    g[\"efficiency_per_game\"] = _safe_div(\n",
    "        (g[\"total_points\"] + g[\"total_rebounds\"] + g[\"total_assists\"]\n",
    "         + g[\"total_stls\"] + g[\"total_blocks\"]\n",
    "         - (g[\"total_fga\"] - g[\"total_fgm\"])\n",
    "         - (g[\"total_fta\"] - g[\"total_ftm\"])\n",
    "         - g[\"total_tov\"]),\n",
    "        g[\"games_played\"]\n",
    "    )\n",
    "\n",
    "    g[\"win_pct\"] = _safe_div(g[\"wins\"], g[\"games_played\"])\n",
    "    g[\"home_games_pct\"] = _safe_div(g[\"home_games\"], g[\"games_played\"])\n",
    "\n",
    "    # primary team by minutes, then attach team final W/L%\n",
    "    ts = team_stats_df.copy()\n",
    "    if \"season\" not in ts.columns:\n",
    "        ts[\"season\"] = _season_from_timestamp(ts[\"gameDate\"])\n",
    "    ts = _to_numeric(ts, [\"seasonWins\",\"seasonLosses\"])\n",
    "    ts[\"seasonWins\"]   = ts[\"seasonWins\"].fillna(0)\n",
    "    ts[\"seasonLosses\"] = ts[\"seasonLosses\"].fillna(0)\n",
    "\n",
    "    team_final = (\n",
    "        ts.groupby([\"teamCity\",\"teamName\",\"season\"], as_index=False)\n",
    "          .agg(final_wins=(\"seasonWins\",\"max\"), final_losses=(\"seasonLosses\",\"max\"))\n",
    "    )\n",
    "    team_final[\"team_win_pct_final\"] = _safe_div(\n",
    "        team_final[\"final_wins\"], (team_final[\"final_wins\"] + team_final[\"final_losses\"])\n",
    "    )\n",
    "\n",
    "    def _pick(col_lower: str, col_camel: str) -> str:\n",
    "        if col_lower in pg.columns: return col_lower\n",
    "        if col_camel in pg.columns: return col_camel\n",
    "        raise KeyError(f\"Expected one of '{col_lower}' or '{col_camel}' in player-game columns.\")\n",
    "\n",
    "    team_city_col = _pick(\"playerteamCity\", \"playerTeamCity\")\n",
    "    team_name_col = _pick(\"playerteamName\", \"playerTeamName\")\n",
    "\n",
    "    per_team_minutes = (\n",
    "        pg.groupby([\"personId\",\"season\",team_city_col,team_name_col], as_index=False)\n",
    "          .agg(minutes_on_team=(\"numMinutes\",\"sum\"))\n",
    "    )\n",
    "    idx = per_team_minutes.groupby([\"personId\",\"season\"])[\"minutes_on_team\"].idxmax()\n",
    "    main_team = per_team_minutes.loc[idx, [\"personId\",\"season\",team_city_col,team_name_col]].copy()\n",
    "    main_team = main_team.rename(columns={team_city_col: \"teamCity\", team_name_col: \"teamName\"})\n",
    "\n",
    "    g_team = g.merge(main_team, on=[\"personId\",\"season\"], how=\"left\", validate=\"one_to_one\")\n",
    "\n",
    "    merged = g_team.merge(\n",
    "        team_final[[\"teamCity\",\"teamName\",\"season\",\"team_win_pct_final\"]],\n",
    "        on=[\"teamCity\",\"teamName\",\"season\"], how=\"left\", validate=\"many_to_one\"\n",
    "    )\n",
    "    _join_diagnostics(g_team, team_final, merged,\n",
    "                      on=[\"teamCity\",\"teamName\",\"season\"], how=\"left\",\n",
    "                      left_name=\"player_season\", right_name=\"team_final\")\n",
    "\n",
    "    merged = merged.rename(columns={\n",
    "        \"total_stls\": \"total_steals\",\n",
    "        \"total_oreb\": \"total_reb_off\",\n",
    "        \"total_dreb\": \"total_reb_def\",\n",
    "    })\n",
    "\n",
    "    # Game Score (season total) + per-36\n",
    "    merged[\"season_game_score_total\"] = (\n",
    "        merged[\"total_points\"]\n",
    "        + 0.4 * merged[\"total_fgm\"]\n",
    "        - 0.7 * merged[\"total_fga\"]\n",
    "        - 0.4 * (merged[\"total_fta\"] - merged[\"total_ftm\"])\n",
    "        + 0.7 * merged[\"total_reb_off\"]\n",
    "        + 0.3 * merged[\"total_reb_def\"]\n",
    "        + merged[\"total_steals\"]\n",
    "        + 0.7 * merged[\"total_assists\"]\n",
    "        + 0.7 * merged[\"total_blocks\"]\n",
    "        - 0.4 * merged[\"total_pf\"]\n",
    "        - merged[\"total_tov\"]\n",
    "    )\n",
    "    merged[\"game_score_per36\"] = _safe_div(merged[\"season_game_score_total\"] * 36.0, merged[\"total_minutes\"])\n",
    "\n",
    "    # minutes gate AFTER derived metrics\n",
    "    before = len(merged)\n",
    "    merged = merged.loc[merged[\"total_minutes\"] >= minutes_total_minimum_per_season].copy()\n",
    "    after = len(merged)\n",
    "    if before != after:\n",
    "        print(f\"[GATE] Applied {minutes_total_minimum_per_season} min gate at player-season: \"\n",
    "              f\"dropped {before - after:,} row(s).\")\n",
    "\n",
    "    return merged\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Ranking cores\n",
    "# ---------------------------\n",
    "\n",
    "# optional explainability bits: break PIE into pos/neg buckets + normalized shares\n",
    "# helps reason about *why* a season sorted where it did\n",
    "def _compute_pie_component_context(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Optional PIE component shares for extended context.\"\"\"\n",
    "    out = df.copy()\n",
    "    required = [\n",
    "        \"total_points\",\"total_fgm\",\"total_ftm\",\"total_fga\",\"total_fta\",\n",
    "        \"total_reb_def\",\"total_reb_off\",\"total_assists\",\"total_steals\",\"total_blocks\",\n",
    "        \"total_pf\",\"total_tov\",\n",
    "        \"season_pie_num\",\"season_pie_den\",\n",
    "    ]\n",
    "    missing = [c for c in required if c not in out.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"[context] missing columns: {missing}\")\n",
    "\n",
    "    pie_pos = (\n",
    "        out[\"total_points\"] + out[\"total_fgm\"] + out[\"total_ftm\"]\n",
    "        + out[\"total_reb_def\"] + 0.5*out[\"total_reb_off\"]\n",
    "        + out[\"total_assists\"] + out[\"total_steals\"] + 0.5*out[\"total_blocks\"]\n",
    "    )\n",
    "    pie_neg = (out[\"total_fga\"] + out[\"total_fta\"] + out[\"total_pf\"] + out[\"total_tov\"])\n",
    "    pie_base = pie_pos + pie_neg\n",
    "\n",
    "    def _share(x: pd.Series) -> pd.Series:\n",
    "        den = pie_base.replace(0, np.nan)\n",
    "        return (x / den).fillna(0.0)\n",
    "\n",
    "    out[\"pie_pos\"] = pie_pos\n",
    "    out[\"pie_neg\"] = pie_neg\n",
    "    out[\"pie_base\"] = pie_base\n",
    "\n",
    "    out[\"share_points\"] = _share(out[\"total_points\"])\n",
    "    out[\"share_makes\"]  = _share(out[\"total_fgm\"] + out[\"total_ftm\"])\n",
    "    out[\"share_reb\"]    = _share(out[\"total_reb_def\"] + 0.5*out[\"total_reb_off\"])\n",
    "    out[\"share_ast\"]    = _share(out[\"total_assists\"])\n",
    "    out[\"share_stl\"]    = _share(out[\"total_steals\"])\n",
    "    out[\"share_blk\"]    = _share(0.5*out[\"total_blocks\"])\n",
    "    out[\"share_fga\"]    = _share(out[\"total_fga\"])\n",
    "    out[\"share_fta\"]    = _share(out[\"total_fta\"])\n",
    "    out[\"share_pf\"]     = _share(out[\"total_pf\"])\n",
    "    out[\"share_tov\"]    = _share(out[\"total_tov\"])\n",
    "    return out\n",
    "\n",
    "# gs36 sort: main metric desc, then minutes, then games/ts% depending on mode\n",
    "# include_context=True adds the exact per-36 inputs used by Game Score\n",
    "def rank_seasons_by_gs36(\n",
    "    player_season_df: pd.DataFrame,\n",
    "    top_n: int = 10,\n",
    "    middle_n: int = 10,\n",
    "    bottom_n: int = 10,\n",
    "    tie_breaker: str = \"python\",  # \"python\" or \"duckdb\" (ts_pct as 3rd tie-breaker)\n",
    "    include_context: bool = False,\n",
    ") -> dict[str, pd.DataFrame]:\n",
    "    df = player_season_df.copy()\n",
    "    needed = [\"personId\",\"player_name\",\"season\",\"game_score_per36\",\"total_minutes\",\"games_played\"]\n",
    "    missing = [c for c in needed if c not in df.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing columns for GS36 ranking: {missing}\")\n",
    "\n",
    "    if tie_breaker == \"duckdb\":\n",
    "        if \"ts_pct\" not in df.columns:\n",
    "            df[\"ts_pct\"] = np.nan\n",
    "        sort_cols_top = [(\"game_score_per36\", False), (\"total_minutes\", False), (\"ts_pct\", False), (\"player_name\", True)]\n",
    "    else:\n",
    "        sort_cols_top = [(\"game_score_per36\", False), (\"total_minutes\", False), (\"games_played\", False), (\"player_name\", True)]\n",
    "\n",
    "    top = df.sort_values([c for c,_ in sort_cols_top],\n",
    "                         ascending=[a for _,a in sort_cols_top]).head(top_n).copy()\n",
    "    bottom = df.sort_values(\n",
    "        [sort_cols_top[0][0], sort_cols_top[1][0], sort_cols_top[2][0], sort_cols_top[3][0]],\n",
    "        ascending=[True, False, False, True]\n",
    "    ).head(bottom_n).copy()\n",
    "\n",
    "    med = df[\"game_score_per36\"].median(skipna=True)\n",
    "    df[\"dist_to_median\"] = (df[\"game_score_per36\"] - med).abs()\n",
    "    middle = df.sort_values(\n",
    "        [\"dist_to_median\"] + [c for c,_ in sort_cols_top],\n",
    "        ascending=[True] + [a for _,a in sort_cols_top]\n",
    "    ).head(middle_n).drop(columns=[\"dist_to_median\"]).copy()\n",
    "\n",
    "    def _minimal(block: pd.DataFrame) -> pd.DataFrame:\n",
    "        out = block.loc[:, [\"player_name\",\"season\",\"game_score_per36\",\"games_played\",\"total_minutes\"]].copy()\n",
    "        out[\"game_score_per36\"] = pd.to_numeric(out[\"game_score_per36\"], errors=\"coerce\").round(6)\n",
    "        out[\"total_minutes\"] = pd.to_numeric(out[\"total_minutes\"], errors=\"coerce\").round(1)\n",
    "        return out.reset_index(drop=True)\n",
    "\n",
    "    if not include_context:\n",
    "        return {\"top\": _minimal(top), \"middle\": _minimal(middle), \"bottom\": _minimal(bottom)}\n",
    "\n",
    "    GS_INPUT_PER36 = [\n",
    "        \"pts_per36\", \"fgm_per36\", \"fga_per36\", \"ftm_per36\", \"fta_per36\",\n",
    "        \"oreb_per36\", \"dreb_per36\", \"stl_per36\", \"ast_per36\", \"blk_per36\",\n",
    "        \"pf_per36\", \"tov_per36\",\n",
    "    ]\n",
    "    def _with_gs_context(block: pd.DataFrame) -> pd.DataFrame:\n",
    "        out = block.reset_index(drop=True).copy()\n",
    "        out.insert(0, \"Rank\", range(1, len(out) + 1))\n",
    "        selected = [\"Rank\",\"player_name\",\"season\",\"game_score_per36\"] + [c for c in GS_INPUT_PER36 if c in out.columns]\n",
    "        out[\"game_score_per36\"] = pd.to_numeric(out[\"game_score_per36\"], errors=\"coerce\").round(6)\n",
    "        for c in GS_INPUT_PER36:\n",
    "            if c in out.columns:\n",
    "                out[c] = pd.to_numeric(out[c], errors=\"coerce\").round(3)\n",
    "        return out.loc[:, selected]\n",
    "\n",
    "    return {\"top\": _with_gs_context(top), \"middle\": _with_gs_context(middle), \"bottom\": _with_gs_context(bottom)}\n",
    "\n",
    "# pie sort: season_pie desc, then minutes, then games or ts% (mode)\n",
    "# include_context=True w/ pie_only_context shows only the inputs + the PIE parts\n",
    "def rank_seasons_by_pie(\n",
    "    player_season_df: pd.DataFrame,\n",
    "    top_n: int = 10,\n",
    "    middle_n: int = 10,\n",
    "    bottom_n: int = 10,\n",
    "    tie_breaker: str = \"python\",\n",
    "    include_context: bool = False,\n",
    "    context_cols: list[str] | None = None,\n",
    "    round_map: dict[str, int] | None = None,\n",
    "    pie_only_context: bool = True,\n",
    ") -> dict[str, pd.DataFrame]:\n",
    "    df = player_season_df.copy()\n",
    "    needed = [\"personId\",\"player_name\",\"season\",\"season_pie\",\"total_minutes\",\"games_played\"]\n",
    "    missing = [c for c in needed if c not in df.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing columns for PIE ranking: {missing}\")\n",
    "\n",
    "    if \"season_pie_pct\" not in df.columns:\n",
    "        df[\"season_pie_pct\"] = 100.0 * df[\"season_pie\"]\n",
    "\n",
    "    if tie_breaker == \"duckdb\":\n",
    "        if \"ts_pct\" not in df.columns:\n",
    "            df[\"ts_pct\"] = np.nan\n",
    "        sort_cols_top = [(\"season_pie\", False), (\"total_minutes\", False), (\"ts_pct\", False), (\"player_name\", True)]\n",
    "    else:\n",
    "        sort_cols_top = [(\"season_pie\", False), (\"total_minutes\", False), (\"games_played\", False), (\"player_name\", True)]\n",
    "\n",
    "    top = df.sort_values([c for c,_ in sort_cols_top],\n",
    "                         ascending=[a for _,a in sort_cols_top]).head(top_n).copy()\n",
    "    bottom = df.sort_values(\n",
    "        [sort_cols_top[0][0], sort_cols_top[1][0], sort_cols_top[2][0], sort_cols_top[3][0]],\n",
    "        ascending=[True, False, False, True]\n",
    "    ).head(bottom_n).copy()\n",
    "\n",
    "    med = df[\"season_pie\"].median(skipna=True)\n",
    "    df[\"dist_to_median\"] = (df[\"season_pie\"] - med).abs()\n",
    "    middle = df.sort_values(\n",
    "        [\"dist_to_median\"] + [c for c,_ in sort_cols_top],\n",
    "        ascending=[True] + [a for _,a in sort_cols_top]\n",
    "    ).head(middle_n).drop(columns=[\"dist_to_median\"]).copy()\n",
    "\n",
    "    def _minimal(block: pd.DataFrame) -> pd.DataFrame:\n",
    "        out = block.loc[:, [\"player_name\",\"season\",\"season_pie\",\"season_pie_pct\",\"games_played\",\"total_minutes\"]].copy()\n",
    "        out[\"season_pie\"] = pd.to_numeric(out[\"season_pie\"], errors=\"coerce\").round(6)\n",
    "        out[\"season_pie_pct\"] = pd.to_numeric(out[\"season_pie_pct\"], errors=\"coerce\").round(2)\n",
    "        out[\"total_minutes\"] = pd.to_numeric(out[\"total_minutes\"], errors=\"coerce\").round(1)\n",
    "        return out.reset_index(drop=True)\n",
    "\n",
    "    if not include_context:\n",
    "        return {\"top\": _minimal(top), \"middle\": _minimal(middle), \"bottom\": _minimal(bottom)}\n",
    "\n",
    "    if pie_only_context:\n",
    "        PIE_INPUT_TOTALS = [\n",
    "            \"total_points\",\n",
    "            \"total_fgm\", \"total_ftm\", \"total_fga\", \"total_fta\",\n",
    "            \"total_reb_def\", \"total_reb_off\",\n",
    "            \"total_assists\", \"total_steals\", \"total_blocks\",\n",
    "            \"total_pf\", \"total_tov\",\n",
    "            \"total_minutes\"\n",
    "        ]\n",
    "        PIE_METRICS = [\"season_pie\", \"season_pie_pct\", \"season_pie_num\", \"season_pie_den\"]\n",
    "\n",
    "        def _pie_only(block: pd.DataFrame) -> pd.DataFrame:\n",
    "            out = block.reset_index(drop=True).copy()\n",
    "            out.insert(0, \"Rank\", range(1, len(out) + 1))\n",
    "            selected = [\"Rank\", \"player_name\", \"season\"] \\\n",
    "                       + [c for c in PIE_METRICS if c in out.columns] \\\n",
    "                       + [c for c in PIE_INPUT_TOTALS if c in out.columns]\n",
    "            out[\"season_pie\"] = pd.to_numeric(out[\"season_pie\"], errors=\"coerce\").round(6)\n",
    "            out[\"season_pie_pct\"] = pd.to_numeric(out[\"season_pie_pct\"], errors=\"coerce\").round(2)\n",
    "            return out.loc[:, selected]\n",
    "\n",
    "        return {\"top\": _pie_only(top), \"middle\": _pie_only(middle), \"bottom\": _pie_only(bottom)}\n",
    "\n",
    "    # extended context (unchanged except we expose season_pie_pct if present)\n",
    "    if context_cols is None:\n",
    "        context_cols = [\n",
    "            \"season_pie_num\",\"season_pie_den\",\"pie_pos\",\"pie_neg\",\"pie_base\",\n",
    "            \"share_points\",\"share_makes\",\"share_reb\",\"share_ast\",\"share_stl\",\"share_blk\",\n",
    "            \"share_fga\",\"share_fta\",\"share_pf\",\"share_tov\",\"season_pie_pct\"\n",
    "        ]\n",
    "    df_ctx = _compute_pie_component_context(df)\n",
    "\n",
    "    def _join_keys(block_cols, ctx_cols):\n",
    "        if all(k in block_cols and k in ctx_cols for k in (\"personId\",\"season\")):\n",
    "            return [\"personId\",\"season\"]\n",
    "        if all(k in block_cols and k in ctx_cols for k in (\"player_name\",\"season\")):\n",
    "            return [\"player_name\",\"season\"]\n",
    "        raise KeyError(\"No suitable join keys found; expected (personId, season) or (player_name, season).\")\n",
    "\n",
    "    def _with_context(block: pd.DataFrame) -> pd.DataFrame:\n",
    "        join_keys = _join_keys(block.columns, df_ctx.columns)\n",
    "        base_cols = set(block.columns)\n",
    "        cand_cols = [c for c in context_cols if c in df_ctx.columns or c in block.columns]\n",
    "        # prefer from block (already has season_pie_pct)\n",
    "        have = [c for c in cand_cols if c in block.columns]\n",
    "        need = [c for c in cand_cols if c not in block.columns and c in df_ctx.columns]\n",
    "        merged = block.merge(df_ctx[join_keys + need].drop_duplicates(), on=join_keys, how=\"left\")\n",
    "        merged = merged.reset_index(drop=True)\n",
    "        merged.insert(0, \"Rank\", range(1, len(merged) + 1))\n",
    "\n",
    "        round_map_local = round_map or {\n",
    "            \"season_pie\": 6, \"season_pie_pct\": 2,\n",
    "            \"share_points\": 3, \"share_makes\": 3, \"share_reb\": 3, \"share_ast\": 3, \"share_stl\": 3, \"share_blk\": 3,\n",
    "            \"share_fga\": 3, \"share_fta\": 3, \"share_pf\": 3, \"share_tov\": 3,\n",
    "        }\n",
    "        for col, nd in round_map_local.items():\n",
    "            if col in merged.columns:\n",
    "                merged[col] = pd.to_numeric(merged[col], errors=\"coerce\").round(nd)\n",
    "\n",
    "        keep = [\"Rank\"] + list(block.columns) + [c for c in need if c in merged.columns]\n",
    "        return merged.loc[:, keep]\n",
    "\n",
    "    return {\"top\": _with_context(top), \"middle\": _with_context(middle), \"bottom\": _with_context(bottom)}\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Generic printers/formatters/exporters\n",
    "# ---------------------------\n",
    "\n",
    "# helper wrapper: choose metric + include context frames\n",
    "def build_rankings_with_context_generic(\n",
    "    player_season_df: pd.DataFrame,\n",
    "    metric: str = \"pie\",  # \"pie\" or \"gs36\"\n",
    "    top_n: int = 10, middle_n: int = 10, bottom_n: int = 10,\n",
    "    tie_breaker: str = \"python\",\n",
    ") -> dict[str, pd.DataFrame]:\n",
    "    if metric == \"pie\":\n",
    "        return rank_seasons_by_pie(\n",
    "            player_season_df, top_n=top_n, middle_n=middle_n, bottom_n=bottom_n,\n",
    "            tie_breaker=tie_breaker, include_context=True, pie_only_context=True\n",
    "        )\n",
    "    elif metric == \"gs36\":\n",
    "        return rank_seasons_by_gs36(\n",
    "            player_season_df, top_n=top_n, middle_n=middle_n, bottom_n=bottom_n,\n",
    "            tie_breaker=tie_breaker, include_context=True\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"metric must be 'pie' or 'gs36'\")\n",
    "\n",
    "# print the context frames in a readable way (meh but handy)\n",
    "def print_rankings_with_context_generic(\n",
    "    player_season_df: pd.DataFrame,\n",
    "    metric: str = \"pie\",\n",
    "    **kwargs,\n",
    ") -> dict[str, pd.DataFrame]:\n",
    "    ctx = build_rankings_with_context_generic(player_season_df, metric=metric, **kwargs)\n",
    "    label = \"PIE\" if metric == \"pie\" else \"Game Score per 36\"\n",
    "    print(f\"\\n=== Top 10 seasons by {label} (with context) ===\")\n",
    "    print(ctx[\"top\"].to_string(index=False))\n",
    "    print(f\"\\n=== Middle 10 seasons by {label} (with context) ===\")\n",
    "    print(ctx[\"middle\"].to_string(index=False))\n",
    "    print(f\"\\n=== Bottom 10 seasons by {label} (with context) ===\")\n",
    "    print(ctx[\"bottom\"].to_string(index=False))\n",
    "    return ctx\n",
    "\n",
    "# squash to submission columns (Rank, Player, Season) for each bucket\n",
    "def format_rankings_for_submission_generic(\n",
    "    player_season_df: pd.DataFrame,\n",
    "    metric: str = \"pie\",\n",
    "    top_n: int = 10,\n",
    "    middle_n: int = 10,\n",
    "    bottom_n: int = 10,\n",
    "    tie_breaker: str = \"python\",\n",
    ") -> dict[str, pd.DataFrame]:\n",
    "    if metric == \"pie\":\n",
    "        ranks = rank_seasons_by_pie(player_season_df, top_n=top_n, middle_n=middle_n, bottom_n=bottom_n, tie_breaker=tie_breaker)\n",
    "        title = \"PIE\"\n",
    "    elif metric == \"gs36\":\n",
    "        ranks = rank_seasons_by_gs36(player_season_df, top_n=top_n, middle_n=middle_n, bottom_n=bottom_n, tie_breaker=tie_breaker)\n",
    "        title = \"Game Score per 36\"\n",
    "    else:\n",
    "        raise ValueError(\"metric must be 'pie' or 'gs36'\")\n",
    "\n",
    "    def _fmt(df: pd.DataFrame, rank_start: int = 1) -> pd.DataFrame:\n",
    "        out = df[[\"player_name\",\"season\"]].copy()\n",
    "        out.insert(0, \"Rank\", range(rank_start, rank_start + len(out)))\n",
    "        out = out.rename(columns={\"player_name\":\"Player\",\"season\":\"Season\"})\n",
    "        return out\n",
    "\n",
    "    out = {\n",
    "        \"top\": _fmt(ranks[\"top\"], rank_start=1),\n",
    "        \"middle\": _fmt(ranks[\"middle\"], rank_start=1),\n",
    "        \"bottom\": _fmt(ranks[\"bottom\"], rank_start=1),\n",
    "    }\n",
    "    out[\"__title__\"] = title  # carry label for printing\n",
    "    return out\n",
    "\n",
    "# print-only version for the submission tables\n",
    "def print_rankings_for_submission_generic(\n",
    "    player_season_df: pd.DataFrame,\n",
    "    metric: str = \"pie\",\n",
    "    top_n: int = 10,\n",
    "    middle_n: int = 10,\n",
    "    bottom_n: int = 10,\n",
    "    tie_breaker: str = \"python\",\n",
    ") -> dict[str, pd.DataFrame]:\n",
    "    sub = format_rankings_for_submission_generic(\n",
    "        player_season_df, metric=metric, top_n=top_n, middle_n=middle_n, bottom_n=bottom_n, tie_breaker=tie_breaker\n",
    "    )\n",
    "    label = sub.pop(\"__title__\")\n",
    "    print(f\"\\n=== Top 10 seasons by {label} ===\")\n",
    "    print(sub[\"top\"].to_string(index=False))\n",
    "    print(f\"\\n=== Middle 10 seasons by {label} (around median) ===\")\n",
    "    print(sub[\"middle\"].to_string(index=False))\n",
    "    print(f\"\\n=== Bottom 10 seasons by {label} ===\")\n",
    "    print(sub[\"bottom\"].to_string(index=False))\n",
    "    return sub\n",
    "\n",
    "# dump the context frames to csvs side-by-side (top/mid/bottom)\n",
    "def export_context_rankings(\n",
    "    ctx_ranks: dict[str, pd.DataFrame],\n",
    "    output_dir: Path | str,\n",
    "    filename_prefix: str,\n",
    ") -> None:\n",
    "    outdir = Path(output_dir)\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "    for k, df in ctx_ranks.items():\n",
    "        path = outdir / f\"{filename_prefix}_{k}.csv\"\n",
    "        df.to_csv(path, index=False)\n",
    "        print(f\"[EXPORT] wrote {k} → {path}\")\n",
    "\n",
    "# plain .txt logs for the submission tables (easier to eyeball in diffs)\n",
    "def export_submission_txt(\n",
    "    submission_ranks: dict[str, pd.DataFrame],\n",
    "    output_dir: Path | str | None,\n",
    "    filename_prefix: str,\n",
    "    heading_label: str,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Writes three files: {prefix}_top.txt, {prefix}_middle.txt, {prefix}_bottom.txt\n",
    "    with a heading line that includes `heading_label`.\n",
    "    \"\"\"\n",
    "    outdir = Path(\".\") if output_dir is None else Path(output_dir)\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "    for rank_type, df in submission_ranks.items():\n",
    "        filename = f\"{filename_prefix}_{rank_type}.txt\"\n",
    "        filepath = outdir / filename\n",
    "        with open(filepath, \"w\") as f:\n",
    "            f.write(f\"=== {rank_type.title()} 10 seasons by {heading_label} ===\\n\")\n",
    "            f.write(df.to_string(index=False))\n",
    "            f.write(\"\\n\")\n",
    "        print(f\"[EXPORT] Saved {rank_type} rankings to: {filepath}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Kaggle loader\n",
    "# ---------------------------\n",
    "\n",
    "# thin wrapper around kagglehub to get named tables as pandas\n",
    "# raises with a readable list if something didn't load\n",
    "def load_nba_csv_tables(\n",
    "    table_names: Iterable[str] = IMPORTANT_TABLES,\n",
    "    dataset: str = KAGGLE_DATASET,\n",
    ") -> Dict[str, pd.DataFrame]:\n",
    "    result: Dict[str, pd.DataFrame] = {}\n",
    "    missing_map = {}\n",
    "\n",
    "    for t in table_names:\n",
    "        csv_name = KAGGLE_TABLE_TO_CSV.get(t)\n",
    "        if not csv_name:\n",
    "            missing_map[t] = \"No CSV mapping in KAGGLE_TABLE_TO_CSV\"\n",
    "            continue\n",
    "        try:\n",
    "            df = kh.dataset_load(\n",
    "                KDA.PANDAS, dataset, csv_name, pandas_kwargs={\"low_memory\": False}\n",
    "            )\n",
    "            result[t] = df\n",
    "            print(f\"[INFO] Loaded via KaggleHub/PANDAS: {csv_name} -> '{t}' (rows={len(df):,})\")\n",
    "        except Exception as e:\n",
    "            missing_map[t] = str(e)\n",
    "\n",
    "    if missing_map:\n",
    "        missing_str = \"\\n\".join([f\"- {t}: {why}\" for t, why in missing_map.items()])\n",
    "        raise RuntimeError(\n",
    "            f\"Some tables could not be loaded.\\nDataset: {dataset}\\nIssues:\\n{missing_str}\"\n",
    "        )\n",
    "    return result\n",
    "\n",
    "# convenience one-shot: load, filter, compute game-level PIE\n",
    "# note: minutes gate deferred to season aggregation step\n",
    "def build_player_game_table_python(\n",
    "    start_season: int = CFG_START_SEASON,\n",
    "    season_type: str = CFG_SEASON_TYPE,\n",
    "    minutes_total_minimum_per_season: int = CFG_MIN_SEASON_MINUTES,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads via KaggleHub → applies basic criteria in Python → computes PIE at game level.\n",
    "    IMPORTANT: Defer minutes gate to player-season (avoid biasing PIE denominators).\n",
    "    \"\"\"\n",
    "    dfs = load_nba_csv_tables([\"PlayerStatistics\",\"TeamStatistics\"])\n",
    "    _, ps_basic, _ = enforce_criteria_python(\n",
    "        None,\n",
    "        dfs[\"PlayerStatistics\"],\n",
    "        dfs[\"TeamStatistics\"],\n",
    "        start_season=start_season,\n",
    "        season_type=season_type,\n",
    "        minutes_total_minimum_per_season=minutes_total_minimum_per_season,\n",
    "        defer_minutes_gate=True,\n",
    "    )\n",
    "    ps_pie = compute_player_game_pie(ps_basic)\n",
    "    return ps_pie\n",
    "\n",
    "# ---------------------------\n",
    "# Main\n",
    "# ---------------------------\n",
    "\n",
    "# basic CLI-ish entrypoint; prints shapes + writes out submission/context artifacts\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== NBA Rankings (PIE & GS36) — Submission + Context ===\")\n",
    "\n",
    "    # 1) Player-game with PIE\n",
    "    print(\"\\n1. Building player-game table with PIE...\")\n",
    "    try:\n",
    "        pg = build_player_game_table_python(\n",
    "            start_season=CFG_START_SEASON,\n",
    "            season_type=CFG_SEASON_TYPE,\n",
    "            minutes_total_minimum_per_season=CFG_MIN_SEASON_MINUTES,\n",
    "        )\n",
    "        print(f\"[SUCCESS] player_game_with_pie shape: {pg.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to build player-game table: {e}\")\n",
    "        pg = None\n",
    "\n",
    "    # 2) Player-season + rankings\n",
    "    if pg is not None:\n",
    "        print(\"\\n2. Building player-season table and generating rankings...\")\n",
    "        try:\n",
    "            dfs = load_nba_csv_tables([\"TeamStatistics\"])\n",
    "            season_df = build_player_season_table_python(pg, dfs[\"TeamStatistics\"])\n",
    "            print(f\"[SUCCESS] player_season shape: {season_df.shape}\")\n",
    "\n",
    "            # Persist ML dataset\n",
    "            season_df.to_parquet(CFG.ml_dataset_path, index=False)\n",
    "\n",
    "            # ---- PIE ----\n",
    "            # tie_breaker='duckdb' uses ts_pct as 3rd tie-breaker to match spec\n",
    "            pie_submission = print_rankings_for_submission_generic(\n",
    "                season_df, metric=\"pie\", tie_breaker=\"duckdb\"\n",
    "            )\n",
    "            export_submission_txt(\n",
    "                pie_submission, output_dir=CFG.processed_dir,\n",
    "                filename_prefix=\"nba_pie_rankings\", heading_label=\"PIE\"\n",
    "            )\n",
    "\n",
    "            pie_ctx = print_rankings_with_context_generic(\n",
    "                season_df, metric=\"pie\", top_n=10, middle_n=10, bottom_n=10, tie_breaker=\"duckdb\"\n",
    "            )\n",
    "            export_context_rankings(\n",
    "                pie_ctx, output_dir=CFG.processed_dir, filename_prefix=\"nba_pie_rankings_context\"\n",
    "            )\n",
    "\n",
    "            # ---- GS36 ----\n",
    "            gs36_submission = print_rankings_for_submission_generic(\n",
    "                season_df, metric=\"gs36\"\n",
    "            )\n",
    "            export_submission_txt(\n",
    "                gs36_submission, output_dir=CFG.processed_dir,\n",
    "                filename_prefix=\"nba_gamescore36_rankings\", heading_label=\"Game Score per 36\"\n",
    "            )\n",
    "\n",
    "            gs36_ctx = print_rankings_with_context_generic(\n",
    "                season_df, metric=\"gs36\", top_n=10, middle_n=10, bottom_n=10\n",
    "            )\n",
    "            export_context_rankings(\n",
    "                gs36_ctx, output_dir=CFG.processed_dir, filename_prefix=\"nba_gamescore36_rankings_context\"\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to generate rankings: {e}\")\n",
    "\n",
    "    print(\"\\n=== Complete ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87c643ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/heat_data_scientist_2025/data/load_data_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/heat_data_scientist_2025/data/load_data_utils.py\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def load_data_optimized(\n",
    "    DATA_PATH: str,\n",
    "    debug: bool = False,\n",
    "    drop_null_rows: bool = False,\n",
    "    drop_null_how: str = 'any',  # 'any' or 'all'\n",
    "    drop_null_subset: list | None = None,  # list of column names or None for all columns\n",
    "    use_sample: bool = False,\n",
    "    sample_size: int = 1000,\n",
    "):\n",
    "    \"\"\"Load data with performance optimizations and enhanced debug diagnostics.\n",
    "\n",
    "    Parameters:\n",
    "    - DATA_PATH: Path to the parquet file.\n",
    "    - debug: If True, prints detailed dataset diagnostics.\n",
    "    - drop_null_rows: If True, drops rows based on null criteria.\n",
    "    - drop_null_how: 'any' to drop rows with any nulls, 'all' to drop rows with all nulls.\n",
    "    - drop_null_subset: List of columns to consider when dropping nulls; defaults to all.\n",
    "\n",
    "    Returns:\n",
    "    - df: Loaded (and optionally filtered) DataFrame.\n",
    "    \"\"\"\n",
    "    print(\"Loading data for enhanced comprehensive EDA...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 1. Load data\n",
    "    if use_sample:\n",
    "        print(f\"⚡ Using sample data (n={sample_size}) instead of real parquet.\")\n",
    "        len_df = sample_size\n",
    "        df = pd.read_parquet(DATA_PATH)\n",
    "        #take only the len of the data\n",
    "        df = df.head(len_df)\n",
    "    else:\n",
    "        if DATA_PATH is None:\n",
    "            raise ValueError(\"DATA_PATH must be provided when not using sample data.\")\n",
    "        df = pd.read_parquet(DATA_PATH)\n",
    "\n",
    "\n",
    "    # 2. Drop null rows if requested\n",
    "    if drop_null_rows:\n",
    "        before = len(df)\n",
    "        # Determine which subset to use for dropna\n",
    "        subset_desc = \"all columns\" if drop_null_subset is None else f\"subset={drop_null_subset}\"\n",
    "        print(f\"→ Applying null dropping: how='{drop_null_how}', {subset_desc}\")\n",
    "        if drop_null_subset is None:\n",
    "            df = df.dropna(how=drop_null_how)\n",
    "        else:\n",
    "            # Defensive: ensure provided columns exist (warn if some missing)\n",
    "            missing_cols = [c for c in drop_null_subset if c.upper() not in df.columns]\n",
    "            if missing_cols:\n",
    "                print(f\"⚠️ Warning: drop_null_subset columns not found in dataframe and will be ignored: {missing_cols}\")\n",
    "            valid_subset = [c.upper() for c in drop_null_subset if c.upper() in df.columns]\n",
    "            df = df.dropna(how=drop_null_how, subset=valid_subset if valid_subset else None)\n",
    "        dropped = before - len(df)\n",
    "        print(f\"✓ Dropped {dropped:,} rows by null criteria (how='{drop_null_how}', subset={drop_null_subset}); remaining {len(df):,} rows\")\n",
    "\n",
    "    # 3. Debug diagnostics\n",
    "    if debug:\n",
    "        print(\"========== Dataset Debug Details ============\")\n",
    "        print(f\"Total rows       : {df.shape[0]:,}\")\n",
    "        print(f\"Total columns    : {df.shape[1]:,}\")\n",
    "        print(f\"Columns          : {df.columns.tolist()}\")\n",
    "\n",
    "        total = len(df)\n",
    "        null_counts = df.isnull().sum()\n",
    "        non_null_counts = total - null_counts\n",
    "        null_percent = (null_counts / total) * 100\n",
    "        dtype_info = df.dtypes\n",
    "\n",
    "        null_summary = pd.DataFrame({\n",
    "            'dtype'          : dtype_info,\n",
    "            'null_count'     : null_counts,\n",
    "            'non_null_count' : non_null_counts,\n",
    "            'null_percent'   : null_percent\n",
    "        }).sort_values(by='null_percent', ascending=False)\n",
    "\n",
    "        pd.set_option('display.max_rows', None)\n",
    "        print(\"---- Nulls Summary (per column) ----\")\n",
    "        print(null_summary)\n",
    "\n",
    "    load_time = time.time() - start_time\n",
    "    print(f\"✓ Dataset loaded: {df.shape[0]:,} rows × {df.shape[1]:,} columns in {load_time:.2f}s\")\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from src.heat_data_scientist_2025.utils.config import CFG\n",
    "    df = load_data_optimized(\n",
    "        CFG.ml_dataset_path,\n",
    "        debug=True,\n",
    "        # use_sample=True,\n",
    "        # drop_null_rows=True,\n",
    "        # drop_null_subset=['AAV']\n",
    "    )\n",
    "    print(df.columns.tolist())\n",
    "    print(df.head())\n",
    "    print(df.shape)\n",
    "    # unique values for season\n",
    "    print(df['season'].unique())\n",
    "    \n",
    "\n",
    "    # df = load_data_optimized(\n",
    "    #     FINAL_DATA_PATH,\n",
    "    #     debug=True,\n",
    "    #     use_sample=True,\n",
    "    # )\n",
    "    # print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a6ae34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/heat_data_scientist_2025/data/feature_engineering.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/heat_data_scientist_2025/data/feature_engineering.py\n",
    "\"\"\"\n",
    "Current columns:\n",
    "Index(['personId', 'player_name', 'season', 'games_played', 'total_minutes',\n",
    "       'total_points', 'total_assists', 'total_rebounds', 'total_reb_off',\n",
    "       'total_reb_def', 'total_blocks', 'total_steals', 'total_pf',\n",
    "       'total_tov', 'total_fga', 'total_fgm', 'total_fta', 'total_ftm',\n",
    "       'total_3pa', 'total_3pm', 'total_plus_minus', 'avg_plus_minus', 'wins',\n",
    "       'home_games', 'season_pie_num', 'season_pie_den', 'fg_pct', 'fg3_pct',\n",
    "       'ft_pct', 'ts_pct', 'season_pie', 'pts_per36', 'ast_per36', 'reb_per36',\n",
    "       'usage_per_min', 'efficiency_per_game', 'win_pct', 'home_games_pct',\n",
    "       'teamCity', 'teamName', 'team_win_pct_final'],\n",
    "      dtype='object')\n",
    "\n",
    "\n",
    "Feature engineering for NBA player-season data.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from __future__ import annotations\n",
    "from typing import List, Tuple, Optional, Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "# quick check: makes sure we have the columns we need\n",
    "def require_columns(df: pd.DataFrame, cols: List[str], context: str) -> None:\n",
    "    \"\"\"Check if required columns exist, throw error if missing.\"\"\"\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing columns for {context}: {missing}\")\n",
    "\n",
    "\n",
    "# helper: finds first column that actually exists in the data  \n",
    "def _first_present(df: pd.DataFrame, candidates: List[str]) -> Optional[str]:\n",
    "    \"\"\"Return first column name that exists in df, else None.\"\"\"\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "\n",
    "# helper: marks each players first season row (used to verify lag nulls)\n",
    "def compute_first_season_mask(df: pd.DataFrame, season_col: str = \"season_start_year\") -> pd.Series:\n",
    "    \"\"\"Return boolean mask for each player's earliest season.\"\"\"\n",
    "    require_columns(df, [\"personId\", season_col], \"compute_first_season_mask\")\n",
    "    idx = (df.sort_values([\"personId\", season_col])\n",
    "           .groupby(\"personId\", group_keys=False)\n",
    "           .head(1)).index\n",
    "    return df.index.isin(idx)\n",
    "\n",
    "\n",
    "# parse season string like '2023-24' into year 2023\n",
    "def add_season_start_year(df: pd.DataFrame) -> Tuple[pd.DataFrame, List[str]]:\n",
    "    \"\"\"Extract numeric start year from season string like 'YYYY-YY'.\"\"\"\n",
    "    out = df.copy()\n",
    "    if \"season\" not in out.columns:\n",
    "        raise ValueError(\"Need 'season' column\")\n",
    "    \n",
    "    out[\"season_start_year\"] = (\n",
    "        out[\"season\"].astype(str).str.extract(r\"(\\d{4})\")[0].astype(int)\n",
    "    )\n",
    "    return out, [\"season_start_year\"]\n",
    "\n",
    "\n",
    "# experience stuff - years in league and rough groupings\n",
    "def add_experience_features(df: pd.DataFrame) -> Tuple[pd.DataFrame, List[str]]:\n",
    "    \"\"\"Add experience features from draft year if available.\"\"\"\n",
    "    out = df.copy()\n",
    "    created: List[str] = []\n",
    "\n",
    "    # cumulative games played (simple running total)\n",
    "    if \"personId\" in out.columns and \"games_played\" in out.columns:\n",
    "        out = out.sort_values([\"personId\", \"season_start_year\"])\n",
    "        out[\"games_played_total\"] = out.groupby(\"personId\")[\"games_played\"].cumsum()\n",
    "        created.append(\"games_played_total\")\n",
    "\n",
    "    # years since draft (if we have draft year)\n",
    "    if \"draftYear\" in out.columns and \"season_start_year\" in out.columns:\n",
    "        out[\"years_experience\"] = (out[\"season_start_year\"] - out[\"draftYear\"]).clip(lower=0)\n",
    "        \n",
    "        # rough experience buckets\n",
    "        def exp_bucket(exp):\n",
    "            if pd.isna(exp): return \"Unknown\"\n",
    "            if exp <= 2: return \"Rookie/Sophomore\"  \n",
    "            if exp <= 5: return \"Young Player\"\n",
    "            if exp <= 9: return \"Prime Years\"\n",
    "            if exp <= 15: return \"Veteran\"\n",
    "            return \"Elder Statesman\"\n",
    "        \n",
    "        out[\"experience_bucket\"] = out[\"years_experience\"].apply(exp_bucket)\n",
    "        created.extend([\"years_experience\", \"experience_bucket\"])\n",
    "        \n",
    "    return out, created\n",
    "\n",
    "\n",
    "# advanced box score metrics - efficiency stuff mostly\n",
    "def add_advanced_metrics(df: pd.DataFrame) -> Tuple[pd.DataFrame, List[str]]:\n",
    "    \"\"\"Create advanced metrics from basic box score stats.\"\"\"\n",
    "    out = df.copy()\n",
    "    created: List[str] = []\n",
    "\n",
    "    # find the columns we need (handles different naming)\n",
    "    fga = _first_present(out, [\"total_fga\"])\n",
    "    fta = _first_present(out, [\"total_fta\"])\n",
    "    fgm = _first_present(out, [\"total_fgm\"])\n",
    "    tpa = _first_present(out, [\"total_3pa\"])\n",
    "    tpm = _first_present(out, [\"total_3pm\"])\n",
    "    ast = _first_present(out, [\"total_assists\"])\n",
    "    blk = _first_present(out, [\"total_blocks\"])\n",
    "    stl = _first_present(out, [\"total_steals\"])\n",
    "    pts = _first_present(out, [\"total_points\"])\n",
    "    mins = _first_present(out, [\"total_minutes\"])\n",
    "    reb = _first_present(out, [\"total_rebounds\"])\n",
    "    dreb = _first_present(out, [\"total_reb_def\"])\n",
    "    oreb = _first_present(out, [\"total_reb_off\"])\n",
    "    tov = _first_present(out, [\"total_tov\", \"total_turnovers\"])\n",
    "\n",
    "    # true shooting attempts estimate\n",
    "    if fga and fta:\n",
    "        out[\"ts_attempts\"] = out[fga] + 0.44 * out[fta]\n",
    "        created.append(\"ts_attempts\")\n",
    "        \n",
    "    # shooting rates and efficiency\n",
    "    if fga and tpa:\n",
    "        out[\"three_point_rate\"] = np.where(out[fga] > 0, out[tpa] / out[fga], 0.0)\n",
    "        created.append(\"three_point_rate\")\n",
    "    if fga and fta:\n",
    "        out[\"ft_rate\"] = np.where(out[fga] > 0, out[fta] / out[fga], 0.0)\n",
    "        created.append(\"ft_rate\")\n",
    "    if fga and fgm and tpm:\n",
    "        out[\"efg_pct\"] = np.where(out[fga] > 0, (out[fgm] + 0.5 * out[tpm]) / out[fga], 0.0)\n",
    "        created.append(\"efg_pct\")\n",
    "    if fga and pts:\n",
    "        out[\"pts_per_shot\"] = np.where(out[fga] > 0, out[pts] / out[fga], 0.0)\n",
    "        created.append(\"pts_per_shot\")\n",
    "\n",
    "    # defensive and overall production per 36\n",
    "    if mins and blk and stl:\n",
    "        out[\"defensive_per36\"] = np.where(out[mins] > 0, (out[blk] + out[stl]) * 36 / out[mins], 0.0)\n",
    "        out[\"stocks_per36\"] = out[\"defensive_per36\"].copy()  # same thing\n",
    "        created.extend([\"defensive_per36\", \"stocks_per36\"])\n",
    "    if mins and pts and ast and reb:\n",
    "        out[\"production_per36\"] = np.where(out[mins] > 0, (out[pts] + out[ast] + out[reb]) * 36 / out[mins], 0.0)\n",
    "        created.append(\"production_per36\")\n",
    "    if mins and tov:\n",
    "        out[\"tov_per36\"] = np.where(out[mins] > 0, out[tov] * 36 / out[mins], 0.0)\n",
    "        created.append(\"tov_per36\")\n",
    "\n",
    "    # rebounding shares\n",
    "    if reb and dreb and oreb:\n",
    "        total_reb_safe = out[reb].replace(0, np.nan)\n",
    "        out[\"dreb_share\"] = (out[dreb] / total_reb_safe).fillna(0.0)\n",
    "        out[\"oreb_share\"] = (out[oreb] / total_reb_safe).fillna(0.0)\n",
    "        created.extend([\"dreb_share\", \"oreb_share\"])\n",
    "\n",
    "    # usage events (shots, fts, turnovers)\n",
    "    if fga and fta and tov and mins:\n",
    "        out[\"usage_events_total\"] = out[fga] + 0.44 * out[fta] + out[tov]\n",
    "        out[\"usage_events_per_min\"] = np.where(out[mins] > 0, out[\"usage_events_total\"] / out[mins], 0.0)\n",
    "        created.extend([\"usage_events_total\", \"usage_events_per_min\"])\n",
    "\n",
    "    # assist to turnover ratio\n",
    "    if ast and tov:\n",
    "        out[\"ast_to_tov\"] = np.where(out[tov] > 0, out[ast] / out[tov], out[ast])\n",
    "        created.append(\"ast_to_tov\")\n",
    "\n",
    "    return out, created\n",
    "\n",
    "\n",
    "# usage and shot creation features\n",
    "def add_usage_features(df: pd.DataFrame) -> Tuple[pd.DataFrame, List[str]]:\n",
    "    \"\"\"Usage and shot creation metrics.\"\"\"\n",
    "    out = df.copy()\n",
    "    created: List[str] = []\n",
    "    \n",
    "    # total usage from per-minute usage  \n",
    "    if \"usage_per_min\" in out.columns:\n",
    "        min_col = _first_present(out, [\"total_minutes\"])\n",
    "        if min_col:\n",
    "            out[\"total_usage\"] = out[\"usage_per_min\"] * out[min_col]\n",
    "            created.append(\"total_usage\")\n",
    "\n",
    "    # shot creation (shots + assists)\n",
    "    fga_col = _first_present(out, [\"total_fga\"])\n",
    "    fta_col = _first_present(out, [\"total_fta\"])\n",
    "    ast_col = _first_present(out, [\"total_assists\"])\n",
    "    min_col = _first_present(out, [\"total_minutes\"])\n",
    "    \n",
    "    if fga_col and fta_col and ast_col:\n",
    "        out[\"shot_creation\"] = out[fga_col] + out[fta_col] + out[ast_col]\n",
    "        if min_col:\n",
    "            out[\"shot_creation_per36\"] = np.where(out[min_col] > 0, out[\"shot_creation\"] * 36 / out[min_col], 0.0)\n",
    "        else:\n",
    "            out[\"shot_creation_per36\"] = 0.0\n",
    "        created.extend([\"shot_creation\", \"shot_creation_per36\"])\n",
    "        \n",
    "    return out, created\n",
    "\n",
    "\n",
    "# rolling averages and trends (optional - currently disabled)\n",
    "def add_rolling_features(df: pd.DataFrame, window: int = 3, stats: List[str] = None) -> Tuple[pd.DataFrame, List[str]]:\n",
    "    \"\"\"Rolling means and linear trends for key stats.\"\"\"\n",
    "    if stats is None:\n",
    "        stats = [\"season_pie\", \"ts_pct\", \"pts_per36\", \"ast_per36\", \"efficiency_per_game\"]\n",
    "        \n",
    "    require_columns(df, [\"season_start_year\", \"personId\"], \"add_rolling_features\")\n",
    "    \n",
    "    out = df.copy().sort_values([\"personId\", \"season_start_year\"])\n",
    "    created: List[str] = []\n",
    "    \n",
    "    valid_stats = [s for s in stats if s in out.columns]\n",
    "    if not valid_stats:\n",
    "        return out, created\n",
    "        \n",
    "    gp = out.groupby(\"personId\")\n",
    "    \n",
    "    for stat in valid_stats:\n",
    "        # rolling mean\n",
    "        col_roll = f\"{stat}_rollmean_{window}\"\n",
    "        out[col_roll] = gp[stat].rolling(window, min_periods=1).mean().reset_index(level=0, drop=True)\n",
    "        created.append(col_roll)\n",
    "        \n",
    "        # linear trend slope\n",
    "        def slope(x: pd.Series) -> float:\n",
    "            arr = x.values\n",
    "            if arr.size < 2:\n",
    "                return float(\"nan\")\n",
    "            X = np.arange(len(arr)).reshape(-1, 1)\n",
    "            y = arr.ravel()\n",
    "            try:\n",
    "                model = LinearRegression().fit(X, y)\n",
    "                return float(model.coef_[0])\n",
    "            except Exception:\n",
    "                return float(\"nan\")\n",
    "                \n",
    "        col_slope = f\"{stat}_trend_{window}\"  \n",
    "        out[col_slope] = gp[stat].rolling(window, min_periods=2).apply(slope, raw=False).reset_index(level=0, drop=True)\n",
    "        created.append(col_slope)\n",
    "        \n",
    "    return out, created\n",
    "\n",
    "\n",
    "# figures out which numeric columns to lag automatically\n",
    "def _build_lag_stat_list_auto(df: pd.DataFrame, season_col: str = \"season_start_year\") -> List[str]:\n",
    "    \"\"\"Auto-select numeric columns to lag, excluding obvious problem ones.\"\"\"\n",
    "    numeric = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # stuff we definitely don't want to lag\n",
    "    exclude_exact = {\n",
    "        \"personId\", season_col, \"games_played_total\", \"forecast_season\", \n",
    "        \"source_season\", \"season_pie_num\", \"season_pie_den\"\n",
    "    }\n",
    "    \n",
    "    base = [c for c in numeric if c not in exclude_exact \n",
    "            and not c.endswith(\"_lag1\") and \"trend_\" not in c and \"rollmean_\" not in c]\n",
    "    \n",
    "    # skip columns with id/index type names\n",
    "    bad_substrings = (\"_id\", \"_idx\", \"_code\")\n",
    "    base = [c for c in base if not any(s in c.lower() for s in bad_substrings)]\n",
    "    \n",
    "    return base\n",
    "\n",
    "\n",
    "# creates lagged features by player\n",
    "def add_lag_features(df: pd.DataFrame, stats: Optional[List[str]] = None, \n",
    "                    lags: List[int] = [1], season_col: str = \"season_start_year\") -> Tuple[pd.DataFrame, List[str]]:\n",
    "    \"\"\"Add lag features by player-season. Nulls will be in first seasons only.\"\"\"\n",
    "    out = df.copy()\n",
    "    created: List[str] = []\n",
    "    require_columns(out, [\"personId\", season_col], \"add_lag_features\")\n",
    "    \n",
    "    # auto-detect stats to lag if not provided\n",
    "    if stats is None:\n",
    "        stats = _build_lag_stat_list_auto(out, season_col=season_col)\n",
    "    else:\n",
    "        # filter to numeric columns only\n",
    "        num_cols = set(out.select_dtypes(include=[np.number]).columns)\n",
    "        stats = [s for s in stats if s in num_cols]\n",
    "    \n",
    "    out = out.sort_values([\"personId\", season_col])\n",
    "    gp = out.groupby(\"personId\", group_keys=False)\n",
    "    \n",
    "    # create lag columns\n",
    "    for col in stats:\n",
    "        for k in lags:\n",
    "            name = f\"{col}_lag{k}\"\n",
    "            out[name] = gp[col].shift(k)\n",
    "            created.append(name)\n",
    "    \n",
    "    # add helper columns\n",
    "    out[\"has_prior_season\"] = gp.cumcount() > 0\n",
    "    created.append(\"has_prior_season\")\n",
    "    \n",
    "    return out, created\n",
    "\n",
    "\n",
    "# minutes and availability features  \n",
    "def add_minutes_features(df: pd.DataFrame) -> Tuple[pd.DataFrame, List[str]]:\n",
    "    \"\"\"Playing time and availability metrics.\"\"\"\n",
    "    out = df.copy()\n",
    "    created: List[str] = []\n",
    "    \n",
    "    if \"games_played\" in out.columns and \"total_minutes\" in out.columns:\n",
    "        out[\"minutes_per_game\"] = np.where(out[\"games_played\"] > 0, out[\"total_minutes\"] / out[\"games_played\"], 0.0)\n",
    "        out[\"games_pct\"] = out[\"games_played\"] / 82.0\n",
    "        created.extend([\"minutes_per_game\", \"games_pct\"])\n",
    "        \n",
    "        # playing time tiers\n",
    "        out[\"minutes_tier\"] = pd.cut(out[\"minutes_per_game\"], bins=[0, 15, 25, 35, 48], \n",
    "                                   labels=[\"Bench\", \"Role Player\", \"Starter\", \"Star\"], include_lowest=True)\n",
    "        created.append(\"minutes_tier\")\n",
    "        \n",
    "        # total minutes tiers (handles duplicate edges)\n",
    "        try:\n",
    "            out[\"total_minutes_tier\"] = pd.qcut(out[\"total_minutes\"], q=5, \n",
    "                                              labels=[\"Very Low\", \"Low\", \"Medium\", \"High\", \"Very High\"])\n",
    "        except ValueError:\n",
    "            ranks = out[\"total_minutes\"].rank(method=\"average\")\n",
    "            out[\"total_minutes_tier\"] = pd.qcut(ranks, q=5,\n",
    "                                              labels=[\"Very Low\", \"Low\", \"Medium\", \"High\", \"Very High\"])\n",
    "        created.append(\"total_minutes_tier\")\n",
    "    \n",
    "    return out, created\n",
    "\n",
    "\n",
    "# shooting performance relative to league average by season  \n",
    "def add_performance_consistency(df: pd.DataFrame) -> Tuple[pd.DataFrame, List[str]]:\n",
    "    \"\"\"Shooting performance vs league medians and composite score.\"\"\"\n",
    "    out = df.copy()\n",
    "    created: List[str] = []\n",
    "    \n",
    "    need = [\"season_start_year\", \"fg_pct\", \"fg3_pct\", \"ft_pct\"]\n",
    "    missing = [c for c in need if c not in out.columns]\n",
    "    if missing:\n",
    "        return out, created\n",
    "        \n",
    "    # season-level medians for comparison\n",
    "    grp = out.groupby(\"season_start_year\", group_keys=False)\n",
    "    league_medians = [\"fg_league_med\", \"fg3_league_med\", \"ft_league_med\"]\n",
    "    out[\"fg_league_med\"] = grp[\"fg_pct\"].transform(\"median\")\n",
    "    out[\"fg3_league_med\"] = grp[\"fg3_pct\"].transform(\"median\") \n",
    "    out[\"ft_league_med\"] = grp[\"ft_pct\"].transform(\"median\")\n",
    "    \n",
    "    # differences from league median\n",
    "    out[\"fg_vs_league\"] = out[\"fg_pct\"] - out[\"fg_league_med\"]\n",
    "    out[\"fg3_vs_league\"] = out[\"fg3_pct\"] - out[\"fg3_league_med\"]\n",
    "    out[\"ft_vs_league\"] = out[\"ft_pct\"] - out[\"ft_league_med\"]\n",
    "    created.extend([\"fg_vs_league\", \"fg3_vs_league\", \"ft_vs_league\"])\n",
    "    \n",
    "    # composite shooting score\n",
    "    out[\"shooting_score\"] = out[\"fg_pct\"] * 0.4 + out[\"fg3_pct\"] * 0.3 + out[\"ft_pct\"] * 0.3\n",
    "    created.append(\"shooting_score\")\n",
    "    \n",
    "    # clean up temp columns\n",
    "    out.drop(columns=league_medians, inplace=True)\n",
    "    return out, created\n",
    "\n",
    "\n",
    "# composite features combining multiple stats\n",
    "def create_composite_features(df: pd.DataFrame) -> Tuple[pd.DataFrame, List[str]]:\n",
    "    \"\"\"Create composite impact metrics.\"\"\"\n",
    "    out = df.copy()\n",
    "    created: List[str] = []\n",
    "    \n",
    "    # make sure we have the base columns (fill with nan if missing)\n",
    "    for need in [\"pts_per36\", \"ast_per36\", \"reb_per36\", \"defensive_per36\"]:\n",
    "        if need not in out.columns:\n",
    "            out[need] = np.nan\n",
    "    \n",
    "    # offensive impact score\n",
    "    out[\"offensive_impact\"] = out[\"pts_per36\"] * 0.4 + out[\"ast_per36\"] * 0.3 + out[\"ts_pct\"] * 100 * 0.3\n",
    "    created.append(\"offensive_impact\")\n",
    "    \n",
    "    # two-way impact (offense + defense)\n",
    "    out[\"two_way_impact\"] = out[\"offensive_impact\"] + out[\"defensive_per36\"] * 10\n",
    "    created.append(\"two_way_impact\")\n",
    "    \n",
    "    # efficiency x volume\n",
    "    if \"efficiency_per_game\" in out.columns and \"total_usage\" in out.columns:\n",
    "        out[\"efficiency_volume_score\"] = out[\"efficiency_per_game\"] * out[\"total_usage\"]  \n",
    "        created.append(\"efficiency_volume_score\")\n",
    "    \n",
    "    # versatility score (above median in multiple areas)\n",
    "    scoring_contrib = (out[\"pts_per36\"] > out[\"pts_per36\"].median()).astype(int)\n",
    "    assist_contrib = (out[\"ast_per36\"] > out[\"ast_per36\"].median()).astype(int)\n",
    "    rebound_contrib = (out[\"reb_per36\"] > out[\"reb_per36\"].median()).astype(int) \n",
    "    defense_contrib = (out[\"defensive_per36\"] > out[\"defensive_per36\"].median()).astype(int)\n",
    "    out[\"versatility_score\"] = scoring_contrib + assist_contrib + rebound_contrib + defense_contrib\n",
    "    created.append(\"versatility_score\")\n",
    "    \n",
    "    return out, created\n",
    "\n",
    "\n",
    "# helper for season-normalized z-scores\n",
    "def _zscore_by_season(df: pd.DataFrame, col: str, season_col: str) -> pd.Series:\n",
    "    \"\"\"Z-score within each season, clipped to avoid extreme outliers.\"\"\"\n",
    "    if col not in df.columns:\n",
    "        return pd.Series(np.nan, index=df.index)\n",
    "    g = df.groupby(season_col)[col]\n",
    "    z = (df[col] - g.transform(\"mean\")) / (g.transform(\"std\").replace(0, np.nan))\n",
    "    return z.clip(-3, 3).fillna(0.0)\n",
    "\n",
    "\n",
    "# portability index - how well skills transfer between situations  \n",
    "def build_portability_index(df: pd.DataFrame) -> Tuple[pd.DataFrame, List[str]]:\n",
    "    \"\"\"Portability index based on transferable skills.\"\"\"\n",
    "    out = df.copy()\n",
    "    created: List[str] = []\n",
    "    season_col = \"season_start_year\"\n",
    "    require_columns(out, [season_col], \"build_portability_index\")\n",
    "    \n",
    "    # season-normalized z-scores for key components\n",
    "    z = {}\n",
    "    for col in [\"ts_pct\", \"efg_pct\", \"pts_per_shot\", \"fg3_pct\", \"three_point_rate\", \"ft_pct\",\n",
    "                \"stocks_per36\", \"dreb_share\", \"oreb_share\", \"ast_per36\", \"ast_to_tov\", \"usage_per_min\"]:\n",
    "        if col in out.columns:\n",
    "            z[col] = _zscore_by_season(out, col, season_col)\n",
    "        else:\n",
    "            z[col] = pd.Series(0.0, index=out.index)\n",
    "    \n",
    "    # component scores\n",
    "    score_eff = (z[\"ts_pct\"] + z[\"efg_pct\"] + z[\"pts_per_shot\"]) / 3.0\n",
    "    shoot_abil = (z[\"fg3_pct\"] + z[\"three_point_rate\"] + z[\"ft_pct\"]) / 3.0  \n",
    "    def_abil = z[\"stocks_per36\"]\n",
    "    \n",
    "    # rebounding versatility (good at both, penalty for imbalance)\n",
    "    reb_mean = (z[\"dreb_share\"] + z[\"oreb_share\"]) / 2.0\n",
    "    reb_gap = (z[\"dreb_share\"] - z[\"oreb_share\"]).abs()\n",
    "    def_vers = reb_mean - 0.25 * reb_gap\n",
    "    \n",
    "    pass_abil = (z[\"ast_per36\"] + z[\"ast_to_tov\"]) / 2.0\n",
    "    \n",
    "    # usage with diminishing returns (too much usage can hurt portability)\n",
    "    usage_term = z[\"usage_per_min\"] - 0.15 * (z[\"usage_per_min\"] ** 2)\n",
    "    \n",
    "    # weighted combination (weights sum to 1.0)\n",
    "    out[\"portability_index\"] = (0.16 * score_eff + 0.40 * shoot_abil + 0.08 * def_abil + \n",
    "                               0.05 * def_vers + 0.25 * pass_abil + 0.06 * usage_term)\n",
    "    created.append(\"portability_index\")\n",
    "    \n",
    "    # save component scores too\n",
    "    out[\"pi_scoring_eff\"] = score_eff\n",
    "    out[\"pi_shooting\"] = shoot_abil  \n",
    "    out[\"pi_defense\"] = def_abil\n",
    "    out[\"pi_versatility\"] = def_vers\n",
    "    out[\"pi_passing\"] = pass_abil\n",
    "    out[\"pi_usage_term\"] = usage_term\n",
    "    created.extend([\"pi_scoring_eff\", \"pi_shooting\", \"pi_defense\", \"pi_versatility\", \"pi_passing\", \"pi_usage_term\"])\n",
    "    \n",
    "    return out, created\n",
    "\n",
    "\n",
    "# main feature engineering function\n",
    "def engineer_features(df: pd.DataFrame, drop_null_lag_rows: bool = True, verbose: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build all features and optionally drop first-season rows with null lags.\n",
    "    \n",
    "    Args:\n",
    "        df: Input dataframe with player-season data\n",
    "        drop_null_lag_rows: If True, drop rows where lag features are null (default True)\n",
    "        verbose: Print progress info (default False)\n",
    "    \n",
    "    Returns:\n",
    "        Processed dataframe with all engineered features\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"Starting feature engineering...\")\n",
    "        \n",
    "    original_shape = df.shape\n",
    "    out = df.copy()\n",
    "    \n",
    "    # check we have required base columns\n",
    "    required_base = [\"personId\", \"season\", \"games_played\", \"total_minutes\", \"season_pie\"]\n",
    "    missing_base = [c for c in required_base if c not in out.columns]\n",
    "    if missing_base:\n",
    "        raise ValueError(f\"Missing required columns: {missing_base}\")\n",
    "    \n",
    "    # 1. parse season to get numeric year\n",
    "    if verbose:\n",
    "        print(\"Parsing seasons...\")\n",
    "    out, _ = add_season_start_year(out)\n",
    "    \n",
    "    # 2. sort by player and season for all subsequent operations\n",
    "    out = out.sort_values([\"personId\", \"season_start_year\"])\n",
    "    \n",
    "    # 3. build features step by step\n",
    "    if verbose:\n",
    "        print(\"Adding experience features...\")\n",
    "    out, _ = add_experience_features(out)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Adding advanced metrics...\")\n",
    "    out, _ = add_advanced_metrics(out)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Adding usage features...\")\n",
    "    out, _ = add_usage_features(out)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Adding minutes features...\")  \n",
    "    out, _ = add_minutes_features(out)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Adding performance consistency...\")\n",
    "    out, _ = add_performance_consistency(out)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Creating composite features...\")\n",
    "    out, _ = create_composite_features(out)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Building portability index...\")\n",
    "    out, _ = build_portability_index(out)\n",
    "    \n",
    "    # 4. add lag features (creates nulls in first seasons)\n",
    "    if verbose:\n",
    "        print(\"Creating lag features...\")\n",
    "    out, _ = add_lag_features(out, lags=[1])\n",
    "    \n",
    "    # 5. clean up any infinite values\n",
    "    numeric_cols = out.select_dtypes(include=[np.number]).columns\n",
    "    inf_cols = []\n",
    "    for col in numeric_cols:\n",
    "        if np.isinf(out[col]).any():\n",
    "            inf_cols.append(col)\n",
    "            out[col] = out[col].replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    if verbose and inf_cols:\n",
    "        print(f\"Cleaned infinite values in {len(inf_cols)} columns\")\n",
    "    \n",
    "    # 6. handle lag nulls\n",
    "    lag_cols = [c for c in out.columns if c.endswith(\"_lag1\")]\n",
    "    if lag_cols:\n",
    "        # check that lag nulls are only in first seasons (as expected)\n",
    "        lag_nulls_mask = out[lag_cols].isnull().any(axis=1)\n",
    "        first_season_mask = compute_first_season_mask(out)\n",
    "        \n",
    "        # simple validation\n",
    "        nulls_are_first_seasons = (lag_nulls_mask == first_season_mask).all()\n",
    "        \n",
    "        if nulls_are_first_seasons:\n",
    "            if verbose:\n",
    "                print(f\"✓ Lag nulls confirmed as first seasons only ({lag_nulls_mask.sum()} rows)\")\n",
    "            \n",
    "            if drop_null_lag_rows:\n",
    "                rows_before = len(out)\n",
    "                out = out[~lag_nulls_mask].copy()\n",
    "                rows_dropped = rows_before - len(out)\n",
    "                if verbose:\n",
    "                    print(f\"Dropped {rows_dropped} first-season rows with null lags\")\n",
    "        else:\n",
    "            print(\"⚠️ Warning: Some lag nulls are not from first seasons - check data quality\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Feature engineering complete: {original_shape[0]} → {len(out)} rows, {original_shape[1]} → {len(out.columns)} columns\")\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from src.heat_data_scientist_2025.data.load_data_utils import load_data_optimized\n",
    "    from src.heat_data_scientist_2025.utils.config import CFG\n",
    "\n",
    "    df = load_data_optimized(\n",
    "        CFG.ml_dataset_path,\n",
    "        debug=True,\n",
    "        drop_null_rows=True,\n",
    "    )\n",
    "\n",
    "    \n",
    "    # run feature engineering\n",
    "    try:\n",
    "        df_eng = engineer_features(df, drop_null_lag_rows=True, verbose=True)\n",
    "        print(f\"✓ Success! Result shape: {df_eng.shape}\")\n",
    "        print(\"Sample lag columns created:\", [c for c in df_eng.columns if c.endswith('_lag1')][:5])\n",
    "        print(f\"First season rows dropped: {len(df) - len(df_eng)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error: {e}\")\n",
    "        \n",
    "    #print columns\n",
    "    print(df_eng.columns)\n",
    "\n",
    "    # check that there are these lists in the dataset\n",
    "    numerical_features = [\n",
    "        # lagged features\n",
    "        \"season_pie_lag1\", \"ts_pct_lag1\", \"efg_pct_lag1\", \"fg_pct_lag1\", \"fg3_pct_lag1\", \"ft_pct_lag1\",\n",
    "        \"pts_per36_lag1\", \"ast_per36_lag1\", \"reb_per36_lag1\", \"defensive_per36_lag1\",\n",
    "        \"production_per36_lag1\", \"stocks_per36_lag1\", \"three_point_rate_lag1\", \"ft_rate_lag1\",\n",
    "        \"pts_per_shot_lag1\", \"ast_to_tov_lag1\", \"usage_events_per_min_lag1\", \"usage_per_min_lag1\",\n",
    "        \"games_played_lag1\", \"total_minutes_lag1\", \"total_points_lag1\", \"total_assists_lag1\",\n",
    "        \"total_rebounds_lag1\", \"total_steals_lag1\", \"total_blocks_lag1\", \"total_fga_lag1\",\n",
    "        \"total_fta_lag1\", \"total_3pa_lag1\", \"total_3pm_lag1\", \"total_tov_lag1\", \"win_pct_lag1\",\n",
    "        \"avg_plus_minus_lag1\", \"team_win_pct_final_lag1\",\n",
    "        \"offensive_impact_lag1\", \"two_way_impact_lag1\", \"efficiency_volume_score_lag1\",\n",
    "        \"versatility_score_lag1\", \"shooting_score_lag1\", \"season\"\n",
    "    ]\n",
    "    \n",
    "    nominal_categoricals = []\n",
    "    ordinal_categoricals = [\"minutes_tier\"] \n",
    "    y_variables = [\"season_pie\", \"game_score_per36\"]\n",
    "    \n",
    "    #check that the features are in the df_eng\n",
    "    for feature in numerical_features:\n",
    "        assert feature in df_eng.columns, f\"{feature} is not in the dataset\"\n",
    "    for feature in nominal_categoricals:\n",
    "        assert feature in df_eng.columns, f\"{feature} is not in the dataset\"\n",
    "    for feature in ordinal_categoricals:\n",
    "        assert feature in df_eng.columns, f\"{feature} is not in the dataset\"\n",
    "    for feature in y_variables:\n",
    "        assert feature in df_eng.columns, f\"{feature} is not in the dataset\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffe640b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/heat_data_scientist_2025/ml/enhanced_ml_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/heat_data_scientist_2025/ml/enhanced_ml_pipeline.py\n",
    "\"\"\"\n",
    "Enhanced Automated ML Pipeline with Feature Evaluation and Multi-Target Support\n",
    "===============================================================================\n",
    "\n",
    "New functions for:\n",
    "1. Feature validation and evaluation\n",
    "2. Permutation importance calculation\n",
    "3. Feature filtering based on importance\n",
    "4. Multi-target prediction support\n",
    "5. Feature restriction to specified lists only\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import existing modules\n",
    "from src.heat_data_scientist_2025.utils.config import CFG, ML_CONFIG\n",
    "from src.heat_data_scientist_2025.data.feature_engineering import (\n",
    "    engineer_features\n",
    ")\n",
    "# --- Add near the top of enhanced_ml_pipeline.py imports ---\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# --- Add these helpers somewhere above your main pipeline class/functions ---\n",
    "\n",
    "# explicit ordinal orders\n",
    "ORDINAL_ORDERS = {\n",
    "    \"minutes_tier\": [\"Bench\", \"Role Player\", \"Starter\", \"Star\"],\n",
    "}\n",
    "\n",
    "def _freeze_label_encoder(le: LabelEncoder) -> dict:\n",
    "    \"\"\"\n",
    "    Convert fitted LabelEncoder to a simple mapping dict (value -> code),\n",
    "    so we can transform robustly (and detect unknowns) without calling .transform().\n",
    "    \"\"\"\n",
    "    classes = list(le.classes_)\n",
    "    mapping = {cls: idx for idx, cls in enumerate(classes)}\n",
    "    reverse = {idx: cls for idx, cls in enumerate(classes)}\n",
    "    return {\"forward\": mapping, \"reverse\": reverse}\n",
    "\n",
    "def ensure_numeric_matrix(df_like: pd.DataFrame, context: str = \"X\") -> None:\n",
    "    \"\"\"\n",
    "    Hard-stop if any object columns exist. Print sample offending values.\n",
    "    \"\"\"\n",
    "    obj_cols = [c for c in df_like.columns if df_like[c].dtype == \"object\"]\n",
    "    if obj_cols:\n",
    "        examples = {c: df_like[c].dropna().astype(str).unique()[:5].tolist() for c in obj_cols}\n",
    "        msg = [\n",
    "            f\"[ensure_numeric_matrix] {context} contains object/string columns:\",\n",
    "            f\"  Columns: {obj_cols}\",\n",
    "            f\"  Sample values: {examples}\",\n",
    "            \"  -> Encode these before calling model.predict / model.fit.\"\n",
    "        ]\n",
    "        raise ValueError(\"\\n\".join(msg))\n",
    "\n",
    "def validate_target_feature_separation(target: str, feature_names: list[str], verbose: bool = True) -> bool:\n",
    "    \"\"\"\n",
    "    Prevent blatant leakage: a feature that is literally the target or same-season alias.\n",
    "    Allowed: *_lag1 variants of the target.\n",
    "    \"\"\"\n",
    "    bad = []\n",
    "    for f in feature_names:\n",
    "        # strict disallow the bare target\n",
    "        if f == target:\n",
    "            bad.append(f)\n",
    "        # disallow target-like aliases without lag\n",
    "        if f.startswith(target + \"_\") and not f.endswith(\"_lag1\"):\n",
    "            bad.append(f)\n",
    "    ok = len(bad) == 0\n",
    "    if verbose:\n",
    "        if ok:\n",
    "            print(f\"[leakage-check] OK for target '{target}'.\")\n",
    "        else:\n",
    "            print(f\"[leakage-check] Potential leakage for target '{target}': {bad}\")\n",
    "    return ok\n",
    "\n",
    "def create_target_specific_features(\n",
    "    target: str,\n",
    "    numerical_features: list[str],\n",
    "    nominal_categoricals: list[str],\n",
    "    ordinal_categoricals: list[str],\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Choose features for a target. Uses your config lists when available.\n",
    "    Falls back to filtering provided feature lists.\n",
    "    \"\"\"\n",
    "    from src.heat_data_scientist_2025.utils.config import (\n",
    "        season_pie_numerical_features as PIE_NUMS,\n",
    "        game_score_per36_numerical_features as GS36_NUMS,\n",
    "    )\n",
    "    if target == \"season_pie\":\n",
    "        numeric = [f for f in PIE_NUMS if f in numerical_features or f in PIE_NUMS]\n",
    "    elif target == \"game_score_per36\":\n",
    "        numeric = [f for f in GS36_NUMS if f in numerical_features or f in GS36_NUMS]\n",
    "    else:\n",
    "        numeric = [f for f in numerical_features]  # generic fallback\n",
    "\n",
    "    return list(dict.fromkeys(numeric + nominal_categoricals + ordinal_categoricals))  # keep order, remove dups\n",
    "\n",
    "@dataclass\n",
    "class EncodersBundle:\n",
    "    \"\"\"\n",
    "    Stores all encoders / mappings for re-use at prediction time and for decoding.\n",
    "    \"\"\"\n",
    "    nominal_maps: dict         # {col: {\"forward\": {val->code}, \"reverse\": {code->val}}}\n",
    "    ordinal_maps: dict         # {col: {\"forward\": {val->code}, \"reverse\": {code->val}}}\n",
    "    raw_label_encoders: dict   # {col: fitted LabelEncoder}  (kept for compatibility / inspection)\n",
    "\n",
    "def encode_categoricals(\n",
    "    df: pd.DataFrame,\n",
    "    nominal_categoricals: list[str],\n",
    "    ordinal_categoricals: list[str],\n",
    "    strict: bool = True,\n",
    "    verbose: bool = True\n",
    ") -> tuple[pd.DataFrame, EncodersBundle]:\n",
    "    \"\"\"\n",
    "    Encode categoricals with strong debugs and easy reversibility.\n",
    "\n",
    "    - Ordinals use explicit domain order (e.g., minutes_tier).\n",
    "    - Nominals use LabelEncoder, but we freeze to dicts for robust transform.\n",
    "    - We don't silently coerce unknowns; we raise unless strict=False.\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "    raw_label_encoders: dict[str, LabelEncoder] = {}\n",
    "    nominal_maps: dict[str, dict] = {}\n",
    "    ordinal_maps: dict[str, dict] = {}\n",
    "\n",
    "    # Nominal\n",
    "    for col in nominal_categoricals:\n",
    "        if col not in out.columns:\n",
    "            continue\n",
    "        ser = out[col].astype(\"string\").fillna(\"Unknown\")\n",
    "        le = LabelEncoder()\n",
    "        le.fit(ser.to_numpy())\n",
    "        raw_label_encoders[col] = le\n",
    "        maps = _freeze_label_encoder(le)\n",
    "        nominal_maps[col] = maps\n",
    "        out[col] = ser.map(maps[\"forward\"])\n",
    "        # enforce no NaNs after mapping unless strict=False\n",
    "        unknown = out[col].isna()\n",
    "        if unknown.any():\n",
    "            unseen = sorted(ser[unknown].dropna().unique().tolist())\n",
    "            msg = (f\"[encode_categoricals] Unknown nominal categories in '{col}': {unseen}. \"\n",
    "                   f\"Known={list(maps['forward'].keys())}\")\n",
    "            if strict:\n",
    "                raise ValueError(msg)\n",
    "            else:\n",
    "                print(\"WARN:\", msg)\n",
    "                out.loc[unknown, col] = -1  # explicit bucket for unknown\n",
    "\n",
    "        out[col] = out[col].astype(\"int32\")\n",
    "\n",
    "    # Ordinal\n",
    "    for col in ordinal_categoricals:\n",
    "        if col not in out.columns:\n",
    "            continue\n",
    "        ser = out[col].astype(\"string\")\n",
    "        # choose explicit order if provided\n",
    "        order = ORDINAL_ORDERS.get(col)\n",
    "        if order is None:\n",
    "            # if no explicit order is known, try to keep category order if any\n",
    "            if pd.api.types.is_categorical_dtype(out[col]) and out[col].cat.ordered:\n",
    "                order = list(out[col].cat.categories.astype(\"string\"))\n",
    "            else:\n",
    "                raise ValueError(f\"[encode_categoricals] No explicit order for ordinal '{col}'. \"\n",
    "                                 f\"Provide ORDINAL_ORDERS['{col}']=[...]\")\n",
    "        # allow 'Unknown' as explicit category so we can map NA deterministically\n",
    "        if \"Unknown\" not in order:\n",
    "            order = order + [\"Unknown\"]\n",
    "        forward = {lvl: i for i, lvl in enumerate(order)}\n",
    "        reverse = {i: lvl for lvl, i in forward.items()}\n",
    "        ordinal_maps[col] = {\"forward\": forward, \"reverse\": reverse}\n",
    "\n",
    "        ser = ser.fillna(\"Unknown\")\n",
    "        out[col] = ser.map(forward)\n",
    "        unknown = out[col].isna()\n",
    "        if unknown.any():\n",
    "            unseen = sorted(ser[unknown].dropna().unique().tolist())\n",
    "            msg = (f\"[encode_categoricals] Unknown ordinal categories in '{col}': {unseen}. \"\n",
    "                   f\"Allowed={order}\")\n",
    "            if strict:\n",
    "                raise ValueError(msg)\n",
    "            else:\n",
    "                print(\"WARN:\", msg)\n",
    "                out.loc[unknown, col] = forward[\"Unknown\"]\n",
    "\n",
    "        out[col] = out[col].astype(\"int16\")\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[encode_categoricals] Encoded: {len(nominal_maps)} nominal, {len(ordinal_maps)} ordinal\")\n",
    "\n",
    "    return out, EncodersBundle(nominal_maps, ordinal_maps, raw_label_encoders)\n",
    "\n",
    "def apply_encoders_to_frame(\n",
    "    df: pd.DataFrame,\n",
    "    encoders: EncodersBundle,\n",
    "    strict: bool = True,\n",
    "    verbose: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply frozen encoders to a new frame (e.g., prediction set) without calling LabelEncoder.transform.\n",
    "    Raises on unknowns unless strict=False.\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "\n",
    "    # Nominal\n",
    "    for col, maps in encoders.nominal_maps.items():\n",
    "        if col not in out.columns:\n",
    "            continue\n",
    "        ser = out[col].astype(\"string\").fillna(\"Unknown\")\n",
    "        out[col] = ser.map(maps[\"forward\"])\n",
    "        unk = out[col].isna()\n",
    "        if unk.any():\n",
    "            unseen = sorted(ser[unk].dropna().unique().tolist())\n",
    "            msg = (f\"[apply_encoders] Unknown nominal categories in '{col}': {unseen}. \"\n",
    "                   f\"Known={list(maps['forward'].keys())}\")\n",
    "            if strict:\n",
    "                raise ValueError(msg)\n",
    "            else:\n",
    "                print(\"WARN:\", msg)\n",
    "                out.loc[unk, col] = -1\n",
    "        out[col] = out[col].astype(\"int32\")\n",
    "\n",
    "    # Ordinal\n",
    "    for col, maps in encoders.ordinal_maps.items():\n",
    "        if col not in out.columns:\n",
    "            continue\n",
    "        ser = out[col].astype(\"string\").fillna(\"Unknown\")\n",
    "        out[col] = ser.map(maps[\"forward\"])\n",
    "        unk = out[col].isna()\n",
    "        if unk.any():\n",
    "            unseen = sorted(ser[unk].dropna().unique().tolist())\n",
    "            msg = (f\"[apply_encoders] Unknown ordinal categories in '{col}': {unseen}. \"\n",
    "                   f\"Allowed={list(maps['forward'].keys())}\")\n",
    "            if strict:\n",
    "                raise ValueError(msg)\n",
    "            else:\n",
    "                print(\"WARN:\", msg)\n",
    "                out.loc[unk, col] = maps[\"forward\"][\"Unknown\"]\n",
    "        out[col] = out[col].astype(\"int16\")\n",
    "\n",
    "    if verbose:\n",
    "        obj_cols = [c for c in out.columns if out[c].dtype == \"object\"]\n",
    "        if obj_cols:\n",
    "            print(f\"[apply_encoders] WARN: still object cols after encoding: {obj_cols}\")\n",
    "\n",
    "    return out\n",
    "\n",
    "def audit_lag_feature_integrity(\n",
    "    df: pd.DataFrame,\n",
    "    person_col: str = \"personId\",\n",
    "    season_col: str = \"season_start_year\",\n",
    "    lag_pairs: list[tuple[str, str]] = [(\"season_pie_lag1\", \"season_pie\")],\n",
    "    verbose: bool = True\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Audit that lag columns are true 1-season lags and not same-season leakage.\n",
    "    Prints diagnostics; raises only on blatant shape problems.\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"\\n[audit] Verifying lag feature integrity...\")\n",
    "    if not {person_col, season_col}.issubset(df.columns):\n",
    "        print(\"[audit] Skipping: missing required id/season cols.\")\n",
    "        return\n",
    "\n",
    "    g = df.sort_values([person_col, season_col]).groupby(person_col, group_keys=False)\n",
    "    prev_year = g[season_col].shift(1)\n",
    "    year_gap = df[season_col] - prev_year\n",
    "\n",
    "    # % rows whose previous row is exactly prior season (for non-first rows)\n",
    "    valid_prev = (g.cumcount() > 0) & (year_gap == 1)\n",
    "    if verbose:\n",
    "        total = int((g.cumcount() > 0).sum())\n",
    "        good = int(valid_prev.sum())\n",
    "        print(f\"[audit] Consecutive season pairs: {good}/{total} ({(good/total*100 if total else 100):.1f}%)\")\n",
    "\n",
    "    for lag_col, base_col in lag_pairs:\n",
    "        if lag_col not in df.columns or base_col not in df.columns:\n",
    "            print(f\"[audit] Skipping pair (missing cols): {lag_col}, {base_col}\")\n",
    "            continue\n",
    "        expected = g[base_col].shift(1)\n",
    "        mism = (df[lag_col] != expected) & valid_prev\n",
    "        mism_ct = int(mism.sum())\n",
    "        if verbose:\n",
    "            print(f\"[audit] {lag_col} vs shift({base_col}): mismatches among consecutive seasons = {mism_ct}\")\n",
    "            if mism_ct:\n",
    "                print(df.loc[mism, [person_col, season_col, lag_col, base_col]].head(5))\n",
    "\n",
    "\n",
    "def validate_and_evaluate_features(df: pd.DataFrame, \n",
    "                                 numerical_features: List[str],\n",
    "                                 nominal_categoricals: List[str],\n",
    "                                 ordinal_categoricals: List[str],\n",
    "                                 y_variables: List[str],\n",
    "                                 verbose: bool = True) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Validate that specified features exist and evaluate their completeness.\n",
    "    Robust to empty feature groups (no division by zero in prints).\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"🔍 VALIDATING AND EVALUATING SPECIFIED FEATURES\")\n",
    "        print(\"=\" * 55)\n",
    "    \n",
    "    results = {\n",
    "        'numerical_features': {},\n",
    "        'nominal_categoricals': {},\n",
    "        'ordinal_categoricals': {},\n",
    "        'y_variables': {},\n",
    "        'missing_features': [],\n",
    "        'available_features': [],\n",
    "        'feature_completeness': {}\n",
    "    }\n",
    "    \n",
    "    all_specified_features = numerical_features + nominal_categoricals + ordinal_categoricals + y_variables\n",
    "\n",
    "    # Per-group validation with safe printing\n",
    "    groups = [\n",
    "        ('numerical_features', numerical_features),\n",
    "        ('nominal_categoricals', nominal_categoricals),\n",
    "        ('ordinal_categoricals', ordinal_categoricals),\n",
    "        ('y_variables', y_variables),\n",
    "    ]\n",
    "    for feature_type, features in groups:\n",
    "        available = [f for f in features if f in df.columns]\n",
    "        missing = [f for f in features if f not in df.columns]\n",
    "\n",
    "        results[feature_type]['available'] = available\n",
    "        results[feature_type]['missing'] = missing\n",
    "        results[feature_type]['availability_pct'] = (len(available) / len(features) * 100) if features else 100.0\n",
    "\n",
    "        # Calculate completeness only for available features\n",
    "        for feature in available:\n",
    "            non_null_count = df[feature].notna().sum()\n",
    "            total_count = len(df)\n",
    "            completeness_pct = (non_null_count / total_count * 100.0) if total_count else 0.0\n",
    "            results['feature_completeness'][feature] = {\n",
    "                'non_null_count': int(non_null_count),\n",
    "                'total_count': int(total_count),\n",
    "                'completeness_pct': float(completeness_pct),\n",
    "            }\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n📊 {feature_type.replace('_', ' ').title()}:\")\n",
    "            if len(features) == 0:\n",
    "                print(\"   Available: 0/0 (n/a — no features specified)\")\n",
    "                print(\"   Completeness: n/a\")\n",
    "            else:\n",
    "                pct = (len(available) / len(features) * 100.0)\n",
    "                print(f\"   Available: {len(available)}/{len(features)} ({pct:.1f}%)\")\n",
    "                if missing:\n",
    "                    print(f\"   Missing: {missing}\")\n",
    "                if available:\n",
    "                    print(f\"   Completeness:\")\n",
    "                    # show at most 5 to avoid flooding logs\n",
    "                    for feature in available[:5]:\n",
    "                        comp_pct = results['feature_completeness'][feature]['completeness_pct']\n",
    "                        print(f\"     {feature:<25} {comp_pct:5.1f}%\")\n",
    "                    if len(available) > 5:\n",
    "                        print(f\"     ... and {len(available)-5} more\")\n",
    "\n",
    "    # Overall summary (robust even if all lists were empty, though in practice y_variables is non-empty)\n",
    "    total_specified = len(all_specified_features)\n",
    "    total_available = len([f for f in all_specified_features if f in df.columns])\n",
    "    overall_availability = (total_available / total_specified * 100.0) if total_specified else 100.0\n",
    "\n",
    "    results['overall'] = {\n",
    "        'total_specified': int(total_specified),\n",
    "        'total_available': int(total_available),\n",
    "        'availability_pct': float(overall_availability)\n",
    "    }\n",
    "    results['missing_features'] = [f for f in all_specified_features if f not in df.columns]\n",
    "    results['available_features'] = [f for f in all_specified_features if f in df.columns]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\n📈 OVERALL FEATURE AVAILABILITY:\")\n",
    "        if total_specified == 0:\n",
    "            print(\"   Specified features: 0 (n/a)\")\n",
    "            print(\"   Available features: 0 (n/a)\")\n",
    "            print(\"   Availability: n/a\")\n",
    "        else:\n",
    "            print(f\"   Specified features: {total_specified}\")\n",
    "            print(f\"   Available features: {total_available}\")\n",
    "            print(f\"   Availability: {overall_availability:.1f}%\")\n",
    "\n",
    "        if results['missing_features']:\n",
    "            print(f\"\\n⚠️  Missing Features ({len(results['missing_features'])}):\")\n",
    "            for feature in results['missing_features'][:10]:\n",
    "                print(f\"     {feature}\")\n",
    "            if len(results['missing_features']) > 10:\n",
    "                print(f\"     ... and {len(results['missing_features'])-10} more\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def calculate_permutation_importance(X_train: pd.DataFrame, \n",
    "                                   y_train: pd.Series,\n",
    "                                   X_test: pd.DataFrame, \n",
    "                                   y_test: pd.Series,\n",
    "                                   target_name: str,\n",
    "                                   numerical_features: List[str],\n",
    "                                   model: Optional[Any] = None,\n",
    "                                   n_repeats: int = 10,\n",
    "                                   random_state: int = 42,\n",
    "                                   verbose: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate permutation importance for numerical features for a specific target.\n",
    "    Hard fail on unexpected nulls to avoid masking upstream issues.\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"\\n🔄 CALCULATING PERMUTATION IMPORTANCE FOR {target_name.upper()}\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "    # Select the numerical features that exist\n",
    "    available_numerical = [f for f in numerical_features if f in X_train.columns]\n",
    "    if not available_numerical:\n",
    "        if verbose:\n",
    "            print(\"⚠️  No numerical features available for permutation importance\")\n",
    "        return pd.DataFrame(columns=['feature','importance_mean','importance_std','target','importance_lower','importance_upper'])\n",
    "\n",
    "    X_train_num = X_train[available_numerical].copy()\n",
    "    X_test_num  = X_test[available_numerical].copy()\n",
    "\n",
    "    # Strict null checks (no imputation here)\n",
    "    n_tr = int(X_train_num.isna().sum().sum())\n",
    "    n_te = int(X_test_num.isna().sum().sum())\n",
    "    n_ytr = int(y_train.isna().sum())\n",
    "    n_yte = int(y_test.isna().sum())\n",
    "    if n_tr or n_te or n_ytr or n_yte:\n",
    "        raise ValueError(\n",
    "            f\"[permutation_importance] Unexpected nulls found \"\n",
    "            f\"(X_train NA={n_tr}, X_test NA={n_te}, y_train NA={n_ytr}, y_test NA={n_yte}). \"\n",
    "            f\"Upstream should use 'filter_complete' to guarantee completeness.\"\n",
    "        )\n",
    "\n",
    "    # Train model if not provided\n",
    "    if model is None:\n",
    "        model = RandomForestRegressor(\n",
    "            n_estimators=100,\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        model.fit(X_train_num, y_train)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"   📊 Features for importance: {len(available_numerical)}\")\n",
    "        print(f\"   🎯 Target: {target_name}\")\n",
    "        print(f\"   🔄 Repeats: {n_repeats}\")\n",
    "\n",
    "    perm = permutation_importance(\n",
    "        model, X_test_num, y_test,\n",
    "        n_repeats=n_repeats,\n",
    "        random_state=random_state,\n",
    "        scoring='r2'\n",
    "    )\n",
    "\n",
    "    importance_df = (\n",
    "        pd.DataFrame({\n",
    "            'feature': available_numerical,\n",
    "            'importance_mean': perm.importances_mean,\n",
    "            'importance_std': perm.importances_std,\n",
    "            'target': target_name\n",
    "        })\n",
    "        .sort_values('importance_mean', ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    importance_df['importance_lower'] = importance_df['importance_mean'] - 1.96 * importance_df['importance_std']\n",
    "    importance_df['importance_upper'] = importance_df['importance_mean'] + 1.96 * importance_df['importance_std']\n",
    "\n",
    "    if verbose and not importance_df.empty:\n",
    "        print(f\"\\n📈 TOP 10 MOST IMPORTANT FEATURES FOR {target_name.upper()}:\")\n",
    "        print(\"-\" * 70)\n",
    "        for i, (_, row) in enumerate(importance_df.head(10).iterrows(), 1):\n",
    "            print(f\"{i:2d}. {row['feature']:<30} {row['importance_mean']:8.4f} ± {row['importance_std']:6.4f}\")\n",
    "\n",
    "    return importance_df\n",
    "\n",
    "\n",
    "\n",
    "def filter_features_by_importance(importance_df: pd.DataFrame,\n",
    "                                min_importance: float = 0.001,\n",
    "                                max_features: Optional[int] = None,\n",
    "                                target_name: str = \"unknown\",\n",
    "                                verbose: bool = True) -> List[str]:\n",
    "    \"\"\"\n",
    "    Filter features based on permutation importance scores.\n",
    "    \n",
    "    Returns:\n",
    "        List of important feature names\n",
    "    \"\"\"\n",
    "    if importance_df.empty:\n",
    "        return []\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n✂️  FILTERING FEATURES FOR {target_name.upper()}\")\n",
    "        print(\"=\" * 50)\n",
    "    \n",
    "    # Filter by minimum importance\n",
    "    important_features = importance_df[\n",
    "        importance_df['importance_mean'] > min_importance\n",
    "    ].copy()\n",
    "    \n",
    "    # Apply max features limit if specified\n",
    "    if max_features and len(important_features) > max_features:\n",
    "        important_features = important_features.head(max_features)\n",
    "    \n",
    "    feature_names = important_features['feature'].tolist()\n",
    "    \n",
    "    if verbose:\n",
    "        total_features = len(importance_df)\n",
    "        kept_features = len(feature_names)\n",
    "        removed_features = total_features - kept_features\n",
    "        \n",
    "        print(f\"   🎯 Min importance threshold: {min_importance}\")\n",
    "        if max_features:\n",
    "            print(f\"   📊 Max features limit: {max_features}\")\n",
    "        print(f\"   ✅ Features kept: {kept_features}/{total_features} ({kept_features/total_features*100:.1f}%)\")\n",
    "        print(f\"   ❌ Features removed: {removed_features}\")\n",
    "        \n",
    "        if kept_features > 0:\n",
    "            print(f\"   📈 Importance range: {important_features['importance_mean'].min():.4f} to {important_features['importance_mean'].max():.4f}\")\n",
    "    \n",
    "    return feature_names\n",
    "\n",
    "\n",
    "def create_game_score_per36_feature(df: pd.DataFrame) -> Tuple[pd.DataFrame, List[str]]:\n",
    "    \"\"\"\n",
    "    Create the game_score_per36 feature since it's not in your original feature engineering.\n",
    "    Game Score = PTS + 0.4*FG - 0.7*FGA - 0.4*(FTA-FT) + 0.7*ORB + 0.3*DRB + STL + 0.7*AST + 0.7*BLK - 0.4*PF - TOV\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "    created = []\n",
    "    \n",
    "    # Required columns for game score calculation\n",
    "    required_cols = ['total_points', 'total_fgm', 'total_fga', 'total_fta', 'total_ftm', \n",
    "                    'total_reb_off', 'total_reb_def', 'total_steals', 'total_assists', \n",
    "                    'total_blocks', 'total_pf', 'total_tov', 'total_minutes']\n",
    "    \n",
    "    missing_cols = [c for c in required_cols if c not in out.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"[create_game_score_per36] Warning: Missing columns {missing_cols}\")\n",
    "        # Create dummy column filled with median season_pie for now\n",
    "        out['game_score_per36'] = out.get('season_pie', 0.1) * 36  # Rough approximation\n",
    "        return out, ['game_score_per36']\n",
    "    \n",
    "    # Calculate game score per game\n",
    "    out['game_score_total'] = (\n",
    "        out['total_points'] + \n",
    "        0.4 * out['total_fgm'] - \n",
    "        0.7 * out['total_fga'] - \n",
    "        0.4 * (out['total_fta'] - out['total_ftm']) +\n",
    "        0.7 * out['total_reb_off'] + \n",
    "        0.3 * out['total_reb_def'] + \n",
    "        out['total_steals'] + \n",
    "        0.7 * out['total_assists'] + \n",
    "        0.7 * out['total_blocks'] - \n",
    "        0.4 * out['total_pf'] - \n",
    "        out['total_tov']\n",
    "    )\n",
    "    \n",
    "    # Convert to per-36 minute rate\n",
    "    out['game_score_per36'] = np.where(\n",
    "        out['total_minutes'] > 0,\n",
    "        out['game_score_total'] * 36 / out['total_minutes'],\n",
    "        0.0\n",
    "    )\n",
    "    \n",
    "    created.extend(['game_score_total', 'game_score_per36'])\n",
    "    return out, created\n",
    "\n",
    "\n",
    "def create_multi_target_datasets(df_engineered: pd.DataFrame,\n",
    "                                numerical_features: List[str],\n",
    "                                nominal_categoricals: List[str], \n",
    "                                ordinal_categoricals: List[str],\n",
    "                                y_variables: List[str],\n",
    "                                strategy: str = \"filter_complete\",\n",
    "                                test_seasons: Optional[List] = None,\n",
    "                                season_col: str = \"season_start_year\",\n",
    "                                verbose: bool = True) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Create ML datasets for multiple target variables with target-specific features.\n",
    "    - Encodes categoricals with explicit ordinal order (e.g., minutes_tier).\n",
    "    - Returns encoders for re-use at prediction time.\n",
    "    - Runs a lag audit for sanity (prints only).\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"\\n📋 CREATING MULTI-TARGET ML DATASETS\")\n",
    "        print(\"=\" * 45)\n",
    "        print(f\"🎯 Targets: {y_variables}\")\n",
    "        print(f\"📊 Strategy: {strategy}\")\n",
    "\n",
    "    # Defensive: audit lag features (print-only)\n",
    "    try:\n",
    "        audit_lag_feature_integrity(df_engineered, verbose=verbose)\n",
    "    except Exception as e:\n",
    "        print(f\"[audit] ERROR during lag integrity check: {e}\")\n",
    "\n",
    "    # 1) Encode categoricals ONCE and keep encoders for reuse\n",
    "    if verbose:\n",
    "        print(\"[step] Encoding categoricals...\")\n",
    "    df_processed, enc_bundle = encode_categoricals(\n",
    "        df_engineered,\n",
    "        nominal_categoricals=nominal_categoricals,\n",
    "        ordinal_categoricals=ordinal_categoricals,\n",
    "        strict=True,\n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "    # 2) Temporal split config\n",
    "    if test_seasons is None:\n",
    "        test_seasons = ML_CONFIG.TEST_YEARS\n",
    "\n",
    "    train_mask = ~df_processed[season_col].isin(test_seasons)\n",
    "    test_mask = df_processed[season_col].isin(test_seasons)\n",
    "\n",
    "    results = {\n",
    "        'datasets': {},\n",
    "        'encoders': enc_bundle,                # <--- new\n",
    "        'label_encoders': enc_bundle.raw_label_encoders,  # keep old key for compatibility\n",
    "        'train_seasons': sorted(df_processed[train_mask][season_col].unique()),\n",
    "        'test_seasons': sorted(df_processed[test_mask][season_col].unique())\n",
    "    }\n",
    "\n",
    "    # 3) Build target-specific datasets\n",
    "    for target in y_variables:\n",
    "        if target not in df_processed.columns:\n",
    "            if verbose:\n",
    "                print(f\"⚠️  Target '{target}' not found in data, skipping\")\n",
    "            continue\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n🎯 Processing target: {target}\")\n",
    "\n",
    "        target_features = create_target_specific_features(\n",
    "            target, numerical_features, nominal_categoricals, ordinal_categoricals\n",
    "        )\n",
    "        available_features = [f for f in target_features if f in df_processed.columns]\n",
    "\n",
    "        # Leakage sanity\n",
    "        if not validate_target_feature_separation(target, available_features, verbose):\n",
    "            if verbose:\n",
    "                print(f\"   ⚠️  Skipping {target} due to target leakage candidates\")\n",
    "            continue\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"   📊 Target-specific features: {len(available_features)}\")\n",
    "\n",
    "        # Filter for complete cases if requested\n",
    "        target_mask = df_processed[target].notna()\n",
    "        if strategy == \"filter_complete\":\n",
    "            feat_mask = df_processed[available_features].notna().all(axis=1)\n",
    "            full_mask = target_mask & feat_mask\n",
    "        else:\n",
    "            full_mask = target_mask\n",
    "\n",
    "        train_data = df_processed[train_mask & full_mask]\n",
    "        test_data  = df_processed[test_mask & full_mask]\n",
    "\n",
    "        if len(train_data) == 0 or len(test_data) == 0:\n",
    "            if verbose:\n",
    "                print(f\"   ⚠️  Insufficient data for {target} (train: {len(train_data)}, test: {len(test_data)})\")\n",
    "            continue\n",
    "\n",
    "        # hard-stop on leftover NA — do not mask issues\n",
    "        n_train_na = train_data[available_features + [target]].isna().sum().sum()\n",
    "        n_test_na  = test_data[available_features + [target]].isna().sum().sum()\n",
    "        if n_train_na or n_test_na:\n",
    "            raise ValueError(\n",
    "                f\"[create_multi_target_datasets] Found NA after 'filter_complete' \"\n",
    "                f\"for {target} (train NA={n_train_na}, test NA={n_test_na}). Diagnose upstream lag/joins.\"\n",
    "            )\n",
    "\n",
    "        X_train = train_data[available_features].copy()\n",
    "        y_train = train_data[target].copy()\n",
    "        X_test  = test_data[available_features].copy()\n",
    "        y_test  = test_data[target].copy()\n",
    "\n",
    "        results['datasets'][target] = {\n",
    "            'X_train': X_train,\n",
    "            'y_train': y_train,\n",
    "            'X_test':  X_test,\n",
    "            'y_test':  y_test,\n",
    "            'feature_names': available_features,\n",
    "            'train_size': len(X_train),\n",
    "            'test_size': len(X_test),\n",
    "            'target_name': target\n",
    "        }\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"   ✅ Train: {len(X_train)}, Test: {len(X_test)}, Features: {len(available_features)}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "def train_multi_target_models(datasets: Dict[str, Any],\n",
    "                             numerical_features: List[str],\n",
    "                             importance_threshold: float = 0.001,\n",
    "                             max_features_per_target: Optional[int] = None,\n",
    "                             n_importance_repeats: int = 10,\n",
    "                             verbose: bool = True) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Train models for multiple targets with permutation importance filtering.\n",
    "    Uses target-specific features to prevent leakage.\n",
    "    \n",
    "    Returns:\n",
    "        Dict containing trained models, importance scores, and filtered features\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"\\n🤖 TRAINING MULTI-TARGET MODELS WITH IMPORTANCE FILTERING\")\n",
    "        print(\"=\" * 65)\n",
    "    \n",
    "    results = {\n",
    "        'models': {},\n",
    "        'importance_scores': {},\n",
    "        'filtered_features': {},\n",
    "        'evaluation_metrics': {}\n",
    "    }\n",
    "    \n",
    "    for target_name, data in datasets['datasets'].items():\n",
    "        if verbose:\n",
    "            print(f\"\\n🎯 Training model for: {target_name}\")\n",
    "            print(\"-\" * 40)\n",
    "        \n",
    "        X_train = data['X_train']\n",
    "        y_train = data['y_train']\n",
    "        X_test = data['X_test']\n",
    "        y_test = data['y_test']\n",
    "        feature_names = data['feature_names']\n",
    "        \n",
    "        # Get target-specific numerical features (subset of all numerical features)\n",
    "        target_numerical = [f for f in numerical_features if f in feature_names]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"   📊 Using {len(target_numerical)} target-specific numerical features\")\n",
    "        \n",
    "        # Train initial model on all features\n",
    "        model = RandomForestRegressor(\n",
    "            n_estimators=100,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Calculate permutation importance (only for target-specific numerical features)\n",
    "        importance_df = calculate_permutation_importance(\n",
    "            X_train, y_train, X_test, y_test,\n",
    "            target_name, target_numerical, None,  # Pass None to train new model\n",
    "            n_repeats=n_importance_repeats, verbose=verbose\n",
    "        )\n",
    "        \n",
    "        results['importance_scores'][target_name] = importance_df\n",
    "        \n",
    "        # Filter features based on importance\n",
    "        if not importance_df.empty:\n",
    "            important_features = filter_features_by_importance(\n",
    "                importance_df, importance_threshold, max_features_per_target,\n",
    "                target_name, verbose=verbose\n",
    "            )\n",
    "            \n",
    "            # Add back categorical features (they weren't in permutation importance)\n",
    "            # Exclude 'prediction_season' to avoid unseen category issues\n",
    "            categorical_features = [f for f in feature_names \n",
    "                                  if f not in numerical_features and f in X_train.columns and f != \"prediction_season\"]\n",
    "            final_features = important_features + categorical_features\n",
    "            \n",
    "            # Retrain with filtered features\n",
    "            if final_features:\n",
    "                X_train_filtered = X_train[final_features]\n",
    "                X_test_filtered = X_test[final_features]\n",
    "                \n",
    "                final_model = RandomForestRegressor(\n",
    "                    n_estimators=100,\n",
    "                    random_state=42,\n",
    "                    n_jobs=-1\n",
    "                )\n",
    "                final_model.fit(X_train_filtered, y_train)\n",
    "                \n",
    "                # Evaluate filtered model\n",
    "                y_pred = final_model.predict(X_test_filtered)\n",
    "                metrics = {\n",
    "                    'r2': r2_score(y_test, y_pred),\n",
    "                    'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "                    'mae': mean_absolute_error(y_test, y_pred)\n",
    "                }\n",
    "                \n",
    "                results['models'][target_name] = final_model\n",
    "                results['filtered_features'][target_name] = final_features\n",
    "                results['evaluation_metrics'][target_name] = metrics\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"   ✅ Final model R²: {metrics['r2']:.3f}, RMSE: {metrics['rmse']:.4f}\")\n",
    "                    print(f\"   📊 Final features: {len(final_features)} ({len(important_features)} numerical + {len(categorical_features)} categorical)\")\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(f\"   ⚠️  No features passed importance threshold for {target_name}\")\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(f\"   ⚠️  Could not calculate importance for {target_name}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def save_feature_importance_results(results: Dict[str, Any],\n",
    "                                  output_dir: Path,\n",
    "                                  verbose: bool = True) -> None:\n",
    "    \"\"\"\n",
    "    Save feature importance results and final feature lists.\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"\\n💾 SAVING FEATURE IMPORTANCE RESULTS\")\n",
    "        print(\"=\" * 40)\n",
    "    \n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save importance scores for each target\n",
    "    for target_name, importance_df in results['importance_scores'].items():\n",
    "        if not importance_df.empty:\n",
    "            importance_path = output_dir / f\"{target_name}_permutation_importance.csv\"\n",
    "            importance_df.to_csv(importance_path, index=False)\n",
    "            if verbose:\n",
    "                print(f\"   📈 {target_name} importance: {importance_path}\")\n",
    "    \n",
    "    # Save filtered features for each target\n",
    "    filtered_features_summary = {}\n",
    "    for target_name, features in results['filtered_features'].items():\n",
    "        filtered_features_summary[target_name] = {\n",
    "            'features': features,\n",
    "            'count': len(features)\n",
    "        }\n",
    "    \n",
    "    features_path = output_dir / \"filtered_features_summary.json\"\n",
    "    with open(features_path, 'w') as f:\n",
    "        json.dump(filtered_features_summary, f, indent=2)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"   📋 Filtered features: {features_path}\")\n",
    "    \n",
    "    # Save evaluation metrics\n",
    "    metrics_path = output_dir / \"model_evaluation_metrics.json\"\n",
    "    with open(metrics_path, 'w') as f:\n",
    "        json.dump(results['evaluation_metrics'], f, indent=2, default=str)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"   📊 Evaluation metrics: {metrics_path}\")\n",
    "\n",
    "\n",
    "def print_final_results(results: Dict[str, Any], verbose: bool = True) -> None:\n",
    "    \"\"\"\n",
    "    Print comprehensive final results summary.\n",
    "    \"\"\"\n",
    "    if not verbose:\n",
    "        return\n",
    "        \n",
    "    print(f\"\\n🎉 FINAL RESULTS SUMMARY\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    for target_name in results['models'].keys():\n",
    "        print(f\"\\n🎯 TARGET: {target_name.upper()}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Model performance\n",
    "        if target_name in results['evaluation_metrics']:\n",
    "            metrics = results['evaluation_metrics'][target_name]\n",
    "            print(f\"📊 Model Performance:\")\n",
    "            print(f\"   R²: {metrics['r2']:.4f}\")\n",
    "            print(f\"   RMSE: {metrics['rmse']:.4f}\") \n",
    "            print(f\"   MAE: {metrics['mae']:.4f}\")\n",
    "        \n",
    "        # Feature count\n",
    "        if target_name in results['filtered_features']:\n",
    "            features = results['filtered_features'][target_name]\n",
    "            print(f\"📈 Features Used: {len(features)}\")\n",
    "        \n",
    "        # Top importance scores\n",
    "        if target_name in results['importance_scores']:\n",
    "            importance_df = results['importance_scores'][target_name]\n",
    "            if not importance_df.empty:\n",
    "                print(f\"🏆 Top 5 Most Important Features:\")\n",
    "                top_5 = importance_df.head(5)\n",
    "                for i, (_, row) in enumerate(top_5.iterrows(), 1):\n",
    "                    print(f\"   {i}. {row['feature']:<25} {row['importance_mean']:.4f}\")\n",
    "\n",
    "\n",
    "def create_target_specific_features(target_name: str, \n",
    "                                   base_numerical_features: List[str],\n",
    "                                   nominal_categoricals: List[str],\n",
    "                                   ordinal_categoricals: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Create target-specific feature lists to prevent leakage.\n",
    "\n",
    "    We only forbid the *current* target (and trivially identical transforms),\n",
    "    but DO allow *lagged* versions (e.g., season_pie_lag1) because those are valid\n",
    "    predictors for next season.\n",
    "\n",
    "    Returns:\n",
    "        List of features appropriate for the specific target\n",
    "    \"\"\"\n",
    "    target_name = str(target_name).strip()\n",
    "\n",
    "    # Disallow contemporaneous target columns (and obvious direct twins)\n",
    "    hard_exclusions = {\n",
    "        'season_pie': {'season_pie'},               # forbid current target only\n",
    "        'game_score_per36': {'game_score_per36'},   # forbid current target only\n",
    "    }\n",
    "    exclusions = hard_exclusions.get(target_name, set())\n",
    "\n",
    "    # keep only allowed features; lags remain allowed (e.g., *_lag1)\n",
    "    safe_numerical = [f for f in base_numerical_features if f not in exclusions]\n",
    "\n",
    "    # Combine all feature types\n",
    "    all_features = safe_numerical + nominal_categoricals + ordinal_categoricals\n",
    "    return all_features\n",
    "\n",
    "\n",
    "def validate_target_feature_separation(target_name: str, \n",
    "                                       features: List[str], \n",
    "                                       verbose: bool = True) -> bool:\n",
    "    \"\"\"\n",
    "    Validate that features don't contain contemporaneous target leakage.\n",
    "\n",
    "    We explicitly allow lagged target features (e.g., season_pie_lag1).\n",
    "    \"\"\"\n",
    "    target_name = str(target_name).strip()\n",
    "    contemporaneous = {target_name}\n",
    "\n",
    "    leaked = [f for f in features if f in contemporaneous]\n",
    "    if leaked:\n",
    "        if verbose:\n",
    "            print(f\"⚠️  TARGET LEAKAGE DETECTED for {target_name}: {leaked} (current target in features)\")\n",
    "        return False\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"✅ No contemporaneous target leakage for {target_name} (lags allowed)\")\n",
    "    return True\n",
    "\n",
    "\n",
    "def generate_and_save_predictions(\n",
    "    df_engineered: pd.DataFrame,\n",
    "    datasets: Dict[str, Any],\n",
    "    model_results: Dict[str, Any],\n",
    "    season_col: str = \"season_start_year\",\n",
    "    id_cols: list[str] = [\"personId\", \"player_name\"],\n",
    "    verbose: bool = True\n",
    ") -> dict[str, Path]:\n",
    "    \"\"\"\n",
    "    Generate predictions for ML_CONFIG.PREDICTION_YEAR using models and features\n",
    "    learned on historical data. Applies the SAME encoders used during dataset creation.\n",
    "\n",
    "    Saves one parquet per target:\n",
    "      .../predictions/{target}_predictions_{PREDICTION_YEAR}.parquet\n",
    "    \"\"\"\n",
    "    enc_bundle: EncodersBundle = datasets.get(\"encoders\")\n",
    "    if enc_bundle is None:\n",
    "        raise RuntimeError(\"[generate_and_save_predictions] Missing encoders in 'datasets'. \"\n",
    "                           \"Ensure create_multi_target_datasets() returned 'encoders'.\")\n",
    "\n",
    "    pred_year   = ML_CONFIG.PREDICTION_YEAR\n",
    "    source_year = ML_CONFIG.SOURCE_YEAR\n",
    "\n",
    "    # Base rows for predictions: use last observed season (source_year)\n",
    "    base = df_engineered.loc[df_engineered[season_col] == source_year].copy()\n",
    "    if verbose:\n",
    "        print(f\"[predict] Base rows from season={source_year}: {len(base)}\")\n",
    "\n",
    "    saved_paths: dict[str, Path] = {}\n",
    "\n",
    "    for target, model in model_results.get(\"models\", {}).items():\n",
    "        final_feats = model_results[\"filtered_features\"].get(target)\n",
    "        if not final_feats:\n",
    "            print(f\"[predict] ⚠️ No final features recorded for target '{target}', skipping.\")\n",
    "            continue\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n[predict] Target={target}\")\n",
    "            print(f\"[predict] Using {len(final_feats)} features\")\n",
    "\n",
    "        # Assemble X_pred from base\n",
    "        missing = [c for c in final_feats if c not in base.columns]\n",
    "        if missing:\n",
    "            raise KeyError(f\"[predict] Base frame missing features for '{target}': {missing}\")\n",
    "\n",
    "        X_pred_raw = base[final_feats].copy()\n",
    "\n",
    "        # Apply encoders to X_pred to match training\n",
    "        X_pred = apply_encoders_to_frame(X_pred_raw, enc_bundle, strict=True, verbose=verbose)\n",
    "\n",
    "        # Final guard: no strings allowed\n",
    "        ensure_numeric_matrix(X_pred, context=f\"X_pred ({target})\")\n",
    "\n",
    "        # Predict\n",
    "        y_hat = model.predict(X_pred)\n",
    "\n",
    "        # Assemble output\n",
    "        pred_df = base[id_cols + [season_col]].copy()\n",
    "        pred_df[\"prediction_season\"] = int(pred_year)\n",
    "        pred_df[f\"{target}_pred\"] = y_hat\n",
    "\n",
    "        # Save\n",
    "        path = CFG.predictions_path(target, year=pred_year)\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        pred_df.to_parquet(path, index=False)\n",
    "        saved_paths[target] = path\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"[predict] Saved {target} predictions → {path}\")\n",
    "\n",
    "    return saved_paths\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Enhanced AutomatedMLPipeline class with multi-target support\n",
    "class EnhancedMultiTargetMLPipeline:\n",
    "    \"\"\"\n",
    "    Enhanced ML Pipeline with multi-target support and feature importance filtering.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 numerical_features: List[str],\n",
    "                 nominal_categoricals: List[str],\n",
    "                 ordinal_categoricals: List[str], \n",
    "                 y_variables: List[str],\n",
    "                 importance_threshold: float = 0.001,\n",
    "                 max_features_per_target: Optional[int] = None,\n",
    "                 verbose: bool = True):\n",
    "        \n",
    "        self.numerical_features = numerical_features\n",
    "        self.nominal_categoricals = nominal_categoricals\n",
    "        self.ordinal_categoricals = ordinal_categoricals\n",
    "        self.y_variables = y_variables\n",
    "        self.importance_threshold = importance_threshold\n",
    "        self.max_features_per_target = max_features_per_target\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.results = {}\n",
    "        \n",
    "        # Ensure directories exist\n",
    "        CFG.ensure_ml_dirs()\n",
    "    \n",
    "    def run_complete_pipeline(self, df_engineered: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Run the complete enhanced pipeline with multi-target support.\n",
    "        \"\"\"\n",
    "        if self.verbose:\n",
    "            print(\"🚀 ENHANCED MULTI-TARGET ML PIPELINE\")\n",
    "            print(\"=\" * 50)\n",
    "            print(f\"🎯 Targets: {self.y_variables}\")\n",
    "            print(f\"📊 Numerical features: {len(self.numerical_features)}\")\n",
    "            print(f\"🏷️  Categorical features: {len(self.nominal_categoricals + self.ordinal_categoricals)}\")\n",
    "        \n",
    "        # Step 1: Validate features\n",
    "        validation_results = validate_and_evaluate_features(\n",
    "            df_engineered, self.numerical_features, self.nominal_categoricals,\n",
    "            self.ordinal_categoricals, self.y_variables, self.verbose\n",
    "        )\n",
    "        \n",
    "        # Step 2: Create multi-target datasets\n",
    "        datasets = create_multi_target_datasets(\n",
    "            df_engineered, self.numerical_features, self.nominal_categoricals,\n",
    "            self.ordinal_categoricals, self.y_variables, verbose=self.verbose\n",
    "        )\n",
    "        \n",
    "        # Step 3: Train models with importance filtering\n",
    "        model_results = train_multi_target_models(\n",
    "            datasets, self.numerical_features, self.importance_threshold,\n",
    "            self.max_features_per_target, verbose=self.verbose\n",
    "        )\n",
    "        \n",
    "        # Step 4: Save importance results\n",
    "        save_feature_importance_results(\n",
    "            model_results, CFG.ml_evaluation_dir, self.verbose\n",
    "        )\n",
    "        \n",
    "        # Step 5: Generate and save predictions\n",
    "        saved_pred_paths = generate_and_save_predictions(\n",
    "            df_engineered, datasets, model_results, verbose=self.verbose\n",
    "        )\n",
    "        \n",
    "        # Step 6: Print final summary\n",
    "        print_final_results(model_results, self.verbose)\n",
    "        \n",
    "        # Combine all results\n",
    "        self.results = {\n",
    "            'feature_validation': validation_results,\n",
    "            'datasets': datasets,\n",
    "            'model_results': model_results,\n",
    "            'saved_predictions': {k: str(v) for k, v in saved_pred_paths.items()},\n",
    "            'config': {\n",
    "                'numerical_features': self.numerical_features,\n",
    "                'nominal_categoricals': self.nominal_categoricals,\n",
    "                'ordinal_categoricals': self.ordinal_categoricals,\n",
    "                'y_variables': self.y_variables,\n",
    "                'importance_threshold': self.importance_threshold,\n",
    "                'max_features_per_target': self.max_features_per_target\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return self.results\n",
    "\n",
    "\n",
    "# Example usage function\n",
    "def run_enhanced_pipeline_example():\n",
    "    \"\"\"\n",
    "    Example of how to use the enhanced pipeline with your specified features.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Your specified features\n",
    "    numerical_features = [\n",
    "        # lagged features\n",
    "        \"season_pie_lag1\", \"ts_pct_lag1\", \"efg_pct_lag1\", \"fg_pct_lag1\", \"fg3_pct_lag1\", \"ft_pct_lag1\",\n",
    "        \"pts_per36_lag1\", \"ast_per36_lag1\", \"reb_per36_lag1\", \"defensive_per36_lag1\",\n",
    "        \"production_per36_lag1\", \"stocks_per36_lag1\", \"three_point_rate_lag1\", \"ft_rate_lag1\",\n",
    "        \"pts_per_shot_lag1\", \"ast_to_tov_lag1\", \"usage_events_per_min_lag1\", \"usage_per_min_lag1\",\n",
    "        \"games_played_lag1\", \"total_minutes_lag1\", \"total_points_lag1\", \"total_assists_lag1\",\n",
    "        \"total_rebounds_lag1\", \"total_steals_lag1\", \"total_blocks_lag1\", \"total_fga_lag1\",\n",
    "        \"total_fta_lag1\", \"total_3pa_lag1\", \"total_3pm_lag1\", \"total_tov_lag1\", \"win_pct_lag1\",\n",
    "        \"avg_plus_minus_lag1\", \"team_win_pct_final_lag1\",\n",
    "        \"offensive_impact_lag1\", \"two_way_impact_lag1\", \"efficiency_volume_score_lag1\",\n",
    "        \"versatility_score_lag1\", \"shooting_score_lag1\"\n",
    "    ]\n",
    "    \n",
    "    nominal_categoricals = [\"prediction_season\"]\n",
    "    ordinal_categoricals = [\"minutes_tier\"] \n",
    "    y_variables = [\"season_pie\", \"game_score_per36\"]\n",
    "    \n",
    "    # Load your engineered data\n",
    "    from src.heat_data_scientist_2025.data.load_data_utils import load_data_optimized\n",
    "    \n",
    "    df = load_data_optimized(CFG.ml_dataset_path, drop_null_rows=True)\n",
    "    df_engineered, _, _ = engineer_features(df, verbose=True)\n",
    "    \n",
    "    # Add game_score_per36 if needed\n",
    "    if 'game_score_per36' in y_variables and 'game_score_per36' not in df_engineered.columns:\n",
    "        df_engineered, _ = create_game_score_per36_feature(df_engineered)\n",
    "    \n",
    "    # Create and run enhanced pipeline\n",
    "    pipeline = EnhancedMultiTargetMLPipeline(\n",
    "        numerical_features=numerical_features,\n",
    "        nominal_categoricals=nominal_categoricals,\n",
    "        ordinal_categoricals=ordinal_categoricals,\n",
    "        y_variables=y_variables,\n",
    "        importance_threshold=0.001,  # Adjust threshold as needed\n",
    "        max_features_per_target=30,  # Limit features per target\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    results = pipeline.run_complete_pipeline(df_engineered)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the example\n",
    "    results = run_enhanced_pipeline_example()\n",
    "    \n",
    "    print(\"\\n✅ Enhanced pipeline completed!\")\n",
    "    print(\"Check the output directories for saved feature importance and model results.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17cadd95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/heat_data_scientist_2025/ml/leaderboard_compare.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/heat_data_scientist_2025/ml/leaderboard_compare.py\n",
    "\"\"\"\n",
    "Enhanced leaderboard_compare.py with fixes for column naming and code cleanup\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from src.heat_data_scientist_2025.utils.config import CFG, ML_CONFIG\n",
    "\n",
    "# ---------- Improved Helper Functions ----------\n",
    "\n",
    "def _season_str(start_year: int) -> str:\n",
    "    \"\"\"Convert year to season string format (e.g., 2024 -> '2024-25').\"\"\"\n",
    "    return f\"{start_year}-{str((start_year + 1) % 100).zfill(2)}\"\n",
    "\n",
    "def _as_float(s) -> pd.Series:\n",
    "    \"\"\"Safely convert series to float, handling errors gracefully.\"\"\"\n",
    "    if isinstance(s, pd.Series):\n",
    "        return pd.to_numeric(s, errors=\"coerce\").astype(float)\n",
    "    else:\n",
    "        # Handle scalar values\n",
    "        return pd.Series([float(s) if pd.notna(s) else np.nan])\n",
    "\n",
    "def _normalize_season(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"Normalize season strings to YYYY-YY format.\"\"\"\n",
    "    s = s.astype(str).str.strip()\n",
    "    # Already in correct format\n",
    "    mask = s.str.match(r\"^\\d{4}-\\d{2}$\")\n",
    "    if mask.all():\n",
    "        return s\n",
    "    # Convert single year to season format\n",
    "    yo = s.str.extract(r\"(\\d{4})\")[0]\n",
    "    ok = yo.notna()\n",
    "    out = s.copy()\n",
    "    out.loc[ok] = yo[ok] + \"-\" + (yo[ok].astype(int).add(1) % 100).astype(str).str.zfill(2)\n",
    "    return out\n",
    "\n",
    "def _find_prediction_column(df: pd.DataFrame, metric: str, verbose: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    IMPROVED: Intelligently find the prediction column with multiple fallback strategies.\n",
    "    \n",
    "    Tries in order:\n",
    "    1. Direct metric name (e.g., 'game_score_per36')\n",
    "    2. {metric}_pred pattern (e.g., 'game_score_per36_pred')  \n",
    "    3. predicted_{metric} pattern (e.g., 'predicted_game_score_per36')\n",
    "    4. Any column containing 'pred' for single fallback\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing predictions\n",
    "        metric: Target metric name\n",
    "        verbose: Whether to print debug info\n",
    "        \n",
    "    Returns:\n",
    "        Column name containing predictions\n",
    "        \n",
    "    Raises:\n",
    "        KeyError: If no suitable prediction column found\n",
    "    \"\"\"\n",
    "    candidates = [\n",
    "        metric,                           # Direct: 'game_score_per36'\n",
    "        f\"{metric}_pred\",                 # Suffix: 'game_score_per36_pred'  \n",
    "        f\"predicted_{metric}\",            # Prefix: 'predicted_game_score_per36'\n",
    "        f\"pred_{metric}\",                 # Alt prefix: 'pred_game_score_per36'\n",
    "    ]\n",
    "    \n",
    "    # Try exact matches first\n",
    "    for candidate in candidates:\n",
    "        if candidate in df.columns:\n",
    "            if verbose:\n",
    "                print(f\"[pred-col] Found exact match: '{candidate}' for metric '{metric}'\")\n",
    "            return candidate\n",
    "    \n",
    "    # Fallback: any numeric column containing 'pred'\n",
    "    pred_cols = [c for c in df.columns if 'pred' in c.lower() and pd.api.types.is_numeric_dtype(df[c])]\n",
    "    if len(pred_cols) == 1:\n",
    "        if verbose:\n",
    "            print(f\"[pred-col] Using fallback: '{pred_cols[0]}' for metric '{metric}'\")\n",
    "        return pred_cols[0]\n",
    "    elif len(pred_cols) > 1:\n",
    "        raise KeyError(\n",
    "            f\"Ambiguous prediction columns for metric '{metric}': {pred_cols}. \"\n",
    "            f\"Expected one of: {candidates}\"\n",
    "        )\n",
    "    \n",
    "    # No suitable column found\n",
    "    raise KeyError(\n",
    "        f\"No prediction column found for metric '{metric}'. \"\n",
    "        f\"Tried: {candidates}. Available columns: {list(df.columns)}\"\n",
    "    )\n",
    "\n",
    "# ---------- Core Data Loading Functions ----------\n",
    "\n",
    "def _load_hist_minimal(metric: str, minutes_gate: int = 500, verbose: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"Load minimal historical frame from the parquet dataset (≤2024).\"\"\"\n",
    "    need_cols = [\"player_name\", \"season\", \"games_played\", \"total_minutes\", metric]\n",
    "    df = pd.read_parquet(CFG.ml_dataset_path)\n",
    "    \n",
    "    # Keep ≤ 2024 seasons only\n",
    "    start_year = df[\"season\"].astype(str).str.extract(r\"^(\\d{4})\")[0].astype(int)\n",
    "    df = df.loc[start_year <= 2024].copy()\n",
    "\n",
    "    # Handle missing game_score_per36 (compute if possible)\n",
    "    if metric == \"game_score_per36\" and \"game_score_per36\" not in df.columns:\n",
    "        required_for_gs = {\n",
    "            \"total_points\", \"total_fgm\", \"total_fga\", \"total_fta\", \"total_ftm\",\n",
    "            \"total_reb_off\", \"total_reb_def\", \"total_steals\", \"total_assists\",\n",
    "            \"total_blocks\", \"total_pf\", \"total_tov\", \"total_minutes\"\n",
    "        }\n",
    "        if required_for_gs.issubset(df.columns):\n",
    "            if verbose:\n",
    "                print(f\"[COMPUTE] Calculating missing {metric} from component stats\")\n",
    "            game_score_total = (\n",
    "                df[\"total_points\"] + 0.4 * df[\"total_fgm\"] - 0.7 * df[\"total_fga\"]\n",
    "                - 0.4 * (df[\"total_fta\"] - df[\"total_ftm\"]) + 0.7 * df[\"total_reb_off\"]\n",
    "                + 0.3 * df[\"total_reb_def\"] + df[\"total_steals\"] + 0.7 * df[\"total_assists\"]\n",
    "                + 0.7 * df[\"total_blocks\"] - 0.4 * df[\"total_pf\"] - df[\"total_tov\"]\n",
    "            )\n",
    "            df[\"game_score_per36\"] = np.where(\n",
    "                df[\"total_minutes\"] > 0, \n",
    "                game_score_total * 36.0 / df[\"total_minutes\"], \n",
    "                np.nan\n",
    "            )\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(f\"[WARN] Cannot compute {metric} - missing required columns\")\n",
    "            df[metric] = np.nan\n",
    "\n",
    "    # Ensure required columns exist and are numeric\n",
    "    for c in [\"games_played\", \"total_minutes\", metric]:\n",
    "        if c not in df.columns:\n",
    "            df[c] = np.nan\n",
    "        df[c] = _as_float(df[c])\n",
    "\n",
    "    # Apply minutes gate and clean up\n",
    "    df = df.loc[df[\"total_minutes\"] >= minutes_gate].copy()\n",
    "    return df[need_cols].dropna(subset=[metric])\n",
    "\n",
    "def _load_predictions(metric: str, prediction_year: int, verbose: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    IMPROVED: Load predictions with flexible column detection and better error handling.\n",
    "    \"\"\"\n",
    "    pth = CFG.predictions_path(metric, prediction_year)\n",
    "    if verbose:\n",
    "        print(f\"[LOAD] Loading predictions: {pth}\")\n",
    "    \n",
    "    if not pth.exists():\n",
    "        raise FileNotFoundError(f\"Predictions file not found: {pth}\")\n",
    "        \n",
    "    df = pd.read_parquet(pth)\n",
    "    if verbose:\n",
    "        print(f\"[LOAD] Loaded {len(df):,} prediction rows with columns: {list(df.columns)}\")\n",
    "\n",
    "    # Use improved column detection\n",
    "    pred_col = _find_prediction_column(df, metric, verbose=verbose)\n",
    "    \n",
    "    # Ensure required ID columns\n",
    "    required_id_cols = [\"player_name\"]\n",
    "    missing_id = [c for c in required_id_cols if c not in df.columns]\n",
    "    if missing_id:\n",
    "        raise KeyError(f\"Predictions missing required columns: {missing_id}\")\n",
    "\n",
    "    # Build output dataframe\n",
    "    out = pd.DataFrame({\n",
    "        \"player_name\": df[\"player_name\"].astype(str),\n",
    "        \"season\": _season_str(prediction_year),\n",
    "        metric: _as_float(df[pred_col]),\n",
    "        \"games_played\": _as_float(df.get(\"games_played\", np.nan)),\n",
    "        \"total_minutes\": _as_float(df.get(\"total_minutes\", np.nan)),\n",
    "        \"source\": \"pred\"\n",
    "    })\n",
    "    \n",
    "    # Remove null predictions\n",
    "    before_len = len(out)\n",
    "    out = out.dropna(subset=[metric])\n",
    "    if verbose and len(out) != before_len:\n",
    "        print(f\"[CLEAN] Removed {before_len - len(out)} null predictions\")\n",
    "    \n",
    "    return out\n",
    "\n",
    "# ---------- Ranking and Comparison Functions ----------\n",
    "\n",
    "def _rank_three_buckets(df: pd.DataFrame, metric: str, top_n: int = 10, \n",
    "                        middle_n: int = 10, bottom_n: int = 10) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Create top/middle/bottom rankings with consistent tie-breaking.\n",
    "    \n",
    "    Tie-breakers (in order):\n",
    "    - Top: metric desc, total_minutes desc, games_played desc, player_name asc\n",
    "    - Bottom: metric asc, total_minutes desc, games_played desc, player_name asc  \n",
    "    - Middle: distance to median asc, then same as top\n",
    "    \"\"\"\n",
    "    use = df.loc[df[metric].notna()].copy()\n",
    "    \n",
    "    # Ensure tie-breaker columns exist (fill NaN with 0 for sorting only)\n",
    "    for col in [\"total_minutes\", \"games_played\"]:\n",
    "        if col not in use.columns:\n",
    "            use[col] = 0.0\n",
    "        else:\n",
    "            use[col] = _as_float(use[col]).fillna(0.0)\n",
    "\n",
    "    # Top rankings (highest metric values)\n",
    "    top = (\n",
    "        use.sort_values([metric, \"total_minutes\", \"games_played\", \"player_name\"],\n",
    "                        ascending=[False, False, False, True], kind=\"stable\")\n",
    "           .head(top_n).copy()\n",
    "    )\n",
    "\n",
    "    # Bottom rankings (lowest metric values)  \n",
    "    bottom = (\n",
    "        use.sort_values([metric, \"total_minutes\", \"games_played\", \"player_name\"],\n",
    "                        ascending=[True, False, False, True], kind=\"stable\")\n",
    "           .head(bottom_n).copy()\n",
    "    )\n",
    "\n",
    "    # Middle rankings (closest to median)\n",
    "    median_val = use[metric].median(skipna=True)\n",
    "    use_middle = use.copy()\n",
    "    use_middle[\"__dist_to_median\"] = (use_middle[metric] - median_val).abs()\n",
    "    middle = (\n",
    "        use_middle.sort_values([\"__dist_to_median\", metric, \"total_minutes\", \"games_played\", \"player_name\"],\n",
    "                               ascending=[True, False, False, False, True], kind=\"stable\")\n",
    "                  .head(middle_n)\n",
    "                  .drop(columns=[\"__dist_to_median\"]).copy()\n",
    "    )\n",
    "\n",
    "    return {\"top\": top, \"middle\": middle, \"bottom\": bottom}\n",
    "\n",
    "def _build_new_boards(hist_df: pd.DataFrame, preds_df: pd.DataFrame,\n",
    "                      metric: str, prediction_year: int) -> Tuple[Dict[str, pd.DataFrame], Dict[str, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    IMPROVED: Combine historical and prediction data to create new leaderboards with notes.\n",
    "    \n",
    "    Returns:\n",
    "        (boards_dict, closest_dict) where boards_dict contains the final top/middle/bottom 10\n",
    "        and closest_dict contains the near-miss predictions for each category.\n",
    "    \"\"\"\n",
    "    # Ensure consistent schema\n",
    "    hist_df = hist_df.copy()\n",
    "    hist_df[\"source\"] = \"historical\"\n",
    "    \n",
    "    preds_df = preds_df.copy()\n",
    "    # Ensure common columns exist with defaults\n",
    "    for col in [\"games_played\", \"total_minutes\"]:\n",
    "        if col not in hist_df.columns:\n",
    "            hist_df[col] = np.nan\n",
    "        if col not in preds_df.columns: \n",
    "            preds_df[col] = np.nan\n",
    "\n",
    "    # Combine datasets\n",
    "    combined = pd.concat([hist_df, preds_df], ignore_index=True, sort=False)\n",
    "    \n",
    "    # Generate rankings from combined data\n",
    "    boards = _rank_three_buckets(combined, metric)\n",
    "\n",
    "    # Add rank numbers and prediction notes\n",
    "    season_tag = _season_str(prediction_year)\n",
    "    for bucket_name, board_df in boards.items():\n",
    "        board_df = board_df.copy()\n",
    "        board_df.insert(0, \"Rank\", range(1, len(board_df) + 1))\n",
    "        \n",
    "        # Add notes for predictions\n",
    "        is_prediction = (board_df.get(\"source\", \"historical\") == \"pred\") | (board_df[\"season\"] == season_tag)\n",
    "        board_df[\"Notes\"] = np.where(is_prediction, f\"NEW {season_tag} prediction\", \"\")\n",
    "        \n",
    "        boards[bucket_name] = board_df\n",
    "\n",
    "    # Generate \"closest miss\" lists for predictions not in top 10 of each bucket\n",
    "    closest = _generate_closest_predictions(combined, boards, metric, prediction_year)\n",
    "\n",
    "    return boards, closest\n",
    "\n",
    "def _generate_closest_predictions(combined_df: pd.DataFrame, boards: Dict[str, pd.DataFrame], \n",
    "                                  metric: str, prediction_year: int, k: int = 10) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"Generate lists of predictions that were closest to making each leaderboard.\"\"\"\n",
    "    season_tag = _season_str(prediction_year)\n",
    "    predictions_only = combined_df.loc[combined_df[\"source\"] == \"pred\"].copy()\n",
    "    \n",
    "    closest = {}\n",
    "    \n",
    "    for bucket_name, board_df in boards.items():\n",
    "        # Get predictions that didn't make this board\n",
    "        board_players = set(zip(board_df[\"player_name\"], board_df[\"season\"]))\n",
    "        missed_preds = predictions_only[\n",
    "            ~predictions_only.apply(lambda r: (r[\"player_name\"], r[\"season\"]) in board_players, axis=1)\n",
    "        ].copy()\n",
    "        \n",
    "        if bucket_name == \"top\":\n",
    "            # For top board: predictions with highest metric values that didn't make it\n",
    "            cutoff_value = board_df[metric].min() if not board_df.empty else float('-inf')\n",
    "            missed_preds[\"gap_to_cutoff\"] = cutoff_value - missed_preds[metric] \n",
    "            closest_missed = (\n",
    "                missed_preds.sort_values([\"gap_to_cutoff\", metric], ascending=[True, False])\n",
    "                           .head(k).copy()\n",
    "            )\n",
    "            \n",
    "        elif bucket_name == \"bottom\":\n",
    "            # For bottom board: predictions with lowest metric values that didn't make it  \n",
    "            cutoff_value = board_df[metric].max() if not board_df.empty else float('inf')\n",
    "            missed_preds[\"gap_to_cutoff\"] = missed_preds[metric] - cutoff_value\n",
    "            closest_missed = (\n",
    "                missed_preds.sort_values([\"gap_to_cutoff\", metric], ascending=[True, True])\n",
    "                           .head(k).copy()\n",
    "            )\n",
    "            \n",
    "        else:  # middle\n",
    "            # For middle board: predictions closest to median that didn't make it\n",
    "            median_val = combined_df[metric].median(skipna=True)\n",
    "            missed_preds[\"gap_to_median\"] = (missed_preds[metric] - median_val).abs()\n",
    "            closest_missed = (\n",
    "                missed_preds.sort_values([\"gap_to_median\", metric], ascending=[True, False])\n",
    "                           .head(k).copy()\n",
    "            )\n",
    "\n",
    "        # Add helpful metadata\n",
    "        closest_missed[\"Notes\"] = f\"Closest {season_tag} prediction to {bucket_name}\"\n",
    "        closest[f\"closest_{bucket_name}\"] = closest_missed.reset_index(drop=True)\n",
    "    \n",
    "    return closest\n",
    "\n",
    "# ---------- Main Public API ----------\n",
    "\n",
    "def build_leaderboards_with_predictions(metrics: Tuple[str, ...] = (\"game_score_per36\", \"season_pie\"),\n",
    "                                        prediction_year: int = 2025,\n",
    "                                        save: bool = True, \n",
    "                                        verbose: bool = True) -> Dict[str, Dict[str, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    IMPROVED: Build comprehensive leaderboards combining historical data with predictions.\n",
    "    \n",
    "    Creates top/middle/bottom-10 leaderboards for each metric, includes 2025 predictions,\n",
    "    adds notes for newcomers, and generates 'closest miss' lists.\n",
    "    \n",
    "    Args:\n",
    "        metrics: Tuple of metric names to process\n",
    "        prediction_year: Year of predictions to include\n",
    "        save: Whether to save results to CSV files\n",
    "        verbose: Whether to print progress information\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of results: {metric: {\"boards\": {...}, \"closest\": {...}}}\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"\\n🏆 BUILDING COMPREHENSIVE LEADERBOARDS\")\n",
    "        print(\"=\" * 45)\n",
    "        print(f\"📊 Metrics: {metrics}\")\n",
    "        print(f\"📅 Prediction year: {prediction_year}\")\n",
    "    \n",
    "    results = {}\n",
    "    output_dir = CFG.ml_predictions_dir\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for metric in metrics:\n",
    "        if verbose:\n",
    "            print(f\"\\n📈 Processing metric: {metric}\")\n",
    "            print(\"-\" * 30)\n",
    "        \n",
    "        try:\n",
    "            # Load historical data and predictions\n",
    "            hist_df = _load_hist_minimal(metric, verbose=verbose)\n",
    "            pred_df = _load_predictions(metric, prediction_year, verbose=verbose)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"✅ Historical data: {len(hist_df):,} player-seasons\")\n",
    "                print(f\"✅ Predictions: {len(pred_df):,} players for {prediction_year}\")\n",
    "\n",
    "            # Build combined leaderboards\n",
    "            boards, closest = _build_new_boards(hist_df, pred_df, metric, prediction_year)\n",
    "            results[metric] = {\"boards\": boards, \"closest\": closest}\n",
    "\n",
    "            if save:\n",
    "                # Save main leaderboards\n",
    "                for bucket_name, board_df in boards.items():\n",
    "                    board_path = output_dir / f\"{metric}_{bucket_name}_leaderboard_{prediction_year}_with_predictions.csv\"\n",
    "                    board_df.to_csv(board_path, index=False)\n",
    "                    if verbose:\n",
    "                        print(f\"💾 Saved {bucket_name} leaderboard: {board_path.name}\")\n",
    "\n",
    "                # Save closest miss lists  \n",
    "                closest_path = output_dir / f\"{metric}_closest_misses_{prediction_year}.csv\"\n",
    "                closest_combined = pd.concat(\n",
    "                    closest.values(), \n",
    "                    keys=list(closest.keys())\n",
    "                ).reset_index(level=0).rename(columns={\"level_0\": \"category\"})\n",
    "                closest_combined.to_csv(closest_path, index=False)\n",
    "                if verbose:\n",
    "                    print(f\"💾 Saved closest misses: {closest_path.name}\")\n",
    "\n",
    "            # Print summary for this metric\n",
    "            if verbose:\n",
    "                print(f\"\\n📋 {metric.upper()} SUMMARY:\")\n",
    "                for bucket_name, board_df in boards.items():\n",
    "                    pred_count = (board_df.get(\"source\", \"historical\") == \"pred\").sum()\n",
    "                    print(f\"   {bucket_name.title()}: {pred_count}/10 are 2025 predictions\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {metric}: {str(e)}\")\n",
    "            if verbose:\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "            continue\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\n✅ Completed leaderboard generation for {len(results)} metrics\")\n",
    "        \n",
    "    return results\n",
    "\n",
    "def create_simple_leaderboards_from_predictions(metrics: Tuple[str, ...] = (\"game_score_per36\", \"season_pie\"),\n",
    "                                                prediction_year: int = 2025,\n",
    "                                                top_n: int = 50,\n",
    "                                                verbose: bool = True) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    IMPROVED: Create simple CSV leaderboards from prediction parquet files.\n",
    "    \n",
    "    This creates the basic leaderboard files that other analysis functions expect,\n",
    "    with improved column detection and error handling.\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"\\n📊 CREATING SIMPLE LEADERBOARDS\")\n",
    "        print(\"=\" * 35)\n",
    "    \n",
    "    CFG.ensure_ml_dirs()\n",
    "    results = {}\n",
    "    \n",
    "    for metric in metrics:\n",
    "        if verbose:\n",
    "            print(f\"\\n🏆 Processing {metric}\")\n",
    "            print(\"-\" * 25)\n",
    "        \n",
    "        try:\n",
    "            # Load predictions \n",
    "            pred_path = CFG.predictions_path(metric, prediction_year)\n",
    "            if not pred_path.exists():\n",
    "                print(f\"❌ Predictions not found: {pred_path}\")\n",
    "                continue\n",
    "                \n",
    "            preds_df = pd.read_parquet(pred_path)\n",
    "            if verbose:\n",
    "                print(f\"✅ Loaded {len(preds_df):,} predictions\")\n",
    "\n",
    "            # Find prediction column using improved detection\n",
    "            pred_col = _find_prediction_column(preds_df, metric, verbose=verbose)\n",
    "            \n",
    "            # Create leaderboard\n",
    "            leaderboard_cols = [\"player_name\", pred_col]\n",
    "            missing_cols = [c for c in leaderboard_cols if c not in preds_df.columns]\n",
    "            if missing_cols:\n",
    "                print(f\"❌ Missing columns: {missing_cols}\")\n",
    "                continue\n",
    "            \n",
    "            # Build leaderboard dataframe\n",
    "            leaderboard_df = preds_df[leaderboard_cols].copy()\n",
    "            leaderboard_df = leaderboard_df.dropna(subset=[pred_col])\n",
    "            \n",
    "            # Add season if not present\n",
    "            if \"season\" not in leaderboard_df.columns:\n",
    "                leaderboard_df[\"season\"] = _season_str(prediction_year)\n",
    "            \n",
    "            # Sort and rank\n",
    "            leaderboard_df = leaderboard_df.sort_values(\n",
    "                [pred_col, \"player_name\"], \n",
    "                ascending=[False, True]\n",
    "            ).head(top_n).reset_index(drop=True)\n",
    "            \n",
    "            # Clean up column names and add rank\n",
    "            leaderboard_df = leaderboard_df.rename(columns={pred_col: metric})\n",
    "            leaderboard_df.insert(0, \"rank\", range(1, len(leaderboard_df) + 1))\n",
    "            leaderboard_df[metric] = leaderboard_df[metric].round(6)\n",
    "            \n",
    "            # Save leaderboard\n",
    "            lb_path = CFG.leaderboard_path(metric, prediction_year)\n",
    "            leaderboard_df.to_csv(lb_path, index=False)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"✅ Saved: {lb_path.name}\")\n",
    "                print(f\"📊 Top 3: {leaderboard_df.head(3)['player_name'].tolist()}\")\n",
    "            \n",
    "            results[metric] = leaderboard_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error with {metric}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n✅ Created {len(results)} simple leaderboards\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a85a7b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting FIXED Enhanced Multi-Target Pipeline...\n",
      "🚀 FIXED ENHANCED MULTI-TARGET PIPELINE\n",
      "=============================================\n",
      "🎯 Targets: season_pie, game_score_per36\n",
      "📊 Total Features: 40\n",
      "=============================================\n",
      "\n",
      "📊 LOADING AND ENGINEERING DATA\n",
      "===================================\n",
      "Loading data for enhanced comprehensive EDA...\n",
      "→ Applying null dropping: how='any', all columns\n",
      "✓ Dropped 0 rows by null criteria (how='any', subset=None); remaining 5,575 rows\n",
      "✓ Dataset loaded: 5,575 rows × 55 columns in 0.04s\n",
      "Starting feature engineering...\n",
      "Parsing seasons...\n",
      "Adding experience features...\n",
      "Adding advanced metrics...\n",
      "Adding usage features...\n",
      "Adding minutes features...\n",
      "Adding performance consistency...\n",
      "Creating composite features...\n",
      "Building portability index...\n",
      "Creating lag features...\n",
      "✓ Lag nulls confirmed as first seasons only (1152 rows)\n",
      "Dropped 1152 first-season rows with null lags\n",
      "Feature engineering complete: 5575 → 4423 rows, 55 → 174 columns\n",
      "✅ Feature engineering completed successfully\n",
      "\n",
      "🏆 Top 5 season_pie players:\n",
      "   LeBron James: 0.174857\n",
      "   LeBron James: 0.174495\n",
      "   Russell Westbrook: 0.171554\n",
      "   Kevin Durant: 0.169186\n",
      "   Nikola Jokic: 0.166170\n",
      "\n",
      "🤖 RUNNING ML PIPELINE\n",
      "=========================\n",
      "🚀 ENHANCED MULTI-TARGET ML PIPELINE\n",
      "==================================================\n",
      "🎯 Targets: ['season_pie', 'game_score_per36']\n",
      "📊 Numerical features: 39\n",
      "🏷️  Categorical features: 1\n",
      "🔍 VALIDATING AND EVALUATING SPECIFIED FEATURES\n",
      "=======================================================\n",
      "\n",
      "📊 Numerical Features:\n",
      "   Available: 39/39 (100.0%)\n",
      "   Completeness:\n",
      "     season_start_year         100.0%\n",
      "     season_pie_lag1           100.0%\n",
      "     ts_pct_lag1               100.0%\n",
      "     efg_pct_lag1              100.0%\n",
      "     fg_pct_lag1               100.0%\n",
      "     ... and 34 more\n",
      "\n",
      "📊 Nominal Categoricals:\n",
      "   Available: 0/0 (n/a — no features specified)\n",
      "   Completeness: n/a\n",
      "\n",
      "📊 Ordinal Categoricals:\n",
      "   Available: 1/1 (100.0%)\n",
      "   Completeness:\n",
      "     minutes_tier              100.0%\n",
      "\n",
      "📊 Y Variables:\n",
      "   Available: 2/2 (100.0%)\n",
      "   Completeness:\n",
      "     season_pie                100.0%\n",
      "     game_score_per36          100.0%\n",
      "\n",
      "📈 OVERALL FEATURE AVAILABILITY:\n",
      "   Specified features: 42\n",
      "   Available features: 42\n",
      "   Availability: 100.0%\n",
      "\n",
      "📋 CREATING MULTI-TARGET ML DATASETS\n",
      "=============================================\n",
      "🎯 Targets: ['season_pie', 'game_score_per36']\n",
      "📊 Strategy: filter_complete\n",
      "\n",
      "[audit] Verifying lag feature integrity...\n",
      "[audit] Consecutive season pairs: 3312/3493 (94.8%)\n",
      "[audit] season_pie_lag1 vs shift(season_pie): mismatches among consecutive seasons = 0\n",
      "[step] Encoding categoricals...\n",
      "[encode_categoricals] Encoded: 0 nominal, 1 ordinal\n",
      "\n",
      "🎯 Processing target: season_pie\n",
      "✅ No contemporaneous target leakage for season_pie (lags allowed)\n",
      "   📊 Target-specific features: 40\n",
      "   ✅ Train: 3808, Test: 615, Features: 40\n",
      "\n",
      "🎯 Processing target: game_score_per36\n",
      "✅ No contemporaneous target leakage for game_score_per36 (lags allowed)\n",
      "   📊 Target-specific features: 40\n",
      "   ✅ Train: 3808, Test: 615, Features: 40\n",
      "\n",
      "🤖 TRAINING MULTI-TARGET MODELS WITH IMPORTANCE FILTERING\n",
      "=================================================================\n",
      "\n",
      "🎯 Training model for: season_pie\n",
      "----------------------------------------\n",
      "   📊 Using 39 target-specific numerical features\n",
      "\n",
      "🔄 CALCULATING PERMUTATION IMPORTANCE FOR SEASON_PIE\n",
      "============================================================\n",
      "   📊 Features for importance: 39\n",
      "   🎯 Target: season_pie\n",
      "   🔄 Repeats: 10\n",
      "\n",
      "📈 TOP 10 MOST IMPORTANT FEATURES FOR SEASON_PIE:\n",
      "----------------------------------------------------------------------\n",
      " 1. season_pie_lag1                  1.0627 ± 0.0548\n",
      " 2. production_per36_lag1            0.0033 ± 0.0021\n",
      " 3. total_fta_lag1                   0.0021 ± 0.0023\n",
      " 4. total_tov_lag1                   0.0019 ± 0.0016\n",
      " 5. avg_plus_minus_lag1              0.0019 ± 0.0013\n",
      " 6. fg_pct_lag1                      0.0016 ± 0.0008\n",
      " 7. ast_per36_lag1                   0.0014 ± 0.0007\n",
      " 8. efficiency_volume_score_lag1     0.0012 ± 0.0017\n",
      " 9. ft_rate_lag1                     0.0008 ± 0.0008\n",
      "10. total_fga_lag1                   0.0008 ± 0.0005\n",
      "\n",
      "✂️  FILTERING FEATURES FOR SEASON_PIE\n",
      "==================================================\n",
      "   🎯 Min importance threshold: 0.001\n",
      "   📊 Max features limit: 30\n",
      "   ✅ Features kept: 8/39 (20.5%)\n",
      "   ❌ Features removed: 31\n",
      "   📈 Importance range: 0.0012 to 1.0627\n",
      "   ✅ Final model R²: 0.852, RMSE: 0.0110\n",
      "   📊 Final features: 9 (8 numerical + 1 categorical)\n",
      "\n",
      "🎯 Training model for: game_score_per36\n",
      "----------------------------------------\n",
      "   📊 Using 39 target-specific numerical features\n",
      "\n",
      "🔄 CALCULATING PERMUTATION IMPORTANCE FOR GAME_SCORE_PER36\n",
      "============================================================\n",
      "   📊 Features for importance: 39\n",
      "   🎯 Target: game_score_per36\n",
      "   🔄 Repeats: 10\n",
      "\n",
      "📈 TOP 10 MOST IMPORTANT FEATURES FOR GAME_SCORE_PER36:\n",
      "----------------------------------------------------------------------\n",
      " 1. production_per36_lag1            0.8088 ± 0.0431\n",
      " 2. offensive_impact_lag1            0.0578 ± 0.0052\n",
      " 3. two_way_impact_lag1              0.0071 ± 0.0027\n",
      " 4. fg_pct_lag1                      0.0064 ± 0.0019\n",
      " 5. total_rebounds_lag1              0.0039 ± 0.0015\n",
      " 6. total_tov_lag1                   0.0021 ± 0.0012\n",
      " 7. pts_per36_lag1                   0.0013 ± 0.0007\n",
      " 8. three_point_rate_lag1            0.0013 ± 0.0011\n",
      " 9. ft_pct_lag1                      0.0010 ± 0.0009\n",
      "10. ts_pct_lag1                      0.0006 ± 0.0012\n",
      "\n",
      "✂️  FILTERING FEATURES FOR GAME_SCORE_PER36\n",
      "==================================================\n",
      "   🎯 Min importance threshold: 0.001\n",
      "   📊 Max features limit: 30\n",
      "   ✅ Features kept: 8/39 (20.5%)\n",
      "   ❌ Features removed: 31\n",
      "   📈 Importance range: 0.0013 to 0.8088\n",
      "   ✅ Final model R²: 0.707, RMSE: 2.1612\n",
      "   📊 Final features: 9 (8 numerical + 1 categorical)\n",
      "\n",
      "💾 SAVING FEATURE IMPORTANCE RESULTS\n",
      "========================================\n",
      "   📈 season_pie importance: /workspace/data/processed/heat_data_scientist_2025/evaluation/season_pie_permutation_importance.csv\n",
      "   📈 game_score_per36 importance: /workspace/data/processed/heat_data_scientist_2025/evaluation/game_score_per36_permutation_importance.csv\n",
      "   📋 Filtered features: /workspace/data/processed/heat_data_scientist_2025/evaluation/filtered_features_summary.json\n",
      "   📊 Evaluation metrics: /workspace/data/processed/heat_data_scientist_2025/evaluation/model_evaluation_metrics.json\n",
      "[predict] Base rows from season=2024: 309\n",
      "\n",
      "[predict] Target=season_pie\n",
      "[predict] Using 9 features\n",
      "[predict] Saved season_pie predictions → /workspace/data/processed/heat_data_scientist_2025/predictions/season_pie_predictions_2025.parquet\n",
      "\n",
      "[predict] Target=game_score_per36\n",
      "[predict] Using 9 features\n",
      "[predict] Saved game_score_per36 predictions → /workspace/data/processed/heat_data_scientist_2025/predictions/game_score_per36_predictions_2025.parquet\n",
      "\n",
      "🎉 FINAL RESULTS SUMMARY\n",
      "==============================\n",
      "\n",
      "🎯 TARGET: SEASON_PIE\n",
      "----------------------------------------\n",
      "📊 Model Performance:\n",
      "   R²: 0.8517\n",
      "   RMSE: 0.0110\n",
      "   MAE: 0.0086\n",
      "📈 Features Used: 9\n",
      "🏆 Top 5 Most Important Features:\n",
      "   1. season_pie_lag1           1.0627\n",
      "   2. production_per36_lag1     0.0033\n",
      "   3. total_fta_lag1            0.0021\n",
      "   4. total_tov_lag1            0.0019\n",
      "   5. avg_plus_minus_lag1       0.0019\n",
      "\n",
      "🎯 TARGET: GAME_SCORE_PER36\n",
      "----------------------------------------\n",
      "📊 Model Performance:\n",
      "   R²: 0.7073\n",
      "   RMSE: 2.1612\n",
      "   MAE: 1.6903\n",
      "📈 Features Used: 9\n",
      "🏆 Top 5 Most Important Features:\n",
      "   1. production_per36_lag1     0.8088\n",
      "   2. offensive_impact_lag1     0.0578\n",
      "   3. two_way_impact_lag1       0.0071\n",
      "   4. fg_pct_lag1               0.0064\n",
      "   5. total_rebounds_lag1       0.0039\n",
      "✅ ML Pipeline completed successfully\n",
      "\n",
      "🏆 CREATING SIMPLE LEADERBOARDS\n",
      "===================================\n",
      "\n",
      "📊 CREATING SIMPLE LEADERBOARDS\n",
      "===================================\n",
      "\n",
      "🏆 Processing game_score_per36\n",
      "-------------------------\n",
      "✅ Loaded 309 predictions\n",
      "[pred-col] Found exact match: 'game_score_per36_pred' for metric 'game_score_per36'\n",
      "✅ Saved: game_score_per36_leaderboard_2025.csv\n",
      "📊 Top 3: ['Giannis Antetokounmpo', 'Nikola Jokic', 'Joel Embiid']\n",
      "\n",
      "🏆 Processing season_pie\n",
      "-------------------------\n",
      "✅ Loaded 309 predictions\n",
      "[pred-col] Found exact match: 'season_pie_pred' for metric 'season_pie'\n",
      "✅ Saved: season_pie_leaderboard_2025.csv\n",
      "📊 Top 3: ['Joel Embiid', 'Nikola Jokic', 'Giannis Antetokounmpo']\n",
      "\n",
      "✅ Created 2 simple leaderboards\n",
      "✅ Simple leaderboards created successfully\n",
      "\n",
      "🏆 CREATING COMPREHENSIVE LEADERBOARDS\n",
      "========================================\n",
      "\n",
      "🏆 BUILDING COMPREHENSIVE LEADERBOARDS\n",
      "=============================================\n",
      "📊 Metrics: ('game_score_per36', 'season_pie')\n",
      "📅 Prediction year: 2025\n",
      "\n",
      "📈 Processing metric: game_score_per36\n",
      "------------------------------\n",
      "[LOAD] Loading predictions: /workspace/data/processed/heat_data_scientist_2025/predictions/game_score_per36_predictions_2025.parquet\n",
      "[LOAD] Loaded 309 prediction rows with columns: ['personId', 'player_name', 'season_start_year', 'prediction_season', 'game_score_per36_pred']\n",
      "[pred-col] Found exact match: 'game_score_per36_pred' for metric 'game_score_per36'\n",
      "✅ Historical data: 5,575 player-seasons\n",
      "✅ Predictions: 309 players for 2025\n",
      "💾 Saved top leaderboard: game_score_per36_top_leaderboard_2025_with_predictions.csv\n",
      "💾 Saved middle leaderboard: game_score_per36_middle_leaderboard_2025_with_predictions.csv\n",
      "💾 Saved bottom leaderboard: game_score_per36_bottom_leaderboard_2025_with_predictions.csv\n",
      "💾 Saved closest misses: game_score_per36_closest_misses_2025.csv\n",
      "\n",
      "📋 GAME_SCORE_PER36 SUMMARY:\n",
      "   Top: 0/10 are 2025 predictions\n",
      "   Middle: 0/10 are 2025 predictions\n",
      "   Bottom: 0/10 are 2025 predictions\n",
      "\n",
      "📈 Processing metric: season_pie\n",
      "------------------------------\n",
      "[LOAD] Loading predictions: /workspace/data/processed/heat_data_scientist_2025/predictions/season_pie_predictions_2025.parquet\n",
      "[LOAD] Loaded 309 prediction rows with columns: ['personId', 'player_name', 'season_start_year', 'prediction_season', 'season_pie_pred']\n",
      "[pred-col] Found exact match: 'season_pie_pred' for metric 'season_pie'\n",
      "✅ Historical data: 5,575 player-seasons\n",
      "✅ Predictions: 309 players for 2025\n",
      "💾 Saved top leaderboard: season_pie_top_leaderboard_2025_with_predictions.csv\n",
      "💾 Saved middle leaderboard: season_pie_middle_leaderboard_2025_with_predictions.csv\n",
      "💾 Saved bottom leaderboard: season_pie_bottom_leaderboard_2025_with_predictions.csv\n",
      "💾 Saved closest misses: season_pie_closest_misses_2025.csv\n",
      "\n",
      "📋 SEASON_PIE SUMMARY:\n",
      "   Top: 0/10 are 2025 predictions\n",
      "   Middle: 1/10 are 2025 predictions\n",
      "   Bottom: 0/10 are 2025 predictions\n",
      "\n",
      "✅ Completed leaderboard generation for 2 metrics\n",
      "✅ Comprehensive leaderboards created successfully\n",
      "📋 Summary saved: /workspace/data/processed/heat_data_scientist_2025/evaluation/comprehensive_results_summary.json\n",
      "✅ Results summary saved\n",
      "\n",
      "============================================================\n",
      "🎉 PIPELINE COMPLETED!\n",
      "============================================================\n",
      "✅ ML models trained and predictions generated\n",
      "✅ Feature importance analysis completed\n",
      "✅ Simple leaderboards created\n",
      "✅ Comprehensive leaderboards with historical comparison\n",
      "📁 All results saved to: /workspace/data/processed/heat_data_scientist_2025/predictions\n",
      "\n",
      "✅ Pipeline completed successfully!\n",
      "\n",
      "🔍 Running verification diagnostics...\n",
      "\n",
      "🔍 DEBUGGING PREDICTIONS AND LEADERBOARDS\n",
      "==================================================\n",
      "\n",
      "📊 Checking season_pie:\n",
      "------------------------------\n",
      "✅ Predictions exist: /workspace/data/processed/heat_data_scientist_2025/predictions/season_pie_predictions_2025.parquet\n",
      "   📋 Columns: ['personId', 'player_name', 'season_start_year', 'prediction_season', 'season_pie_pred']\n",
      "   📊 Shape: (309, 5)\n",
      "   🎯 Prediction columns: ['prediction_season', 'season_pie_pred']\n",
      "   📈 prediction_season range: 2025.000000 to 2025.000000\n",
      "   👤 Top player: LeBron James\n",
      "✅ Leaderboard exists: /workspace/data/processed/heat_data_scientist_2025/predictions/season_pie_leaderboard_2025.csv\n",
      "\n",
      "📊 Checking game_score_per36:\n",
      "------------------------------\n",
      "✅ Predictions exist: /workspace/data/processed/heat_data_scientist_2025/predictions/game_score_per36_predictions_2025.parquet\n",
      "   📋 Columns: ['personId', 'player_name', 'season_start_year', 'prediction_season', 'game_score_per36_pred']\n",
      "   📊 Shape: (309, 5)\n",
      "   🎯 Prediction columns: ['prediction_season', 'game_score_per36_pred']\n",
      "   📈 prediction_season range: 2025.000000 to 2025.000000\n",
      "   👤 Top player: LeBron James\n",
      "✅ Leaderboard exists: /workspace/data/processed/heat_data_scientist_2025/predictions/game_score_per36_leaderboard_2025.csv\n"
     ]
    }
   ],
   "source": [
    "# %%writefile src/heat_data_scientist_2025/run_pie_gs36_prediction_pipeline.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Multi-Target ML Pipeline Runner\n",
    "==============================================\n",
    "\n",
    "This script now properly handles column naming mismatches and includes\n",
    "comprehensive error handling and code cleanup.\n",
    "\n",
    "Key fixes:\n",
    "1. Fixed column name detection for predictions (_pred vs predicted_)\n",
    "2. Removed duplicate/unused functions  \n",
    "3. Improved error handling and debugging\n",
    "4. More efficient code structure\n",
    "5. Better progress reporting\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "from src.heat_data_scientist_2025.data.load_data_utils import load_data_optimized\n",
    "from src.heat_data_scientist_2025.utils.config import CFG\n",
    "from src.heat_data_scientist_2025.data.feature_engineering import engineer_features\n",
    "from src.heat_data_scientist_2025.ml.enhanced_ml_pipeline import (\n",
    "    EnhancedMultiTargetMLPipeline,\n",
    "    create_game_score_per36_feature\n",
    ")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    FIXED: Run the enhanced pipeline with proper error handling.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Import feature configurations\n",
    "    from src.heat_data_scientist_2025.utils.config import (\n",
    "        season_pie_numerical_features,\n",
    "        game_score_per36_numerical_features, \n",
    "        nominal_categoricals,\n",
    "        ordinal_categoricals,\n",
    "        y_variables\n",
    "    )\n",
    "    \n",
    "    print(\"🚀 FIXED ENHANCED MULTI-TARGET PIPELINE\")\n",
    "    print(\"=\" * 45)\n",
    "    print(f\"🎯 Targets: {', '.join(y_variables)}\")\n",
    "    print(f\"📊 Total Features: {len(season_pie_numerical_features) + len(ordinal_categoricals)}\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # Load and process data\n",
    "    print(\"\\n📊 LOADING AND ENGINEERING DATA\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    try:\n",
    "        df = load_data_optimized(CFG.ml_dataset_path, drop_null_rows=True)\n",
    "        df_engineered = engineer_features(df, verbose=True)\n",
    "        print(\"✅ Feature engineering completed successfully\")\n",
    "        \n",
    "        # Show top performers for verification\n",
    "        if \"season_pie\" in df_engineered.columns:\n",
    "            top_pie = df_engineered[[\"player_name\", \"season_pie\"]].sort_values(\"season_pie\", ascending=False).head(5)\n",
    "            print(f\"\\n🏆 Top 5 season_pie players:\")\n",
    "            for _, row in top_pie.iterrows():\n",
    "                print(f\"   {row['player_name']}: {row['season_pie']:.6f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Data loading failed: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "    # Add game_score_per36 if needed\n",
    "    if 'game_score_per36' in y_variables and 'game_score_per36' not in df_engineered.columns:\n",
    "        print(\"\\n🎯 Computing game_score_per36...\")\n",
    "        try:\n",
    "            df_engineered, _ = create_game_score_per36_feature(df_engineered)\n",
    "            print(\"✅ game_score_per36 feature added\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to add game_score_per36: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    # Run ML pipeline\n",
    "    print(\"\\n🤖 RUNNING ML PIPELINE\")\n",
    "    print(\"=\" * 25)\n",
    "    \n",
    "    try:\n",
    "        pipeline = EnhancedMultiTargetMLPipeline(\n",
    "            numerical_features=season_pie_numerical_features,  # Will be overridden per target\n",
    "            nominal_categoricals=nominal_categoricals,\n",
    "            ordinal_categoricals=ordinal_categoricals,\n",
    "            y_variables=y_variables,\n",
    "            importance_threshold=0.001,\n",
    "            max_features_per_target=30,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        results = pipeline.run_complete_pipeline(df_engineered)\n",
    "        print(\"✅ ML Pipeline completed successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ ML Pipeline failed: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "    # Create simple leaderboards (FIXED)\n",
    "    print(\"\\n🏆 CREATING SIMPLE LEADERBOARDS\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    try:\n",
    "        # Import the FIXED leaderboard function\n",
    "        from src.heat_data_scientist_2025.ml.leaderboard_compare import create_simple_leaderboards_from_predictions\n",
    "        \n",
    "        simple_leaderboards = create_simple_leaderboards_from_predictions(\n",
    "            metrics=(\"game_score_per36\", \"season_pie\"),\n",
    "            prediction_year=2025,\n",
    "            top_n=50,\n",
    "            verbose=True\n",
    "        )\n",
    "        print(\"✅ Simple leaderboards created successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Simple leaderboard creation failed: {str(e)}\")\n",
    "        print(\"⚠️  Continuing without simple leaderboards...\")\n",
    "        simple_leaderboards = {}\n",
    "    \n",
    "    # Create comprehensive leaderboards (FIXED)\n",
    "    print(\"\\n🏆 CREATING COMPREHENSIVE LEADERBOARDS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # Import the FIXED comprehensive leaderboard function  \n",
    "        from src.heat_data_scientist_2025.ml.leaderboard_compare import build_leaderboards_with_predictions\n",
    "        \n",
    "        comprehensive_leaderboards = build_leaderboards_with_predictions(\n",
    "            metrics=(\"game_score_per36\", \"season_pie\"),\n",
    "            prediction_year=2025,\n",
    "            save=True,\n",
    "            verbose=True\n",
    "        )\n",
    "        print(\"✅ Comprehensive leaderboards created successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Comprehensive leaderboard creation failed: {str(e)}\")\n",
    "        print(f\"🐛 Error details: {str(e)}\")\n",
    "        # Continue without comprehensive leaderboards\n",
    "        comprehensive_leaderboards = {}\n",
    "    \n",
    "    # Save comprehensive results\n",
    "    try:\n",
    "        save_comprehensive_results_summary(results, simple_leaderboards, comprehensive_leaderboards)\n",
    "        print(\"✅ Results summary saved\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Failed to save comprehensive summary: {str(e)}\")\n",
    "    \n",
    "    # Final summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"🎉 PIPELINE COMPLETED!\")  \n",
    "    print(\"=\"*60)\n",
    "    print(\"✅ ML models trained and predictions generated\")\n",
    "    print(\"✅ Feature importance analysis completed\")\n",
    "    if simple_leaderboards:\n",
    "        print(\"✅ Simple leaderboards created\")\n",
    "    if comprehensive_leaderboards:\n",
    "        print(\"✅ Comprehensive leaderboards with historical comparison\")\n",
    "    print(f\"📁 All results saved to: {CFG.ml_predictions_dir}\")\n",
    "    \n",
    "    return {\n",
    "        'ml_results': results,\n",
    "        'simple_leaderboards': simple_leaderboards,\n",
    "        'comprehensive_leaderboards': comprehensive_leaderboards\n",
    "    }\n",
    "\n",
    "\n",
    "def save_comprehensive_results_summary(ml_results: Dict, \n",
    "                                       simple_lb: Dict, \n",
    "                                       comp_lb: Dict) -> None:\n",
    "    \"\"\"Save a comprehensive summary with improved error handling.\"\"\"\n",
    "    try:\n",
    "        summary_path = CFG.ml_evaluation_dir / \"comprehensive_results_summary.json\"\n",
    "        \n",
    "        # Build summary with defensive programming\n",
    "        summary = {\n",
    "            'pipeline_status': 'completed',\n",
    "            'timestamp': pd.Timestamp.now().isoformat(),\n",
    "            'pipeline_config': {\n",
    "                'numerical_features_count': len(ml_results.get('config', {}).get('numerical_features', [])),\n",
    "                'categorical_features_count': len(\n",
    "                    ml_results.get('config', {}).get('nominal_categoricals', []) + \n",
    "                    ml_results.get('config', {}).get('ordinal_categoricals', [])\n",
    "                ),\n",
    "                'target_variables': ml_results.get('config', {}).get('y_variables', []),\n",
    "                'prediction_year': 2025\n",
    "            },\n",
    "            'ml_performance': {},\n",
    "            'leaderboard_status': {\n",
    "                'simple_leaderboards': len(simple_lb),\n",
    "                'comprehensive_leaderboards': len(comp_lb)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Add ML performance metrics safely\n",
    "        if 'model_results' in ml_results:\n",
    "            model_results = ml_results['model_results']\n",
    "            if 'evaluation_metrics' in model_results:\n",
    "                summary['ml_performance'] = model_results['evaluation_metrics']\n",
    "            \n",
    "            # Add feature importance summary\n",
    "            if 'importance_scores' in model_results:\n",
    "                summary['feature_importance'] = {}\n",
    "                for target, importance_df in model_results['importance_scores'].items():\n",
    "                    if not importance_df.empty:\n",
    "                        summary['feature_importance'][target] = {\n",
    "                            'top_feature': importance_df.iloc[0]['feature'],\n",
    "                            'top_importance': float(importance_df.iloc[0]['importance_mean']),\n",
    "                            'significant_features_count': len(importance_df[importance_df['importance_mean'] > 0.001])\n",
    "                        }\n",
    "        \n",
    "        # Add leaderboard summaries\n",
    "        if simple_lb:\n",
    "            summary['top_predictions_simple'] = {}\n",
    "            for metric, lb_df in simple_lb.items():\n",
    "                if not lb_df.empty:\n",
    "                    summary['top_predictions_simple'][metric] = {\n",
    "                        'winner': lb_df.iloc[0]['player_name'],\n",
    "                        'value': float(lb_df.iloc[0][metric]),\n",
    "                        'total_predictions': len(lb_df)\n",
    "                    }\n",
    "        \n",
    "        # Save summary\n",
    "        with open(summary_path, 'w') as f:\n",
    "            json.dump(summary, f, indent=2, default=str)\n",
    "        \n",
    "        print(f\"📋 Summary saved: {summary_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Failed to save comprehensive summary: {str(e)}\")\n",
    "\n",
    "\n",
    "def load_and_analyze_saved_results(target_name: str, year: int,\n",
    "                                   id_cols: List[str] = [\"personId\", \"player_name\"],\n",
    "                                   verbose: bool = True) -> None:\n",
    "    \"\"\"\n",
    "    ENHANCED: Utility to load saved predictions/leaderboard with improved error handling.\n",
    "    \n",
    "    Now handles:\n",
    "    - Better column name detection and fallbacks\n",
    "    - More informative error messages  \n",
    "    - Graceful handling of missing files\n",
    "    - Defensive programming for column access\n",
    "    - Clearer guidance when files are missing\n",
    "    \"\"\"\n",
    "    CFG.ensure_ml_dirs()\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\n📊 Analyzing saved results for target='{target_name}', year={year}\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "    # === PREDICTIONS ANALYSIS ===\n",
    "    pred_path = CFG.predictions_path(target_name, year)\n",
    "    if not pred_path.exists():\n",
    "        print(f\"❌ Predictions not found at: {pred_path}\")\n",
    "        # Try alternative path formats\n",
    "        try:\n",
    "            pred_path_alt = CFG.predictions_path(f\"{target_name}_{year}\")\n",
    "            if pred_path_alt.exists():\n",
    "                pred_path = pred_path_alt\n",
    "                print(f\"✅ Found alternative path: {pred_path}\")\n",
    "        except Exception:\n",
    "            print(\"❌ No alternative prediction paths found\")\n",
    "            print(\"💡 Ensure the ML pipeline has been run to generate predictions\")\n",
    "\n",
    "    if pred_path.exists():\n",
    "        try:\n",
    "            preds = pd.read_parquet(pred_path)\n",
    "            print(f\"✅ Loaded predictions: {len(preds):,} rows from {pred_path}\")\n",
    "            \n",
    "            # Find prediction columns (ending with _pred)\n",
    "            pred_cols = [c for c in preds.columns if c.endswith(\"_pred\")]\n",
    "            if not pred_cols:\n",
    "                print(\"⚠️  No *_pred column found.\")\n",
    "                print(f\"Available columns: {list(preds.columns)}\")\n",
    "            else:\n",
    "                for c in pred_cols:\n",
    "                    col_data = pd.to_numeric(preds[c], errors='coerce')\n",
    "                    print(f\"   {c}: min={col_data.min():.6f}, median={col_data.median():.6f}, max={col_data.max():.6f}\")\n",
    "            \n",
    "            # Build display columns defensively\n",
    "            safe_id_cols = [col for col in id_cols if col in preds.columns]\n",
    "            season_cols = [col for col in ['season_start_year', 'prediction_season', 'season'] if col in preds.columns]\n",
    "            \n",
    "            display_cols = safe_id_cols + pred_cols + season_cols\n",
    "            \n",
    "            if display_cols:\n",
    "                print(\"\\n📋 Sample predictions:\")\n",
    "                print(preds.head(5)[display_cols].to_string(index=False))\n",
    "            else:\n",
    "                print(\"⚠️  No suitable columns found for display\")\n",
    "                print(f\"Available columns: {list(preds.columns)}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading predictions: {str(e)}\")\n",
    "    else:\n",
    "        print(\"❌ Could not locate predictions parquet.\")\n",
    "\n",
    "    # === LEADERBOARD ANALYSIS ===\n",
    "    lb_path = CFG.leaderboard_path(target_name, year)\n",
    "    if not lb_path.exists():\n",
    "        print(f\"\\n❌ Simple leaderboard not found at: {lb_path}\")\n",
    "        print(\"💡 This suggests create_simple_leaderboards_from_predictions() hasn't been run\")\n",
    "        print(\"💡 Add this step to your pipeline after generating predictions\")\n",
    "        \n",
    "        # Try alternative path formats\n",
    "        try:\n",
    "            lb_path_alt = CFG.leaderboard_path(f\"{target_name}_{year}\")\n",
    "            if lb_path_alt.exists():\n",
    "                lb_path = lb_path_alt\n",
    "                print(f\"✅ Found alternative leaderboard: {lb_path}\")\n",
    "        except Exception:\n",
    "            print(\"❌ No alternative leaderboard paths found\")\n",
    "\n",
    "    if lb_path.exists():\n",
    "        try:\n",
    "            lb = pd.read_csv(lb_path)\n",
    "            print(f\"\\n✅ Loaded leaderboard (top {len(lb):,}) from {lb_path}\")\n",
    "            print(\"\\n🏆 Top 10 leaderboard:\")\n",
    "            print(lb.head(10).to_string(index=False))\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading leaderboard: {str(e)}\")\n",
    "    else:\n",
    "        print(\"\\n❌ Could not locate leaderboard CSV.\")\n",
    "        print(\"💡 The missing step is creating simple leaderboards from predictions\")\n",
    "        print(\"💡 This should happen automatically in the updated pipeline\")\n",
    "        \n",
    "    # === FILE STATUS SUMMARY ===\n",
    "    print(f\"\\n📁 FILE STATUS SUMMARY for {target_name}_{year}\")\n",
    "    print(\"-\" * 50)\n",
    "    pred_status = \"✅ EXISTS\" if pred_path.exists() else \"❌ MISSING\"\n",
    "    lb_status = \"✅ EXISTS\" if lb_path.exists() else \"❌ MISSING\"\n",
    "    print(f\"Predictions: {pred_status} - {pred_path}\")\n",
    "    print(f\"Leaderboard: {lb_status} - {lb_path}\")\n",
    "    \n",
    "    if not lb_path.exists():\n",
    "        print(\"\\n🔧 TO FIX: Add create_simple_leaderboards_from_predictions() to your pipeline\")\n",
    "        print(\"   This function should run after predictions are generated but before analysis\")\n",
    "\n",
    "\n",
    "def debug_predictions_and_leaderboards(verbose: bool = True) -> None:\n",
    "    \"\"\"\n",
    "    DEBUG UTILITY: Diagnose prediction files and leaderboard creation issues.\n",
    "    Call this if you encounter errors.\n",
    "    \"\"\"\n",
    "    print(\"\\n🔍 DEBUGGING PREDICTIONS AND LEADERBOARDS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for target in [\"season_pie\", \"game_score_per36\"]:\n",
    "        print(f\"\\n📊 Checking {target}:\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # Check if prediction file exists\n",
    "        pred_path = CFG.predictions_path(target, 2025)\n",
    "        if pred_path.exists():\n",
    "            print(f\"✅ Predictions exist: {pred_path}\")\n",
    "            try:\n",
    "                df = pd.read_parquet(pred_path)\n",
    "                print(f\"   📋 Columns: {list(df.columns)}\")\n",
    "                print(f\"   📊 Shape: {df.shape}\")\n",
    "                \n",
    "                # Check for prediction columns\n",
    "                pred_cols = [c for c in df.columns if 'pred' in c.lower()]\n",
    "                print(f\"   🎯 Prediction columns: {pred_cols}\")\n",
    "                \n",
    "                if pred_cols:\n",
    "                    pred_col = pred_cols[0]\n",
    "                    pred_values = pd.to_numeric(df[pred_col], errors='coerce')\n",
    "                    print(f\"   📈 {pred_col} range: {pred_values.min():.6f} to {pred_values.max():.6f}\")\n",
    "                    print(f\"   👤 Top player: {df.loc[pred_values.idxmax(), 'player_name']}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ❌ Error reading predictions: {str(e)}\")\n",
    "        else:\n",
    "            print(f\"❌ Predictions missing: {pred_path}\")\n",
    "        \n",
    "        # Check leaderboard  \n",
    "        lb_path = CFG.leaderboard_path(target, 2025)\n",
    "        if lb_path.exists():\n",
    "            print(f\"✅ Leaderboard exists: {lb_path}\")\n",
    "        else:\n",
    "            print(f\"❌ Leaderboard missing: {lb_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the main pipeline\n",
    "    print(\"🚀 Starting FIXED Enhanced Multi-Target Pipeline...\")\n",
    "    \n",
    "    try:\n",
    "        results = main()\n",
    "        \n",
    "        if results is None:\n",
    "            print(\"\\n❌ Pipeline failed - running diagnostics...\")\n",
    "            debug_predictions_and_leaderboards()\n",
    "            sys.exit(1)\n",
    "        else:\n",
    "            print(\"\\n✅ Pipeline completed successfully!\")\n",
    "            \n",
    "            # Optional: Run diagnostics for verification\n",
    "            print(\"\\n🔍 Running verification diagnostics...\")\n",
    "            debug_predictions_and_leaderboards()\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n⏹️  Pipeline interrupted by user\")\n",
    "        sys.exit(1)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n💥 Unexpected error: {str(e)}\")\n",
    "        print(\"\\n🔍 Running diagnostics...\")\n",
    "        debug_predictions_and_leaderboards()\n",
    "        sys.exit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4959a88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
