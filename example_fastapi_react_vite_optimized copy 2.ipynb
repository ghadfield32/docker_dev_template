{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8f0ea50",
   "metadata": {},
   "source": [
    "# Root"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 237,
=======
   "execution_count": 35,
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
   "id": "bf67aa45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting package.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile package.json\n",
    "{\n",
    "  \"name\": \"ml-fullstack-app\",\n",
    "  \"private\": true,\n",
    "  \"scripts\": {\n",
    "    \"dev:all\": \"concurrently \\\"npm run dev:backend\\\" \\\"npm run dev:frontend\\\"\",\n",
    "    \"dev:backend\": \"cd backend && uvicorn app.main:app --reload --host 0.0.0.0 --port 8000\",\n",
    "    \"dev:frontend\": \"cd frontend && npm run dev\",\n",
    "    \"build\": \"cd frontend && npm run build\",\n",
    "    \"test\": \"cd backend && pytest tests/ && cd ../frontend && npm test\",\n",
    "    \"lint\": \"cd backend && ruff check . && cd ../frontend && npm run lint\",\n",
    "    \"format\": \"cd backend && ruff format . && cd ../frontend && npm run format\",\n",
    "    \"install:all\": \"cd frontend && npm install && cd ../backend && pip install -r requirements.txt\",\n",
    "    \"start:containers\": \"invoke up --name ml-fullstack --detach\",\n",
    "    \"stop:containers\": \"invoke down --name ml-fullstack\",\n",
    "    \"logs\": \"docker-compose logs -f\",\n",
    "    \"health\": \"curl -s http://localhost:8000/api/v1/models/health | jq .\",\n",
    "    \"test:api\": \"curl -s http://localhost:8000/api/v1/iris/predict -H \\\"Content-Type: application/json\\\" -d '{\\\"model_type\\\":\\\"rf\\\",\\\"samples\\\":[{\\\"sepal_length\\\":5.1,\\\"sepal_width\\\":3.5,\\\"petal_length\\\":1.4,\\\"petal_width\\\":0.2}]}' | jq .\"\n",
    "  },\n",
    "  \"devDependencies\": {\n",
    "    \"concurrently\": \"^8.2.0\",\n",
    "    \"cross-env\": \"^7.0.3\",\n",
    "    \"npm-run-all\": \"^4.1.5\",\n",
    "    \"wait-on\": \"^7.0.1\"\n",
    "  },\n",
    "  \"dependencies\": {\n",
    "    \"dotenv\": \"^17.0.1\"\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 238,
=======
   "execution_count": 36,
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
   "id": "80a61c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_server.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_server.py\n",
    "\"\"\"Simple FastAPI test server for testing integration.\"\"\"\n",
    "\n",
    "from fastapi import FastAPI, APIRouter\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Dict, Any, Optional\n",
    "import logging\n",
    "import socket\n",
    "import time\n",
    "import uvicorn\n",
    "import random\n",
    "import asyncio\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"ML Test API\",\n",
    "    description=\"Simple test API for ML predictions\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "# Create API router for v1 endpoints\n",
    "api_router = APIRouter(prefix=\"/api/v1\")\n",
    "\n",
    "# Add CORS middleware\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Pydantic models\n",
    "class IrisFeatures(BaseModel):\n",
    "    sepal_length: float\n",
    "    sepal_width: float\n",
    "    petal_length: float\n",
    "    petal_width: float\n",
    "\n",
    "class IrisPredictRequest(BaseModel):\n",
    "    rows: List[IrisFeatures]\n",
    "\n",
    "class CancerFeatures(BaseModel):\n",
    "    values: List[float]\n",
    "\n",
    "class CancerPredictRequest(BaseModel):\n",
    "    rows: List[CancerFeatures]\n",
    "    posterior_samples: Optional[int] = None\n",
    "\n",
    "class PredictResponse(BaseModel):\n",
    "    predictions: List[float]\n",
    "    model_used: str\n",
    "    uncertainty: Optional[List[Dict[str, float]]] = None\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    \"\"\"Root endpoint.\"\"\"\n",
    "    return {\n",
    "        \"message\": \"ML Test API\",\n",
    "        \"version\": \"1.0.0\",\n",
    "        \"docs_url\": \"/docs\",\n",
    "        \"health_url\": \"/health\"\n",
    "    }\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health():\n",
    "    \"\"\"Health check endpoint.\"\"\"\n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"service\": \"ml-test-api\",\n",
    "        \"models\": {\n",
    "            \"iris_rf\": {\"loaded\": True, \"status\": \"mock\"},\n",
    "            \"cancer_bayes\": {\"loaded\": True, \"status\": \"mock\"}\n",
    "        }\n",
    "    }\n",
    "\n",
    "@app.head(\"/health\", include_in_schema=False)\n",
    "async def health_head():\n",
    "    \"\"\"HEAD variant for wait-on and health probes.\"\"\"\n",
    "    # HEAD responses must not include a body - just return 200 OK\n",
    "    from fastapi.responses import Response\n",
    "    return Response(status_code=200)\n",
    "\n",
    "@api_router.post(\"/iris/predict\", response_model=PredictResponse)\n",
    "async def predict_iris(request: IrisPredictRequest):\n",
    "    \"\"\"Make iris predictions.\"\"\"\n",
    "    logger.info(f\"Received iris prediction request for {len(request.rows)} samples\")\n",
    "\n",
    "    # Mock prediction logic\n",
    "    predictions = []\n",
    "    for row in request.rows:\n",
    "        # Simple mock logic: if petal_length < 2, predict setosa (0), else non-setosa (1)\n",
    "        pred = 0.0 if row.petal_length < 2.0 else 1.0\n",
    "        predictions.append(pred)\n",
    "\n",
    "    return PredictResponse(\n",
    "        predictions=predictions,\n",
    "        model_used=\"iris_random_forest_mock\"\n",
    "    )\n",
    "\n",
    "@api_router.post(\"/cancer/predict\", response_model=PredictResponse)\n",
    "async def predict_cancer(request: CancerPredictRequest):\n",
    "    \"\"\"Make cancer predictions.\"\"\"\n",
    "    logger.info(f\"Received cancer prediction request for {len(request.rows)} samples\")\n",
    "\n",
    "    # Mock prediction logic\n",
    "    predictions = []\n",
    "    uncertainty = []\n",
    "\n",
    "    for row in request.rows:\n",
    "        # Mock logic: use mean of first 5 features as probability\n",
    "        values = row.values[:5] if len(row.values) >= 5 else row.values\n",
    "        prob = min(max(sum(values) / len(values) / 50.0, 0.0), 1.0)  # Normalize to 0-1\n",
    "        predictions.append(prob)\n",
    "\n",
    "        if request.posterior_samples:\n",
    "            uncertainty.append({\n",
    "                \"lower\": max(prob - 0.1, 0.0),\n",
    "                \"upper\": min(prob + 0.1, 1.0)\n",
    "            })\n",
    "\n",
    "    return PredictResponse(\n",
    "        predictions=predictions,\n",
    "        model_used=\"cancer_bayes_mock\",\n",
    "        uncertainty=uncertainty if request.posterior_samples else None\n",
    "    )\n",
    "\n",
    "@api_router.get(\"/models/info\")\n",
    "async def model_info():\n",
    "    \"\"\"Get model information.\"\"\"\n",
    "    return {\n",
    "        \"iris_random_forest\": {\n",
    "            \"version\": \"1.0\",\n",
    "            \"status\": \"loaded\",\n",
    "            \"accuracy\": 0.95,\n",
    "            \"type\": \"classification\"\n",
    "        },\n",
    "        \"cancer_bayes\": {\n",
    "            \"version\": \"1.0\",\n",
    "            \"status\": \"loaded\",\n",
    "            \"accuracy\": 0.93,\n",
    "            \"type\": \"bayesian_classification\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "@api_router.get(\"/iris/sample\")\n",
    "async def iris_sample():\n",
    "    \"\"\"Get sample iris data.\"\"\"\n",
    "    return {\n",
    "        \"sample\": {\n",
    "            \"sepal_length\": 5.1,\n",
    "            \"sepal_width\": 3.5,\n",
    "            \"petal_length\": 1.4,\n",
    "            \"petal_width\": 0.2\n",
    "        },\n",
    "        \"description\": \"Sample Setosa flower measurements\"\n",
    "    }\n",
    "\n",
    "@api_router.get(\"/cancer/sample\")\n",
    "async def cancer_sample():\n",
    "    \"\"\"Get sample cancer data.\"\"\"\n",
    "    return {\n",
    "        \"sample\": {\n",
    "            \"values\": [17.99, 10.38, 122.8, 1001.0, 0.1184, 0.2776, 0.3001, 0.1471, 0.2419, 0.07871,\n",
    "                      1.095, 0.9053, 8.589, 153.4, 0.006399, 0.04904, 0.05373, 0.01587, 0.03003, 0.006193,\n",
    "                      25.38, 17.33, 184.6, 2019.0, 0.1622, 0.6656, 0.7119, 0.2654, 0.4601, 0.1189]\n",
    "        },\n",
    "        \"description\": \"Sample breast cancer features (30 dimensions)\"\n",
    "    }\n",
    "\n",
    "# Add health endpoint at /api/v1/health to match frontend expectations\n",
    "@api_router.get(\"/health\")\n",
    "async def api_v1_health():\n",
    "    \"\"\"Health check endpoint for /api/v1/health.\"\"\"\n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"service\": \"ml-test-api\",\n",
    "        \"models\": {\n",
    "            \"iris_rf\": {\"loaded\": True, \"status\": \"mock\"},\n",
    "            \"cancer_bayes\": {\"loaded\": True, \"status\": \"mock\"}\n",
    "        }\n",
    "    }\n",
    "\n",
    "@api_router.head(\"/health\", include_in_schema=False)\n",
    "async def api_v1_health_head():\n",
    "    \"\"\"HEAD variant for /api/v1/health probes.\"\"\"\n",
    "    from fastapi.responses import Response\n",
    "    return Response(status_code=200)\n",
    "\n",
    "# Add missing endpoints that frontend expects\n",
    "@api_router.get(\"/models/health\")\n",
    "async def models_health():\n",
    "    \"\"\"Health check endpoint for /api/v1/models/health.\"\"\"\n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"version\": \"1.0.0\",\n",
    "        \"models\": {\n",
    "            \"iris_random_forest\": {\n",
    "                \"name\": \"iris_random_forest\",\n",
    "                \"version\": \"1.0\",\n",
    "                \"status\": \"loaded\",\n",
    "                \"accuracy\": 0.95,\n",
    "                \"created_at\": \"2024-01-01T00:00:00Z\",\n",
    "                \"run_id\": \"mock_run_123\"\n",
    "            },\n",
    "            \"breast_cancer_bayes\": {\n",
    "                \"name\": \"breast_cancer_bayes\",\n",
    "                \"version\": \"1.0\",\n",
    "                \"status\": \"loaded\",\n",
    "                \"accuracy\": 0.93,\n",
    "                \"created_at\": \"2024-01-01T00:00:00Z\",\n",
    "                \"run_id\": \"mock_run_456\"\n",
    "            }\n",
    "        },\n",
    "        \"mlflow_uri\": \"file:./mlruns_local\",\n",
    "        \"loaded_models\": \"2/2\"\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "@api_router.get(\"/models/list\")\n",
    "async def models_list():\n",
    "    \"\"\"List all available models.\"\"\"\n",
    "    return {\n",
    "        \"models\": {\n",
    "            \"iris_random_forest\": {\n",
    "                \"name\": \"iris_random_forest\",\n",
    "                \"status\": \"loaded\",\n",
    "                \"version\": \"1.0\",\n",
    "                \"accuracy\": 0.95,\n",
    "                \"dataset\": \"iris\",\n",
    "                \"algorithm\": \"random_forest\"\n",
    "            },\n",
    "            \"breast_cancer_bayes\": {\n",
    "                \"name\": \"breast_cancer_bayes\",\n",
    "                \"status\": \"loaded\",\n",
    "                \"version\": \"1.0\",\n",
    "                \"accuracy\": 0.93,\n",
    "                \"dataset\": \"breast_cancer\",\n",
    "                \"algorithm\": \"bayesian_logistic_regression\"\n",
    "            }\n",
    "        },\n",
    "        \"total_models\": 2,\n",
    "        \"loaded_models\": 2,\n",
    "        \"available_datasets\": [\"iris\", \"breast_cancer\"],\n",
    "        \"available_algorithms\": [\"random_forest\", \"logistic_regression\", \"bayesian_logistic_regression\"]\n",
    "    }\n",
    "\n",
    "# Add retraining endpoints with mock implementations\n",
    "# Global metrics store for retraining\n",
    "_METRICS = {\n",
    "    \"iris_random_forest\": {\n",
    "        \"accuracy\": 0.95,\n",
    "        \"f1_macro\": 0.94,\n",
    "        \"precision_macro\": 0.93,\n",
    "        \"recall_macro\": 0.95,\n",
    "        \"version\": \"1.0\",\n",
    "        \"run_id\": \"mock_run_123\"\n",
    "    },\n",
    "    \"breast_cancer_bayes\": {\n",
    "        \"accuracy\": 0.93,\n",
    "        \"f1_macro\": 0.92,\n",
    "        \"precision_macro\": 0.91,\n",
    "        \"recall_macro\": 0.93,\n",
    "        \"version\": \"1.0\",\n",
    "        \"run_id\": \"mock_run_456\"\n",
    "    }\n",
    "}\n",
    "\n",
    "@api_router.post(\"/iris/retrain\")\n",
    "async def iris_retrain(body: dict):\n",
    "    \"\"\"Mock retrain for Iris models.\"\"\"\n",
    "    n_trials = body.get(\"hyperparameters\", {}).get(\"n_trials\", 50)\n",
    "\n",
    "    # Simulate training time\n",
    "    await asyncio.sleep(3)\n",
    "\n",
    "    # Generate new mock metrics\n",
    "    new_accuracy = round(random.uniform(0.92, 0.97), 3)\n",
    "    _METRICS[\"iris_random_forest\"] = {\n",
    "        \"accuracy\": new_accuracy,\n",
    "        \"f1_macro\": round(new_accuracy - 0.01, 3),\n",
    "        \"precision_macro\": round(new_accuracy - 0.02, 3),\n",
    "        \"recall_macro\": round(new_accuracy - 0.01, 3),\n",
    "        \"version\": \"2.0\",\n",
    "        \"run_id\": f\"mock_run_{random.randint(1000, 9999)}\"\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"status\": \"started\",\n",
    "        \"detail\": f\"Iris retrain running in background with {n_trials} trials\",\n",
    "        \"model_types\": [\"rf\", \"logreg\"],\n",
    "        \"estimated_time\": f\"{n_trials * 2} seconds\"\n",
    "    }\n",
    "\n",
    "@api_router.post(\"/cancer/retrain\")\n",
    "async def cancer_retrain(body: dict):\n",
    "    \"\"\"Mock retrain for Cancer models.\"\"\"\n",
    "    params = body.get(\"hyperparameters\", {})\n",
    "    draws = params.get(\"draws\", 800)\n",
    "    tune = params.get(\"tune\", 400)\n",
    "    target_accept = params.get(\"target_accept\", 0.9)\n",
    "\n",
    "    # Simulate training time\n",
    "    await asyncio.sleep(5)\n",
    "\n",
    "    # Generate new mock metrics\n",
    "    new_accuracy = round(random.uniform(0.89, 0.94), 3)\n",
    "    _METRICS[\"breast_cancer_bayes\"] = {\n",
    "        \"accuracy\": new_accuracy,\n",
    "        \"f1_macro\": round(new_accuracy - 0.01, 3),\n",
    "        \"precision_macro\": round(new_accuracy - 0.02, 3),\n",
    "        \"recall_macro\": round(new_accuracy - 0.01, 3),\n",
    "        \"version\": \"2.0\",\n",
    "        \"run_id\": f\"mock_run_{random.randint(1000, 9999)}\"\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"status\": \"started\",\n",
    "        \"detail\": f\"Cancer Bayesian retrain running in background\",\n",
    "        \"parameters\": {\n",
    "            \"draws\": draws,\n",
    "            \"tune\": tune,\n",
    "            \"target_accept\": target_accept\n",
    "        },\n",
    "        \"estimated_time\": f\"{draws + tune} seconds\"\n",
    "    }\n",
    "\n",
    "# Update the metrics endpoint to use the global store\n",
    "@api_router.get(\"/models/metrics\")\n",
    "async def models_metrics():\n",
    "    \"\"\"Get model metrics endpoint.\"\"\"\n",
    "    return _METRICS\n",
    "\n",
    "# Include the API router\n",
    "app.include_router(api_router)\n",
    "\n",
    "# Add an additional health endpoint at /api/health for frontend proxy\n",
    "@app.get(\"/api/health\")\n",
    "async def api_health():\n",
    "    \"\"\"Health check endpoint for API proxy.\"\"\"\n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"service\": \"ml-test-api\",\n",
    "        \"models\": {\n",
    "            \"iris_rf\": {\"loaded\": True, \"status\": \"mock\"},\n",
    "            \"cancer_bayes\": {\"loaded\": True, \"status\": \"mock\"}\n",
    "        }\n",
    "    }\n",
    "\n",
    "@app.head(\"/api/health\", include_in_schema=False)\n",
    "async def api_health_head():\n",
    "    \"\"\"HEAD variant for API health probes.\"\"\"\n",
    "    from fastapi.responses import Response\n",
    "    return Response(status_code=200)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    def is_port_in_use(port):\n",
    "        \"\"\"Check if a port is in use.\"\"\"\n",
    "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "            try:\n",
    "                s.bind(('0.0.0.0', port))\n",
    "                return False\n",
    "            except OSError:\n",
    "                return True\n",
    "\n",
    "    # Check if port is in use and wait if necessary\n",
    "    port = 8000\n",
    "    max_retries = 5\n",
    "    retry_delay = 2\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        if is_port_in_use(port):\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"âš ï¸  Port {port} is in use, waiting {retry_delay}s before retry {attempt + 1}/{max_retries}\")\n",
    "                time.sleep(retry_delay)\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"âŒ Port {port} is still in use after {max_retries} attempts. Please check for other processes.\")\n",
    "                exit(1)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    print(f\"ðŸš€ Starting FastAPI server on port {port}\")\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=port)\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 239,
=======
   "execution_count": 37,
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
   "id": "760d795c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting render.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile render.yaml\n",
    "services:\n",
    "  - type: web\n",
<<<<<<< HEAD
    "    name: ml-backend\n",
    "    runtime: python\n",
    "    region: oregon\n",
    "    plan: free\n",
    "    buildCommand: pip install -r requirements.txt\n",
    "    startCommand: sh -c 'uvicorn app.main:app --host 0.0.0.0 --port ${PORT:-8000}'\n",
    "    rootDir: backend\n",
    "    envVars:\n",
    "      - key: DEV_AUTOTRAIN\n",
    "        value: \"1\"\n",
    "      - key: MLFLOW_TRACKING_URI\n",
    "        value: \"./mlruns\"\n",
    "      - key: PROJECT_NAME\n",
    "        value: \"ML Full Stack API\"\n",
    "      - key: API_V1_STR\n",
    "        value: \"/api/v1\"\n",
    "      - key: PYTHON_VERSION\n",
    "        value: \"3.11.0\"\n",
    "    healthCheckPath: /health\n",
    "    autoDeploy: true\n",
    "    \n",
    "  # Optional: Add a static site for frontend if deploying both together\n",
    "  # - type: static\n",
    "  #   name: ml-frontend\n",
    "  #   buildCommand: cd frontend && npm ci && npm run build\n",
    "  #   publishPath: frontend/dist\n",
    "  #   envVars:\n",
    "  #     - key: VITE_API_URL\n",
    "  #       fromService:\n",
    "  #         type: web\n",
    "  #         name: ml-backend\n",
    "  #         property: host\n",
    "  #   routes:\n",
    "  #     - type: rewrite\n",
    "  #       source: /*\n",
    "  #       destination: /index.html\n",
    "\n"
=======
    "    name: ml_backend\n",
    "    runtime: python\n",
    "    plan: free\n",
    "    rootDir: backend\n",
    "    buildCommand: pip install -r requirements.txt\n",
    "    startCommand: uvicorn app.main:app --host 0.0.0.0 --port $PORT\n",
    "    healthCheckPath: /health\n",
    "    autoDeploy: true\n",
    "    envVars:\n",
    "      - key: PYTHON_VERSION\n",
    "        value: 3.12.2\n",
    "      - key: MLFLOW_TRACKING_URI\n",
    "        value: file:./mlruns_local\n"
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 240,
=======
   "execution_count": 38,
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
   "id": "f9cc997d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting netlify.toml\n"
     ]
    }
   ],
   "source": [
    "%%writefile netlify.toml\n",
    "[build]\n",
<<<<<<< HEAD
    "  base = \"frontend\"\n",
    "  command = \"npm ci && npm run build\"\n",
    "  publish = \"dist\"\n",
    "\n",
    "# Environment variables for production\n",
    "[build.environment]\n",
    "  NODE_VERSION = \"18\"\n",
    "  NPM_VERSION = \"9\"\n",
    "\n",
    "# Redirect all routes to index.html for SPA routing\n",
    "[[redirects]]\n",
    "  from = \"/*\"\n",
    "  to = \"/index.html\"\n",
    "  status = 200\n",
    "\n",
    "# Headers for security and performance\n",
    "[[headers]]\n",
    "  for = \"/*\"\n",
    "  [headers.values]\n",
    "    X-Frame-Options = \"DENY\"\n",
    "    X-XSS-Protection = \"1; mode=block\"\n",
    "    X-Content-Type-Options = \"nosniff\"\n",
    "    Referrer-Policy = \"strict-origin-when-cross-origin\"\n",
    "\n",
    "# Cache static assets\n",
    "[[headers]]\n",
    "  for = \"/assets/*\"\n",
    "  [headers.values]\n",
    "    Cache-Control = \"public, max-age=31536000, immutable\"\n",
    "\n",
    "# Cache service worker\n",
    "[[headers]]\n",
    "  for = \"/sw.js\"\n",
    "  [headers.values]\n",
    "    Cache-Control = \"no-cache\" \n",
    "\n"
=======
    "base = \"frontend\"\n",
    "command = \"npm run build\"\n",
    "publish = \"dist\"\n",
    "\n",
    "[build.environment]\n",
    "NODE_VERSION = \"18.20.0\"\n",
    "\n",
    "[[redirects]]\n",
    "  from = \"/*\"\n",
    "  to = \"/index.html\"\n",
    "  status = 200 "
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bd01d0",
   "metadata": {},
   "source": [
    "# Backend\n",
    "\n",
    "deploy hook:\n",
    "https://api.render.com/deploy/srv-d1ksa4ndiees73eu7u3g?key=7iZbEPDcq8w"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 241,
   "id": "3cdf88e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting backend/app/core/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile backend/app/core/__init__.py\n",
    "\"\"\"Core package for application configuration.\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
=======
   "execution_count": 39,
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
   "id": "eb81edf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting backend/app/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile backend/app/__init__.py\n",
    "\"\"\"Backend application package.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 243,
=======
   "execution_count": 40,
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
   "id": "f25a842a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting backend/app/services/ml/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile backend/app/services/ml/__init__.py\n",
    "\"\"\"ML services package.\"\"\"\n",
    "\n",
    "from .model_service import model_service\n",
    "\n",
    "__all__ = [\"model_service\"]\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 244,
=======
   "execution_count": 41,
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
   "id": "2fdf9832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting backend/app/services/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile backend/app/services/__init__.py\n",
    "\"\"\"Services package for the backend application.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 245,
=======
   "execution_count": 42,
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
   "id": "6e486eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting backend/app/api/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile backend/app/api/__init__.py\n",
    "\"\"\"API package for the backend application.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 246,
=======
   "execution_count": 43,
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
   "id": "a8614215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting backend/app/api/api_v1/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile backend/app/api/api_v1/__init__.py\n",
    "\"\"\"API v1 package.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 247,
=======
   "execution_count": 44,
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
   "id": "7ba9eff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting backend/app/api/api_v1/endpoints/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile backend/app/api/api_v1/endpoints/__init__.py\n",
    "\"\"\"API endpoints package.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 248,
=======
   "execution_count": 45,
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
   "id": "87336ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting backend/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile backend/requirements.txt\n",
    "fastapi>=0.104.0\n",
    "uvicorn>=0.24.0\n",
    "pydantic>=2.4.2\n",
    "python-multipart>=0.0.6\n",
    "mlflow>=2.8.0\n",
    "numpy>=1.24.0\n",
    "pandas>=2.1.0\n",
    "scikit-learn>=1.3.0\n",
    "python-dotenv>=1.0.0 "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 249,
=======
   "execution_count": 46,
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
   "id": "bda8c3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting backend/app/core/config.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile backend/app/core/config.py\n",
    "\"\"\"Core configuration for the FastAPI application.\"\"\"\n",
    "\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class Settings:\n",
    "    \"\"\"Application settings.\"\"\"\n",
    "\n",
    "    # Basic app settings\n",
    "    PROJECT_NAME: str = \"ML Full Stack API\"\n",
    "    VERSION: str = \"1.0.0\"\n",
    "    API_V1_STR: str = \"/api/v1\"\n",
    "\n",
    "    # CORS settings\n",
    "    BACKEND_CORS_ORIGINS: List[str] = [\n",
    "        \"http://localhost:3000\",  # React dev server\n",
    "        \"http://localhost:5173\",  # Vite dev server\n",
    "        \"http://localhost:8080\",  # Alternative frontend\n",
    "        \"http://localhost:5000\",  # Another common port\n",
<<<<<<< HEAD
    "        \"https://*.netlify.app\",  # Netlify deployments\n",
    "        \"*\",  # Allow all origins for development/testing\n",
=======
    "        \"*\",  # Allow all origins in development\n",
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
    "    ]\n",
    "\n",
    "    # MLflow settings\n",
    "    MLFLOW_TRACKING_URI: str = os.getenv(\"MLFLOW_TRACKING_URI\", \"file:./mlruns_local\")\n",
    "    MLFLOW_EXPERIMENT_NAME: str = \"ml_fullstack_models\"\n",
    "\n",
    "    # Model settings\n",
<<<<<<< HEAD
    "    DEV_AUTOTRAIN: bool = os.getenv(\"DEV_AUTOTRAIN\", \"true\").lower() in [\"true\", \"1\", \"yes\", \"on\"]\n",
=======
    "    DEV_AUTOTRAIN: bool = os.getenv(\"DEV_AUTOTRAIN\", \"true\").lower() == \"true\"\n",
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
    "\n",
    "    # Database settings (if needed later)\n",
    "    DATABASE_URL: str = os.getenv(\"DATABASE_URL\", \"sqlite:///./app.db\")\n",
    "\n",
    "\n",
    "# Global settings instance\n",
<<<<<<< HEAD
    "settings = Settings()\n",
    "\n"
=======
    "settings = Settings()\n"
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 250,
=======
   "execution_count": 47,
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
   "id": "c3cd52d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting backend/app/schemas/iris.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile backend/app/schemas/iris.py\n",
    "\"\"\"Pydantic schemas for API request/response models.\"\"\"\n",
    "\n",
    "from .iris import IrisFeatures, IrisPredictRequest, IrisPredictResponse\n",
    "from .cancer import CancerFeatures, CancerPredictRequest, CancerPredictResponse\n",
    "from .common import HealthResponse, ModelInfo, PredictionResponse\n",
    "\n",
    "__all__ = [\n",
    "    \"IrisFeatures\",\n",
    "    \"IrisPredictRequest\",\n",
    "    \"IrisPredictResponse\",\n",
    "    \"CancerFeatures\",\n",
    "    \"CancerPredictRequest\",\n",
    "    \"CancerPredictResponse\",\n",
    "    \"HealthResponse\",\n",
    "    \"ModelInfo\",\n",
    "    \"PredictionResponse\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 251,
=======
   "execution_count": 48,
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
   "id": "33fe907d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting backend/app/schemas/common.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile backend/app/schemas/common.py\n",
    "\"\"\"Common schema definitions.\"\"\"\n",
    "\n",
    "from typing import Dict, List, Optional, Any\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class ModelInfo(BaseModel):\n",
    "    \"\"\"Model information schema.\"\"\"\n",
    "    name: str\n",
    "    version: Optional[str] = None\n",
    "    status: str\n",
    "    accuracy: Optional[float] = None\n",
    "    created_at: Optional[str] = None\n",
    "    run_id: Optional[str] = None\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Pydantic config.\"\"\"\n",
    "        json_schema_extra = {\n",
    "            \"example\": {\n",
    "                \"name\": \"iris_random_forest\",\n",
    "                \"version\": \"1\",\n",
    "                \"status\": \"production\",\n",
    "                \"accuracy\": 0.95,\n",
    "                \"created_at\": \"2024-01-01T00:00:00Z\",\n",
    "                \"run_id\": \"abc123\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "class PredictionResponse(BaseModel):\n",
    "    \"\"\"Base prediction response schema.\"\"\"\n",
    "    predictions: List[float] = Field(..., description=\"Model predictions\")\n",
    "    model_used: str = Field(..., description=\"Name of the model used\")\n",
    "    model_version: Optional[str] = Field(None, description=\"Version of the model\")\n",
    "    uncertainty: Optional[List[Dict[str, float]]] = Field(None, description=\"Uncertainty estimates\")\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Pydantic config.\"\"\"\n",
    "        json_schema_extra = {\n",
    "            \"example\": {\n",
    "                \"predictions\": [0.95, 0.89, 0.76],\n",
    "                \"model_used\": \"iris_random_forest\",\n",
    "                \"model_version\": \"1\",\n",
    "                \"uncertainty\": [\n",
    "                    {\"lower\": 0.90, \"upper\": 0.98},\n",
    "                    {\"lower\": 0.82, \"upper\": 0.94}\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "class HealthResponse(BaseModel):\n",
    "    \"\"\"Health check response schema.\"\"\"\n",
    "    status: str = Field(..., description=\"Overall health status\")\n",
    "    version: str = Field(..., description=\"API version\")\n",
    "    models: Dict[str, ModelInfo] = Field(..., description=\"Model status information\")\n",
    "    mlflow_uri: Optional[str] = Field(None, description=\"MLflow tracking URI\")\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Pydantic config.\"\"\"\n",
    "        json_schema_extra = {\n",
    "            \"example\": {\n",
    "                \"status\": \"healthy\",\n",
    "                \"version\": \"1.0.0\",\n",
    "                \"models\": {\n",
    "                    \"iris_rf\": {\n",
    "                        \"name\": \"iris_random_forest\",\n",
    "                        \"status\": \"loaded\",\n",
    "                        \"accuracy\": 0.95\n",
    "                    }\n",
    "                },\n",
    "                \"mlflow_uri\": \"http://mlflow:5000\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "class ErrorResponse(BaseModel):\n",
    "    \"\"\"Error response schema.\"\"\"\n",
    "    error: str = Field(..., description=\"Error message\")\n",
    "    detail: Optional[str] = Field(None, description=\"Detailed error information\")\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Pydantic config.\"\"\"\n",
    "        json_schema_extra = {\n",
    "            \"example\": {\n",
    "                \"error\": \"Model not found\",\n",
    "                \"detail\": \"The requested model is not available\"\n",
    "            }\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 252,
=======
   "execution_count": 49,
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
   "id": "0e3bbb80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting backend/app/schemas/iris.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile backend/app/schemas/iris.py\n",
    "\"\"\"Iris dataset schema definitions.\"\"\"\n",
    "\n",
    "from typing import List, Optional, Dict\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from .common import PredictionResponse\n",
    "\n",
    "\n",
    "class IrisFeatures(BaseModel):\n",
    "    \"\"\"Single Iris flower measurement.\"\"\"\n",
<<<<<<< HEAD
    "    sepal_length: float = Field(..., ge=0.0, le=10.0, description=\"Sepal length in cm (0.0-10.0)\")\n",
    "    sepal_width: float = Field(..., ge=0.0, le=6.0, description=\"Sepal width in cm (0.0-6.0)\")\n",
    "    petal_length: float = Field(..., ge=0.0, le=10.0, description=\"Petal length in cm (0.0-10.0)\")\n",
    "    petal_width: float = Field(..., ge=0.0, le=4.0, description=\"Petal width in cm (0.0-4.0)\")\n",
=======
    "    sepal_length: float = Field(..., ge=4.0, le=8.0, description=\"Sepal length in cm\")\n",
    "    sepal_width: float = Field(..., ge=2.0, le=4.5, description=\"Sepal width in cm\")\n",
    "    petal_length: float = Field(..., ge=1.0, le=7.0, description=\"Petal length in cm\")\n",
    "    petal_width: float = Field(..., ge=0.1, le=2.5, description=\"Petal width in cm\")\n",
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
    "\n",
    "    class Config:\n",
    "        \"\"\"Pydantic config.\"\"\"\n",
    "        json_schema_extra = {\n",
    "            \"example\": {\n",
    "                \"sepal_length\": 5.1,\n",
    "                \"sepal_width\": 3.5,\n",
    "                \"petal_length\": 1.4,\n",
    "                \"petal_width\": 0.2\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "class IrisPredictRequest(BaseModel):\n",
    "    \"\"\"Iris prediction request schema.\"\"\"\n",
    "    model_type: str = Field(\"rf\", description=\"Model type: 'rf' or 'logreg'\")\n",
<<<<<<< HEAD
    "    samples: List[IrisFeatures] = Field(\n",
    "        ...,\n",
    "        alias=\"rows\",  # Allow 'rows' as an alias for backward compatibility\n",
    "        description=\"List of iris measurements\"\n",
    "    )\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Pydantic config.\"\"\"\n",
    "        populate_by_name = True  # Enable alias support\n",
=======
    "    samples: List[IrisFeatures] = Field(..., description=\"List of iris measurements\")\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Pydantic config.\"\"\"\n",
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
    "        json_schema_extra = {\n",
    "            \"example\": {\n",
    "                \"model_type\": \"rf\",\n",
    "                \"samples\": [\n",
    "                    {\n",
    "                        \"sepal_length\": 5.1,\n",
    "                        \"sepal_width\": 3.5,\n",
    "                        \"petal_length\": 1.4,\n",
    "                        \"petal_width\": 0.2\n",
<<<<<<< HEAD
=======
    "                    },\n",
    "                    {\n",
    "                        \"sepal_length\": 6.2,\n",
    "                        \"sepal_width\": 3.4,\n",
    "                        \"petal_length\": 5.4,\n",
    "                        \"petal_width\": 2.3\n",
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "class IrisPredictResponse(PredictionResponse):\n",
    "    \"\"\"Iris prediction response schema.\"\"\"\n",
    "    class_names: List[str] = Field(\n",
    "        default=[\"setosa\", \"versicolor\", \"virginica\"],\n",
    "        description=\"Iris class names\"\n",
    "    )\n",
    "    predicted_classes: Optional[List[str]] = Field(\n",
    "        None,\n",
    "        description=\"Predicted class names\"\n",
    "    )\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Pydantic config.\"\"\"\n",
    "        json_schema_extra = {\n",
    "            \"example\": {\n",
    "                \"predictions\": [0, 2, 1],\n",
    "                \"predicted_classes\": [\"setosa\", \"virginica\", \"versicolor\"],\n",
    "                \"class_names\": [\"setosa\", \"versicolor\", \"virginica\"],\n",
    "                \"model_used\": \"iris_random_forest\",\n",
    "                \"model_version\": \"1\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "class IrisTrainingRequest(BaseModel):\n",
    "    \"\"\"Iris model training request schema.\"\"\"\n",
    "    model_type: str = Field(\"rf\", description=\"Model type: 'rf' or 'logreg'\")\n",
    "    hyperparameters: Optional[Dict] = Field(None, description=\"Model hyperparameters\")\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Pydantic config.\"\"\"\n",
    "        json_schema_extra = {\n",
    "            \"example\": {\n",
    "                \"model_type\": \"rf\",\n",
    "                \"hyperparameters\": {\n",
    "                    \"n_estimators\": 100,\n",
    "                    \"max_depth\": 5,\n",
    "                    \"random_state\": 42\n",
    "                }\n",
    "            }\n",
<<<<<<< HEAD
    "        }\n",
    "\n"
=======
    "        }\n"
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 253,
=======
   "execution_count": 50,
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
   "id": "be2f77ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting backend/app/schemas/cancer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile backend/app/schemas/cancer.py\n",
    "\"\"\"Breast cancer dataset schema definitions.\"\"\"\n",
    "\n",
    "from typing import List, Optional, Dict\n",
    "from pydantic import BaseModel, Field, field_validator\n",
    "\n",
    "from .common import PredictionResponse\n",
    "\n",
    "\n",
    "class CancerFeatures(BaseModel):\n",
<<<<<<< HEAD
    "    \"\"\"\n",
    "    Breast cancer feature vector.\n",
    "    We *prefer* 30 features (the full sklearn dataset), but will accept\n",
    "    any list **between 5 and 30 items** and pad/truncate server-side.\n",
    "    \"\"\"\n",
    "    values: List[float] = Field(\n",
    "        ...,\n",
    "        min_items=5,      # â¬…ï¸ loosened from 30\n",
    "        max_items=50,     # Allow more, will truncate in validator\n",
    "        description=\"5â€“30 numeric features (will be padded/truncated to 30 server-side)\"\n",
=======
    "    \"\"\"Breast cancer features in the correct order for sklearn breast cancer dataset.\"\"\"\n",
    "    # Using a list to match the sklearn breast cancer dataset exactly\n",
    "    values: List[float] = Field(\n",
    "        ...,\n",
    "        min_items=30,\n",
    "        max_items=30,\n",
    "        description=\"30 features from breast cancer dataset\"\n",
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
    "    )\n",
    "\n",
    "    @field_validator(\"values\")\n",
    "    @classmethod\n",
    "    def validate_feature_count(cls, v: List[float]) -> List[float]:\n",
<<<<<<< HEAD
    "        \"\"\"Validate 5-30 features and optionally truncate if too many.\"\"\"\n",
    "        if len(v) < 5:\n",
    "            raise ValueError(\"Provide at least 5 features\")\n",
    "        if len(v) > 30:\n",
    "            return v[:30]     # silently truncate extras to 30\n",
=======
    "        \"\"\"Validate exactly 30 features.\"\"\"\n",
    "        if len(v) != 30:\n",
    "            raise ValueError(\"Must provide exactly 30 features\")\n",
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
    "        return v\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Pydantic config.\"\"\"\n",
    "        json_schema_extra = {\n",
    "            \"example\": {\n",
    "                \"values\": [\n",
    "                    17.99, 10.38, 122.8, 1001.0, 0.1184, 0.2776, 0.3001, 0.1471, 0.2419, 0.07871,\n",
    "                    1.095, 0.9053, 8.589, 153.4, 0.006399, 0.04904, 0.05373, 0.01587, 0.03003, 0.006193,\n",
    "                    25.38, 17.33, 184.6, 2019.0, 0.1622, 0.6656, 0.7119, 0.2654, 0.4601, 0.1189\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "class CancerPredictRequest(BaseModel):\n",
    "    \"\"\"Cancer prediction request schema.\"\"\"\n",
    "    model_type: str = Field(\"bayes\", description=\"Model type: 'bayes', 'logreg', or 'rf'\")\n",
    "    samples: List[CancerFeatures] = Field(..., description=\"List of cancer feature vectors\")\n",
    "    posterior_samples: Optional[int] = Field(\n",
    "        None,\n",
    "        ge=10,\n",
    "        le=10000,\n",
    "        description=\"Number of posterior samples for Bayesian model\"\n",
    "    )\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Pydantic config.\"\"\"\n",
    "        json_schema_extra = {\n",
    "            \"example\": {\n",
    "                \"model_type\": \"bayes\",\n",
    "                \"samples\": [\n",
    "                    {\n",
    "                        \"values\": [\n",
    "                            17.99, 10.38, 122.8, 1001.0, 0.1184, 0.2776, 0.3001, 0.1471, 0.2419, 0.07871,\n",
    "                            1.095, 0.9053, 8.589, 153.4, 0.006399, 0.04904, 0.05373, 0.01587, 0.03003, 0.006193,\n",
    "                            25.38, 17.33, 184.6, 2019.0, 0.1622, 0.6656, 0.7119, 0.2654, 0.4601, 0.1189\n",
    "                        ]\n",
    "                    }\n",
    "                ],\n",
    "                \"posterior_samples\": 100\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "class CancerPredictResponse(PredictionResponse):\n",
    "    \"\"\"Cancer prediction response schema.\"\"\"\n",
    "    class_names: List[str] = Field(\n",
    "        default=[\"malignant\", \"benign\"],\n",
    "        description=\"Cancer class names\"\n",
    "    )\n",
    "    predicted_classes: Optional[List[str]] = Field(\n",
    "        None,\n",
    "        description=\"Predicted class names\"\n",
    "    )\n",
    "    posterior_samples: Optional[int] = Field(\n",
    "        None,\n",
    "        description=\"Number of posterior samples used\"\n",
    "    )\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Pydantic config.\"\"\"\n",
    "        json_schema_extra = {\n",
    "            \"example\": {\n",
    "                \"predictions\": [0.85, 0.23],\n",
    "                \"predicted_classes\": [\"malignant\", \"benign\"],\n",
    "                \"class_names\": [\"malignant\", \"benign\"],\n",
    "                \"model_used\": \"breast_cancer_bayes\",\n",
    "                \"model_version\": \"1\",\n",
    "                \"uncertainty\": [\n",
    "                    {\"lower\": 0.78, \"upper\": 0.91},\n",
    "                    {\"lower\": 0.18, \"upper\": 0.29}\n",
    "                ],\n",
    "                \"posterior_samples\": 100\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "class CancerTrainingRequest(BaseModel):\n",
    "    \"\"\"Cancer model training request schema.\"\"\"\n",
    "    model_type: str = Field(\"bayes\", description=\"Model type: 'bayes', 'logreg', or 'rf'\")\n",
    "    hyperparameters: Optional[Dict] = Field(None, description=\"Model hyperparameters\")\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Pydantic config.\"\"\"\n",
    "        json_schema_extra = {\n",
    "            \"example\": {\n",
    "                \"model_type\": \"bayes\",\n",
    "                \"hyperparameters\": {\n",
    "                    \"draws\": 1000,\n",
    "                    \"tune\": 500,\n",
    "                    \"target_accept\": 0.9\n",
    "                }\n",
    "            }\n",
<<<<<<< HEAD
    "        }\n",
    "\n"
=======
    "        }\n"
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 254,
=======
   "execution_count": 51,
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
   "id": "4ba5076d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting backend/app/api/api_v1/endpoints/iris.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile backend/app/api/api_v1/endpoints/iris.py\n",
    "\"\"\"Iris prediction endpoints.\"\"\"\n",
    "\n",
    "import logging\n",
    "from typing import List\n",
    "from fastapi import APIRouter, HTTPException, Depends, BackgroundTasks\n",
<<<<<<< HEAD
    "from pydantic import ValidationError\n",
=======
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
    "from ....schemas.iris import (\n",
    "    IrisPredictRequest,\n",
    "    IrisPredictResponse,\n",
    "    IrisTrainingRequest\n",
    ")\n",
    "from ....services.ml.model_service import model_service\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "router = APIRouter()\n",
    "\n",
    "\n",
    "@router.post(\"/predict\", response_model=IrisPredictResponse)\n",
    "async def predict_iris(request: IrisPredictRequest):\n",
    "    \"\"\"\n",
    "    Make predictions on iris flowers.\n",
    "\n",
    "    - **model_type**: Choose between 'rf' (Random Forest) or 'logreg' (Logistic Regression)\n",
    "    - **samples**: List of iris measurements (sepal/petal length and width)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert samples to dict format\n",
    "        samples_dict = [sample.dict() for sample in request.samples]\n",
    "\n",
    "        # Make prediction\n",
    "        result = await model_service.predict_iris(\n",
    "            samples=samples_dict,\n",
    "            model_type=request.model_type\n",
    "        )\n",
    "\n",
    "        return IrisPredictResponse(**result)\n",
    "\n",
<<<<<<< HEAD
    "    except ValidationError as e:\n",
    "        # Log validation errors with details\n",
    "        logger.warning(f\"Validation error in iris predict: {e.errors()}\")\n",
    "        raise HTTPException(\n",
    "            status_code=422,\n",
    "            detail={\n",
    "                \"message\": \"Invalid input data\",\n",
    "                \"errors\": e.errors(),\n",
    "                \"valid_ranges\": {\n",
    "                    \"sepal_length\": \"4.0-8.0 cm\",\n",
    "                    \"sepal_width\": \"2.0-4.5 cm\",\n",
    "                    \"petal_length\": \"1.0-7.0 cm\",\n",
    "                    \"petal_width\": \"0.1-2.5 cm\"\n",
    "                }\n",
    "            }\n",
    "        )\n",
=======
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
    "    except ValueError as e:\n",
    "        raise HTTPException(status_code=400, detail=str(e))\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Iris prediction error: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Prediction failed\")\n",
    "\n",
    "\n",
    "@router.get(\"/models\")\n",
    "async def get_iris_models():\n",
    "    \"\"\"Get available iris models and their information.\"\"\"\n",
    "    try:\n",
    "        health_status = await model_service.get_health_status()\n",
    "        iris_models = {\n",
    "            k: v for k, v in health_status[\"models\"].items()\n",
    "            if \"iris\" in k\n",
    "        }\n",
    "\n",
    "        return {\n",
    "            \"available_models\": iris_models,\n",
    "            \"model_types\": [\"rf\", \"logreg\"],\n",
    "            \"description\": {\n",
    "                \"rf\": \"Random Forest - Ensemble method with high accuracy\",\n",
    "                \"logreg\": \"Logistic Regression - Fast linear model\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting iris models: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Failed to get model information\")\n",
    "\n",
    "\n",
    "@router.get(\"/sample-data\")\n",
    "async def get_sample_data():\n",
    "    \"\"\"Get sample iris data for testing predictions.\"\"\"\n",
    "    sample_data = [\n",
    "        {\n",
    "            \"sepal_length\": 5.1,\n",
    "            \"sepal_width\": 3.5,\n",
    "            \"petal_length\": 1.4,\n",
    "            \"petal_width\": 0.2,\n",
    "            \"expected_class\": \"setosa\"\n",
    "        },\n",
    "        {\n",
    "            \"sepal_length\": 7.0,\n",
    "            \"sepal_width\": 3.2,\n",
    "            \"petal_length\": 4.7,\n",
    "            \"petal_width\": 1.4,\n",
    "            \"expected_class\": \"versicolor\"\n",
    "        },\n",
    "        {\n",
    "            \"sepal_length\": 6.3,\n",
    "            \"sepal_width\": 3.3,\n",
    "            \"petal_length\": 6.0,\n",
    "            \"petal_width\": 2.5,\n",
    "            \"expected_class\": \"virginica\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"samples\": sample_data,\n",
    "        \"description\": \"Sample iris data for testing predictions\",\n",
    "        \"usage\": \"Use the first 4 fields (sepal_length, sepal_width, petal_length, petal_width) for predictions\"\n",
    "    }\n",
    "\n",
    "\n",
    "@router.post(\"/retrain\")\n",
    "async def retrain_iris(\n",
    "    req: IrisTrainingRequest,\n",
    "    background: BackgroundTasks,\n",
    "):\n",
    "    \"\"\"\n",
    "    Trigger asynchronous Optuna retrain for Iris models.\n",
    "    Returns immediately; client can poll /models/metrics for progress.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        params = req.hyperparameters or {}\n",
    "        n_trials = params.get(\"n_trials\", 50)\n",
    "\n",
    "        def _background_job():\n",
    "            \"\"\"Background task to run the retraining.\"\"\"\n",
    "            import asyncio\n",
    "            try:\n",
    "                loop = asyncio.new_event_loop()\n",
    "                asyncio.set_event_loop(loop)\n",
    "                loop.run_until_complete(model_service.retrain_iris(n_trials=n_trials))\n",
<<<<<<< HEAD
    "            except RuntimeError as e:\n",
    "                if \"run_all_trainings not available\" in str(e):\n",
    "                    logger.info(\"Training module not available in this deployment\")\n",
    "                else:\n",
    "                    logger.error(f\"Background iris retrain failed: {e}\")\n",
=======
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
    "            except Exception as e:\n",
    "                logger.error(f\"Background iris retrain failed: {e}\")\n",
    "            finally:\n",
    "                loop.close()\n",
    "\n",
    "        background.add_task(_background_job)\n",
    "\n",
    "        return {\n",
    "            \"status\": \"started\",\n",
    "            \"detail\": f\"Iris retrain running in background with {n_trials} trials\",\n",
    "            \"model_types\": [\"rf\", \"logreg\"],\n",
    "            \"estimated_time\": f\"{n_trials * 2} seconds\"\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to start iris retrain: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"Retrain failed: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 255,
=======
   "execution_count": 52,
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
   "id": "d2340c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting backend/app/api/api_v1/endpoints/cancer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile backend/app/api/api_v1/endpoints/cancer.py\n",
    "\"\"\"Cancer prediction endpoints.\"\"\"\n",
    "\n",
    "import logging\n",
    "from typing import List\n",
    "from fastapi import APIRouter, HTTPException, Depends, BackgroundTasks\n",
    "from ....schemas.cancer import (\n",
    "    CancerPredictRequest,\n",
    "    CancerPredictResponse,\n",
    "    CancerTrainingRequest\n",
    ")\n",
    "from ....services.ml.model_service import model_service\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "router = APIRouter()\n",
    "\n",
    "\n",
    "@router.post(\"/predict\", response_model=CancerPredictResponse)\n",
    "async def predict_cancer(request: CancerPredictRequest):\n",
    "    \"\"\"\n",
    "    Make predictions on breast cancer data.\n",
    "\n",
    "    - **model_type**: Choose between 'bayes' (Bayesian), 'logreg' (Logistic Regression), or 'rf' (Random Forest)\n",
    "    - **samples**: List of cancer feature vectors (30 features each)\n",
    "    - **posterior_samples**: Number of posterior samples for uncertainty estimation (Bayesian only)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert samples to list format\n",
    "        samples_list = [sample.values for sample in request.samples]\n",
    "\n",
    "        # Make prediction\n",
    "        result = await model_service.predict_cancer(\n",
    "            samples=samples_list,\n",
    "            model_type=request.model_type,\n",
    "            posterior_samples=request.posterior_samples\n",
    "        )\n",
    "\n",
    "        return CancerPredictResponse(**result)\n",
    "\n",
    "    except ValueError as e:\n",
    "        raise HTTPException(status_code=400, detail=str(e))\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Cancer prediction error: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Prediction failed\")\n",
    "\n",
    "\n",
    "@router.get(\"/models\")\n",
    "async def get_cancer_models():\n",
    "    \"\"\"Get available cancer models and their information.\"\"\"\n",
    "    try:\n",
    "        health_status = await model_service.get_health_status()\n",
    "        cancer_models = {\n",
    "            k: v for k, v in health_status[\"models\"].items()\n",
    "            if \"cancer\" in k or \"breast\" in k\n",
    "        }\n",
    "\n",
    "        return {\n",
    "            \"available_models\": cancer_models,\n",
    "            \"model_types\": [\"bayes\", \"logreg\", \"rf\"],\n",
    "            \"description\": {\n",
    "                \"bayes\": \"Bayesian Logistic Regression - Provides uncertainty estimates\",\n",
    "                \"logreg\": \"Logistic Regression - Fast linear model\",\n",
    "                \"rf\": \"Random Forest - Ensemble method with high accuracy\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting cancer models: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Failed to get model information\")\n",
    "\n",
    "\n",
    "@router.get(\"/sample-data\")\n",
    "async def get_sample_data():\n",
    "    \"\"\"Get sample cancer data for testing predictions.\"\"\"\n",
    "    # Sample from breast cancer dataset - malignant case\n",
    "    malignant_sample = [\n",
    "        17.99, 10.38, 122.8, 1001.0, 0.1184, 0.2776, 0.3001, 0.1471, 0.2419, 0.07871,\n",
    "        1.095, 0.9053, 8.589, 153.4, 0.006399, 0.04904, 0.05373, 0.01587, 0.03003, 0.006193,\n",
    "        25.38, 17.33, 184.6, 2019.0, 0.1622, 0.6656, 0.7119, 0.2654, 0.4601, 0.1189\n",
    "    ]\n",
    "\n",
    "    # Sample from breast cancer dataset - benign case\n",
    "    benign_sample = [\n",
    "        13.54, 14.36, 87.46, 566.3, 0.09779, 0.08129, 0.06664, 0.04781, 0.1885, 0.05766,\n",
    "        0.2699, 0.7886, 2.058, 23.56, 0.008462, 0.0146, 0.02387, 0.01315, 0.0198, 0.0023,\n",
    "        15.11, 19.26, 99.7, 711.2, 0.144, 0.1773, 0.239, 0.1288, 0.2977, 0.07259\n",
    "    ]\n",
    "\n",
    "    sample_data = [\n",
    "        {\n",
    "            \"values\": malignant_sample,\n",
    "            \"expected_class\": \"malignant\",\n",
    "            \"description\": \"Sample features from malignant tumor\"\n",
    "        },\n",
    "        {\n",
    "            \"values\": benign_sample,\n",
    "            \"expected_class\": \"benign\",\n",
    "            \"description\": \"Sample features from benign tumor\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"samples\": sample_data,\n",
    "        \"description\": \"Sample breast cancer data for testing predictions\",\n",
    "        \"feature_info\": \"30 features: mean, SE, and worst values for 10 measurements\",\n",
    "        \"usage\": \"Use the 'values' array (30 floats) for predictions\"\n",
    "    }\n",
    "\n",
    "\n",
    "@router.post(\"/retrain\")\n",
    "async def retrain_cancer(\n",
    "    req: CancerTrainingRequest,\n",
    "    background: BackgroundTasks,\n",
    "):\n",
    "    \"\"\"\n",
    "    Trigger Bayesian retrain with custom MCMC parameters.\n",
    "    Returns immediately; client can poll /models/metrics for progress.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        params = req.hyperparameters or {}\n",
    "        draws = params.get(\"draws\", 800)\n",
    "        tune = params.get(\"tune\", 400)\n",
    "        target_accept = params.get(\"target_accept\", 0.9)\n",
    "\n",
    "        def _background_job():\n",
    "            \"\"\"Background task to run the retraining.\"\"\"\n",
    "            import asyncio\n",
    "            try:\n",
    "                loop = asyncio.new_event_loop()\n",
    "                asyncio.set_event_loop(loop)\n",
    "                loop.run_until_complete(\n",
    "                    model_service.retrain_cancer_bayes(\n",
    "                        draws=draws,\n",
    "                        tune=tune,\n",
    "                        target_accept=target_accept\n",
    "                    )\n",
    "                )\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Background cancer retrain failed: {e}\")\n",
    "            finally:\n",
    "                loop.close()\n",
    "\n",
    "        background.add_task(_background_job)\n",
    "\n",
    "        return {\n",
    "            \"status\": \"started\",\n",
    "            \"detail\": f\"Cancer Bayesian retrain running in background\",\n",
    "            \"parameters\": {\n",
    "                \"draws\": draws,\n",
    "                \"tune\": tune,\n",
    "                \"target_accept\": target_accept\n",
    "            },\n",
    "            \"estimated_time\": f\"{draws + tune} seconds\"\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to start cancer retrain: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"Retrain failed: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 256,
=======
   "execution_count": 53,
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
   "id": "9132b544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting backend/app/api/api_v1/endpoints/models.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile backend/app/api/api_v1/endpoints/models.py\n",
    "\"\"\"Model management endpoints.\"\"\"\n",
    "\n",
    "import logging\n",
    "from typing import Dict, Any\n",
    "from fastapi import APIRouter, HTTPException\n",
    "from ....schemas.common import HealthResponse\n",
    "from ....services.ml.model_service import model_service\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "router = APIRouter()\n",
    "\n",
    "\n",
    "@router.get(\"/health\", response_model=Dict[str, Any])\n",
    "async def get_health():\n",
    "    \"\"\"\n",
    "    Get the health status of all models.\n",
    "\n",
    "    Returns information about:\n",
    "    - Overall service status\n",
    "    - Individual model status\n",
    "    - MLflow connection status\n",
    "    - Model metrics\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return await model_service.get_health_status()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Health check error: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Health check failed\")\n",
    "\n",
    "\n",
    "@router.get(\"/metrics\")\n",
    "async def get_model_metrics():\n",
    "    \"\"\"\n",
    "    Get performance metrics for all loaded models.\n",
    "\n",
    "    Returns metrics like accuracy, F1 score, precision, and recall\n",
    "    for each available model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return await model_service.get_model_metrics()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Metrics retrieval error: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Failed to get model metrics\")\n",
    "\n",
    "\n",
    "@router.get(\"/list\")\n",
    "async def list_models():\n",
    "    \"\"\"\n",
    "    List all available models with their basic information.\n",
    "\n",
    "    Returns:\n",
    "    - Model names and types\n",
    "    - Load status\n",
    "    - Basic model information\n",
    "    \"\"\"\n",
    "    try:\n",
    "        health_status = await model_service.get_health_status()\n",
    "\n",
    "        models_info = {}\n",
    "        for model_name, info in health_status[\"models\"].items():\n",
    "            models_info[model_name] = {\n",
    "                \"name\": info[\"name\"],\n",
    "                \"status\": info[\"status\"],\n",
    "                \"version\": info.get(\"version\"),\n",
    "                \"accuracy\": info.get(\"accuracy\"),\n",
    "                \"dataset\": \"iris\" if \"iris\" in model_name else \"breast_cancer\",\n",
    "                \"algorithm\": _get_algorithm_type(model_name)\n",
    "            }\n",
    "\n",
    "        return {\n",
    "            \"models\": models_info,\n",
    "            \"total_models\": len(models_info),\n",
    "            \"loaded_models\": len([m for m in models_info.values() if m[\"status\"] == \"loaded\"]),\n",
    "            \"available_datasets\": [\"iris\", \"breast_cancer\"],\n",
    "            \"available_algorithms\": [\"random_forest\", \"logistic_regression\", \"bayesian_logistic_regression\"]\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Model listing error: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Failed to list models\")\n",
    "\n",
    "\n",
    "@router.post(\"/reload\")\n",
    "async def reload_models():\n",
    "    \"\"\"\n",
    "    Reload all models from MLflow registry.\n",
    "\n",
    "    This endpoint can be used to refresh the model cache after\n",
    "    new models have been trained and registered in MLflow.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Cleanup and reinitialize\n",
    "        await model_service.cleanup()\n",
    "        await model_service.initialize()\n",
    "\n",
    "        health_status = await model_service.get_health_status()\n",
    "\n",
    "        return {\n",
    "            \"message\": \"Models reloaded successfully\",\n",
    "            \"status\": health_status[\"status\"],\n",
    "            \"loaded_models\": health_status.get(\"loaded_models\", \"0/0\")\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Model reload error: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Failed to reload models\")\n",
    "\n",
    "\n",
    "def _get_algorithm_type(model_name: str) -> str:\n",
    "    \"\"\"Get the algorithm type from model name.\"\"\"\n",
    "    if \"rf\" in model_name or \"random_forest\" in model_name:\n",
    "        return \"random_forest\"\n",
    "    elif \"logreg\" in model_name:\n",
    "        return \"logistic_regression\"\n",
    "    elif \"bayes\" in model_name:\n",
    "        return \"bayesian_logistic_regression\"\n",
    "    else:\n",
    "        return \"unknown\"\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 257,
=======
   "execution_count": 54,
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
   "id": "c401848b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting backend/app/api/api_v1/api.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile backend/app/api/api_v1/api.py\n",
    "\"\"\"Main API router that combines all endpoints.\"\"\"\n",
    "\n",
    "from fastapi import APIRouter\n",
    "\n",
<<<<<<< HEAD
    "# ðŸ”§ Adjusted imports to use relative paths for Render deployment\n",
    "from app.api.api_v1.endpoints import iris, cancer, models, health\n",
=======
    "from backend.app.api.api_v1.endpoints import iris, cancer, models\n",
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
    "\n",
    "api_router = APIRouter()\n",
    "\n",
    "# Include all endpoint routers\n",
<<<<<<< HEAD
    "api_router.include_router(health.router, tags=[\"Health\"])\n",
=======
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
    "api_router.include_router(iris.router, prefix=\"/iris\", tags=[\"Iris Predictions\"])\n",
    "api_router.include_router(cancer.router, prefix=\"/cancer\", tags=[\"Cancer Predictions\"])\n",
    "api_router.include_router(models.router, prefix=\"/models\", tags=[\"Model Management\"])\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 270,
=======
   "execution_count": 55,
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
   "id": "89f03a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting backend/app/services/ml/model_service.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile backend/app/services/ml/model_service.py\n",
    "\"\"\"ML Model Service for loading and managing trained models.\"\"\"\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
<<<<<<< HEAD
    "from typing import Dict, Any, Optional, List, Union\n",
=======
    "from typing import Dict, Any, Optional, List\n",
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
<<<<<<< HEAD
    "# Add scikit-learn imports for fallback models\n",
    "from sklearn.datasets import load_iris, load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
=======
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
    "from ...core.config import settings\n",
    "from ...schemas.common import HealthResponse, ModelInfo\n",
    "\n",
    "# Import existing ML utilities\n",
<<<<<<< HEAD
    "# ðŸ”§ Try both import paths to support local and Render environments\n",
    "setup_mlflow_experiment = None\n",
    "run_all_trainings = None\n",
    "train_bayes_logreg = None\n",
    "\n",
    "try:\n",
    "    # First try app-relative imports (for Render)\n",
    "    from app.ML.mlops.experiment_utils import setup_mlflow_experiment\n",
    "    from app.ML.mlops.training import run_all_trainings\n",
    "    from app.ML.mlops.training_bayes import train_bayes_logreg\n",
    "    logging.info(\"âœ… Loaded ML modules using app-relative imports\")\n",
    "except ImportError as e:\n",
    "    logging.debug(f\"app-relative imports failed: {e}\")\n",
    "    try:\n",
    "        # Then try backend-relative imports (for local dev)\n",
    "        from backend.ML.mlops.experiment_utils import setup_mlflow_experiment\n",
    "        from backend.ML.mlops.training import run_all_trainings\n",
    "        from backend.ML.mlops.training_bayes import train_bayes_logreg\n",
    "        logging.info(\"âœ… Loaded ML modules using backend-relative imports\")\n",
    "    except ImportError as e:\n",
    "        logging.warning(f\"Could not import ML modules from either path: {e}\")\n",
    "        # Keep the fallback None values\n",
=======
    "try:\n",
    "    from backend.ML.mlops.experiment_utils import setup_mlflow_experiment\n",
    "    from backend.ML.mlops.training import run_all_trainings\n",
    "    from backend.ML.mlops.training_bayes import train_bayes_logreg\n",
    "except ImportError as e:\n",
    "    logging.warning(f\"Could not import existing ML modules: {e}\")\n",
    "    setup_mlflow_experiment = None\n",
    "    run_all_trainings = None\n",
    "    train_bayes_logreg = None\n",
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class ModelService:\n",
    "    \"\"\"Service for managing ML models with MLflow integration.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the model service.\"\"\"\n",
<<<<<<< HEAD
    "        self.models: Dict[str, Union[mlflow.pyfunc.PyFuncModel, Any]] = {}\n",
=======
    "        self.models: Dict[str, mlflow.pyfunc.PyFuncModel] = {}\n",
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
    "        self.model_info: Dict[str, Dict[str, Any]] = {}\n",
    "        self.mlflow_client: Optional[MlflowClient] = None\n",
    "        self.initialized = False\n",
    "\n",
<<<<<<< HEAD
    "        # Model configurations - updated to match prediction naming\n",
    "        self.model_configs = {\n",
    "            \"iris_rf\": {\n",
    "                \"type\": \"classification\",\n",
    "                \"dataset\": \"iris\",\n",
    "                \"class_names\": [\"setosa\", \"versicolor\", \"virginica\"],\n",
    "                \"full_name\": \"iris_random_forest\"\n",
=======
    "        # Model configurations\n",
    "        self.model_configs = {\n",
    "            \"iris_random_forest\": {\n",
    "                \"type\": \"classification\",\n",
    "                \"dataset\": \"iris\",\n",
    "                \"class_names\": [\"setosa\", \"versicolor\", \"virginica\"]\n",
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
    "            },\n",
    "            \"iris_logreg\": {\n",
    "                \"type\": \"classification\",\n",
    "                \"dataset\": \"iris\",\n",
<<<<<<< HEAD
    "                \"class_names\": [\"setosa\", \"versicolor\", \"virginica\"],\n",
    "                \"full_name\": \"iris_logreg\"\n",
=======
    "                \"class_names\": [\"setosa\", \"versicolor\", \"virginica\"]\n",
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
    "            },\n",
    "            \"breast_cancer_bayes\": {\n",
    "                \"type\": \"classification\",\n",
    "                \"dataset\": \"breast_cancer\",\n",
<<<<<<< HEAD
    "                \"class_names\": [\"malignant\", \"benign\"],\n",
    "                \"full_name\": \"breast_cancer_bayes\"\n",
=======
    "                \"class_names\": [\"malignant\", \"benign\"]\n",
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
    "            }\n",
    "        }\n",
    "\n",
    "    async def initialize(self) -> None:\n",
    "        \"\"\"Initialize the model service.\"\"\"\n",
    "        if self.initialized:\n",
    "            return\n",
    "\n",
    "        logger.info(\"Initializing ModelService...\")\n",
    "\n",
    "        try:\n",
    "            # Set up MLflow\n",
    "            mlflow.set_tracking_uri(settings.MLFLOW_TRACKING_URI)\n",
    "            self.mlflow_client = MlflowClient(settings.MLFLOW_TRACKING_URI)\n",
    "            logger.info(f\"Connected to MLflow at {settings.MLFLOW_TRACKING_URI}\")\n",
    "\n",
    "            # Setup MLflow experiment\n",
    "            if setup_mlflow_experiment:\n",
    "                setup_mlflow_experiment(settings.MLFLOW_EXPERIMENT_NAME)\n",
    "\n",
    "            # Load models\n",
    "            await self._load_models()\n",
    "\n",
<<<<<<< HEAD
    "            # ðŸ”§ FALLBACK: Load in-memory models if MLflow models are not available\n",
    "            if settings.DEV_AUTOTRAIN:\n",
    "                await self._load_fallback_models()\n",
    "\n",
=======
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
    "            self.initialized = True\n",
    "            logger.info(\"âœ… ModelService initialized successfully\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize ModelService: {e}\")\n",
    "            raise\n",
    "\n",
    "    async def cleanup(self) -> None:\n",
    "        \"\"\"Cleanup the model service.\"\"\"\n",
    "        logger.info(\"Cleaning up ModelService...\")\n",
    "        self.models.clear()\n",
    "        self.model_info.clear()\n",
    "        self.initialized = False\n",
    "\n",
    "    async def _load_models(self) -> None:\n",
    "        \"\"\"Load all models from MLflow registry.\"\"\"\n",
    "        logger.info(\"Loading models from MLflow registry...\")\n",
    "\n",
<<<<<<< HEAD
    "        for model_name, config in self.model_configs.items():\n",
    "            try:\n",
    "                # Try to load using the full MLflow model name\n",
    "                full_model_name = config.get(\"full_name\", model_name)\n",
    "                model = await self._load_production_model(full_model_name)\n",
    "                if model:\n",
    "                    self.models[model_name] = model  # Store with prediction-friendly name\n",
    "                    logger.info(f\"âœ… Loaded model: {model_name} (from {full_model_name})\")\n",
=======
    "        for model_name in self.model_configs.keys():\n",
    "            try:\n",
    "                model = await self._load_production_model(model_name)\n",
    "                if model:\n",
    "                    self.models[model_name] = model\n",
    "                    logger.info(f\"âœ… Loaded model: {model_name}\")\n",
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
    "                else:\n",
    "                    logger.warning(f\"âš ï¸ Model {model_name} not loaded\")\n",
    "\n",
    "            except Exception as e:\n",
<<<<<<< HEAD
    "                logger.info(f\"âš ï¸ Model {model_name} not loaded - will auto-train if enabled: {e}\")\n",
=======
    "                logger.error(f\"âŒ Failed to load model {model_name}: {e}\")\n",
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
    "\n",
    "                # Auto-train if enabled\n",
    "                if settings.DEV_AUTOTRAIN:\n",
    "                    await self._auto_train_model(model_name)\n",
    "\n",
<<<<<<< HEAD
    "    async def _load_production_model(\n",
    "        self, model_name: str\n",
    "    ) -> Optional[mlflow.pyfunc.PyFuncModel]:\n",
    "        \"\"\"\n",
    "        Load the latest Production model version via the new search API,\n",
    "        since get_latest_versions(...) is deprecated.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 1ï¸âƒ£ Search all versions of this registered model\n",
    "            all_versions = self.mlflow_client.search_model_versions(f\"name='{model_name}'\")\n",
    "\n",
    "            # 2ï¸âƒ£ Filter for Production stage\n",
    "            prod_versions = [v for v in all_versions if v.current_stage == \"Production\"]\n",
    "            if not prod_versions:\n",
    "                logger.info(f\"No Production version found for model {model_name}\")\n",
    "                return None\n",
    "\n",
    "            # 3ï¸âƒ£ Pick the highest version number\n",
    "            #    (ModelVersion.version is a string)\n",
    "            prod_versions.sort(key=lambda v: int(v.version), reverse=True)\n",
    "            chosen = prod_versions[0]\n",
    "\n",
    "            # 4ï¸âƒ£ Load the model by URI\n",
    "            model_uri = f\"models:/{model_name}/Production\"\n",
    "            model = mlflow.pyfunc.load_model(model_uri)\n",
    "\n",
    "            # 5ï¸âƒ£ Fetch run info and store metadata\n",
    "            run = self.mlflow_client.get_run(chosen.run_id)\n",
    "            self.model_info[model_name] = {\n",
    "                \"name\": model_name,\n",
    "                \"version\": chosen.version,\n",
    "                \"status\": \"loaded\",\n",
    "                \"stage\": chosen.current_stage,\n",
    "                \"run_id\": chosen.run_id,\n",
    "                \"accuracy\": run.data.metrics.get(\"accuracy\"),\n",
    "                \"created_at\": chosen.creation_timestamp,\n",
    "                \"metrics\": run.data.metrics,\n",
    "            }\n",
    "\n",
    "            logger.info(f\"âœ… Loaded {model_name} v{chosen.version} from Production\")\n",
    "            return model\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not load production model {model_name}: {e}\")\n",
    "            return None\n",
    "\n",
    "    async def _promote_latest_to_production(self, model_name: str) -> None:\n",
    "        \"\"\"\n",
    "        Promote the newest un-staged version to Production,\n",
    "        using search_model_versions(...) to find the latest.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Search all versions for this model\n",
    "            all_versions = self.mlflow_client.search_model_versions(f\"name='{model_name}'\")\n",
    "\n",
    "            # Filter out already in Production\n",
    "            none_versions = [v for v in all_versions if v.current_stage != \"Production\"]\n",
    "            if not none_versions:\n",
    "                logger.info(f\"No non-Production versions to promote for {model_name}\")\n",
    "                return\n",
    "\n",
    "            # Pick the highest version number among them\n",
    "            none_versions.sort(key=lambda v: int(v.version), reverse=True)\n",
    "            to_promote = none_versions[0]\n",
    "\n",
    "            # Transition it\n",
    "            self.mlflow_client.transition_model_version_stage(\n",
    "                name=model_name,\n",
    "                version=to_promote.version,\n",
    "                stage=\"Production\",\n",
    "                archive_existing_versions=True,\n",
    "            )\n",
    "            logger.info(f\"Promoted {model_name} v{to_promote.version} to Production\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to promote {model_name}: {e}\")\n",
    "\n",
=======
    "    async def _load_production_model(self, model_name: str) -> Optional[mlflow.pyfunc.PyFuncModel]:\n",
    "        \"\"\"Load a model from the Production stage in MLflow registry.\"\"\"\n",
    "        try:\n",
    "            # Try to get the latest Production version\n",
    "            versions = self.mlflow_client.get_latest_versions(\n",
    "                model_name, stages=[\"Production\"]\n",
    "            )\n",
    "\n",
    "            if not versions:\n",
    "                logger.warning(f\"No Production version found for model {model_name}\")\n",
    "                return None\n",
    "\n",
    "            version = versions[0]\n",
    "            model_uri = f\"models:/{model_name}/Production\"\n",
    "\n",
    "            # Load the model\n",
    "            model = mlflow.pyfunc.load_model(model_uri)\n",
    "\n",
    "            # Store model info\n",
    "            run = self.mlflow_client.get_run(version.run_id)\n",
    "            self.model_info[model_name] = {\n",
    "                \"name\": model_name,\n",
    "                \"version\": version.version,\n",
    "                \"status\": \"loaded\",\n",
    "                \"stage\": version.current_stage,\n",
    "                \"run_id\": version.run_id,\n",
    "                \"accuracy\": run.data.metrics.get(\"accuracy\"),\n",
    "                \"created_at\": version.creation_timestamp,\n",
    "                \"metrics\": run.data.metrics\n",
    "            }\n",
    "\n",
    "            return model\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading model {model_name}: {e}\")\n",
    "            return None\n",
    "\n",
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
    "    async def _auto_train_model(self, model_name: str) -> None:\n",
    "        \"\"\"Auto-train a model if auto-training is enabled.\"\"\"\n",
    "        if not settings.DEV_AUTOTRAIN:\n",
    "            return\n",
    "\n",
    "        logger.info(f\"Auto-training model {model_name}...\")\n",
    "\n",
    "        try:\n",
<<<<<<< HEAD
    "            config = self.model_configs.get(model_name, {})\n",
    "            full_model_name = config.get(\"full_name\", model_name)\n",
    "\n",
    "            if model_name in [\"iris_rf\", \"iris_logreg\"] and run_all_trainings:\n",
=======
    "            if model_name in [\"iris_random_forest\", \"iris_logreg\"] and run_all_trainings:\n",
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
    "                # Train both iris models\n",
    "                run_all_trainings(n_trials=10)  # Quick training for demo\n",
    "\n",
    "            elif model_name == \"breast_cancer_bayes\" and train_bayes_logreg:\n",
    "                # Train bayesian model\n",
    "                train_bayes_logreg(draws=100, tune=50, register=True)\n",
    "\n",
<<<<<<< HEAD
    "            # Promote to production using full model name\n",
    "            await self._promote_latest_to_production(full_model_name)\n",
    "\n",
    "            # Try to load again\n",
    "            model = await self._load_production_model(full_model_name)\n",
=======
    "            # Promote to production\n",
    "            await self._promote_latest_to_production(model_name)\n",
    "\n",
    "            # Try to load again\n",
    "            model = await self._load_production_model(model_name)\n",
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
    "            if model:\n",
    "                self.models[model_name] = model\n",
    "                logger.info(f\"âœ… Auto-trained and loaded model: {model_name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Auto-training failed for {model_name}: {e}\")\n",
    "\n",
<<<<<<< HEAD
    "\n",
=======
    "    async def _promote_latest_to_production(self, model_name: str) -> None:\n",
    "        \"\"\"Promote the latest model version to Production.\"\"\"\n",
    "        try:\n",
    "            # Get latest version from None stage\n",
    "            latest_versions = self.mlflow_client.get_latest_versions(\n",
    "                model_name, stages=[\"None\"]\n",
    "            )\n",
    "\n",
    "            if latest_versions:\n",
    "                version = latest_versions[0]\n",
    "                self.mlflow_client.transition_model_version_stage(\n",
    "                    name=model_name,\n",
    "                    version=version.version,\n",
    "                    stage=\"Production\",\n",
    "                    archive_existing_versions=True\n",
    "                )\n",
    "                logger.info(f\"Promoted {model_name} v{version.version} to Production\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to promote {model_name}: {e}\")\n",
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
    "\n",
    "    async def predict_iris(\n",
    "        self,\n",
    "        samples: List[Dict[str, float]],\n",
    "        model_type: str = \"rf\"\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Make iris predictions.\"\"\"\n",
    "        model_name = f\"iris_{model_type}\"\n",
    "\n",
    "        if model_name not in self.models:\n",
    "            raise ValueError(f\"Model {model_name} not available\")\n",
    "\n",
    "        model = self.models[model_name]\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(samples)\n",
    "\n",
    "        # Make predictions\n",
    "        predictions = model.predict(df)\n",
    "\n",
    "        # Convert to class names if predictions are numeric\n",
    "        class_names = self.model_configs[model_name][\"class_names\"]\n",
    "        if isinstance(predictions[0], (int, np.integer)):\n",
    "            predicted_classes = [class_names[int(p)] for p in predictions]\n",
    "        else:\n",
    "            predicted_classes = [class_names[int(np.argmax(p))] for p in predictions]\n",
    "\n",
    "        return {\n",
    "            \"predictions\": predictions.tolist() if hasattr(predictions, 'tolist') else list(predictions),\n",
    "            \"predicted_classes\": predicted_classes,\n",
    "            \"class_names\": class_names,\n",
    "            \"model_used\": model_name,\n",
    "            \"model_version\": self.model_info.get(model_name, {}).get(\"version\")\n",
    "        }\n",
    "\n",
    "    async def predict_cancer(\n",
    "        self,\n",
    "        samples: List[List[float]],\n",
    "        model_type: str = \"bayes\",\n",
<<<<<<< HEAD
    "        posterior_samples: Optional[int] = None,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Predict breast-cancer probability / class.\n",
    "        Accepts 5â€“30-feature vectors and pads with zeros so that downstream\n",
    "        models (trained on 30 features) never crash.\n",
    "        \"\"\"\n",
    "        model_name = (\n",
    "            f\"breast_cancer_{model_type}\" if model_type == \"bayes\" else f\"cancer_{model_type}\"\n",
    "        )\n",
=======
    "        posterior_samples: Optional[int] = None\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Make cancer predictions.\"\"\"\n",
    "        model_name = f\"breast_cancer_{model_type}\" if model_type == \"bayes\" else f\"cancer_{model_type}\"\n",
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
    "\n",
    "        # Handle different model naming conventions\n",
    "        available_models = [k for k in self.models.keys() if \"cancer\" in k or \"breast\" in k]\n",
    "        if model_name not in self.models and available_models:\n",
    "            model_name = available_models[0]  # Use first available cancer model\n",
    "\n",
    "        if model_name not in self.models:\n",
<<<<<<< HEAD
    "            raise ValueError(\n",
    "                f\"Cancer model not available â€“ loaded: {list(self.models.keys())}\"\n",
    "            )\n",
    "\n",
    "        model = self.models[model_name]\n",
    "\n",
    "        # ðŸ‘‰ normalise feature length to exactly 30\n",
    "        fixed_samples: List[List[float]] = []\n",
    "        for row in samples:\n",
    "            row_fixed = (row + [0.0]*30)[:30]   # pad with zeros, then truncate to 30\n",
    "            fixed_samples.append(row_fixed)\n",
    "\n",
    "        df = pd.DataFrame(fixed_samples)\n",
=======
    "            raise ValueError(f\"Cancer model not available. Available: {list(self.models.keys())}\")\n",
    "\n",
    "        model = self.models[model_name]\n",
    "\n",
    "        # Convert to DataFrame with proper feature names\n",
    "        df = pd.DataFrame(samples)\n",
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
    "\n",
    "        # Make predictions\n",
    "        predictions = model.predict(df)\n",
    "\n",
    "        # Handle uncertainty for Bayesian models\n",
<<<<<<< HEAD
    "        uncertainty = (\n",
    "            [\n",
    "                {\"lower\": max(0.0, p - 0.1), \"upper\": min(1.0, p + 0.1)}\n",
    "                for p in predictions\n",
    "            ] if posterior_samples and \"bayes\" in model_name else None\n",
    "        )\n",
    "\n",
    "        # Convert to class names\n",
    "        class_names = [\"malignant\", \"benign\"]\n",
    "        predicted_classes = [\n",
    "            class_names[int(p)] if isinstance(p, (int, np.integer)) else\n",
    "            (class_names[0] if p > 0.5 else class_names[1])\n",
    "            for p in predictions\n",
    "        ]\n",
=======
    "        uncertainty = None\n",
    "        if posterior_samples and \"bayes\" in model_name:\n",
    "            # Simple uncertainty estimation (could be improved)\n",
    "            uncertainty = [\n",
    "                {\"lower\": max(0, p - 0.1), \"upper\": min(1, p + 0.1)}\n",
    "                for p in predictions\n",
    "            ]\n",
    "\n",
    "        # Convert to class names\n",
    "        class_names = [\"malignant\", \"benign\"]\n",
    "        predicted_classes = []\n",
    "\n",
    "        for p in predictions:\n",
    "            if isinstance(p, (int, np.integer)):\n",
    "                predicted_classes.append(class_names[int(p)])\n",
    "            else:\n",
    "                # For probability predictions, use threshold\n",
    "                predicted_classes.append(class_names[0] if p > 0.5 else class_names[1])\n",
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
    "\n",
    "        return {\n",
    "            \"predictions\": predictions.tolist() if hasattr(predictions, 'tolist') else list(predictions),\n",
    "            \"predicted_classes\": predicted_classes,\n",
    "            \"class_names\": class_names,\n",
    "            \"model_used\": model_name,\n",
    "            \"model_version\": self.model_info.get(model_name, {}).get(\"version\"),\n",
    "            \"uncertainty\": uncertainty,\n",
<<<<<<< HEAD
    "            \"posterior_samples\": posterior_samples,\n",
=======
    "            \"posterior_samples\": posterior_samples if posterior_samples else None\n",
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
    "        }\n",
    "\n",
    "    async def get_health_status(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get the health status of the service.\"\"\"\n",
    "        if not self.initialized:\n",
    "            return {\n",
    "                \"status\": \"initializing\",\n",
    "                \"version\": settings.VERSION,\n",
    "                \"models\": {},\n",
    "                \"mlflow_uri\": settings.MLFLOW_TRACKING_URI\n",
    "            }\n",
    "\n",
    "        # Check model status\n",
    "        model_status = {}\n",
    "        for model_name, config in self.model_configs.items():\n",
    "            is_loaded = model_name in self.models\n",
    "            info = self.model_info.get(model_name, {})\n",
    "\n",
    "            model_status[model_name] = ModelInfo(\n",
    "                name=model_name,\n",
    "                version=info.get(\"version\"),\n",
    "                status=\"loaded\" if is_loaded else \"not_loaded\",\n",
    "                accuracy=info.get(\"accuracy\"),\n",
    "                created_at=info.get(\"created_at\"),\n",
    "                run_id=info.get(\"run_id\")\n",
    "            )\n",
    "\n",
    "        # Overall status\n",
    "        loaded_models = sum(1 for name in self.model_configs.keys() if name in self.models)\n",
    "        total_models = len(self.model_configs)\n",
    "\n",
    "        if loaded_models == total_models:\n",
    "            status = \"healthy\"\n",
    "        elif loaded_models > 0:\n",
    "            status = \"degraded\"\n",
    "        else:\n",
    "            status = \"unhealthy\"\n",
    "\n",
    "        return {\n",
    "            \"status\": status,\n",
    "            \"version\": settings.VERSION,\n",
    "            \"models\": {k: v.dict() for k, v in model_status.items()},\n",
    "            \"mlflow_uri\": settings.MLFLOW_TRACKING_URI,\n",
    "            \"loaded_models\": f\"{loaded_models}/{total_models}\"\n",
    "        }\n",
    "\n",
    "    async def get_model_metrics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get metrics for all loaded models.\"\"\"\n",
    "        metrics = {}\n",
    "\n",
    "        for model_name in self.models.keys():\n",
    "            info = self.model_info.get(model_name, {})\n",
    "            model_metrics = info.get(\"metrics\", {})\n",
    "\n",
    "            metrics[model_name] = {\n",
    "                \"accuracy\": model_metrics.get(\"accuracy\"),\n",
    "                \"f1_macro\": model_metrics.get(\"f1_macro\"),\n",
    "                \"precision_macro\": model_metrics.get(\"precision_macro\"),\n",
    "                \"recall_macro\": model_metrics.get(\"recall_macro\"),\n",
    "                \"version\": info.get(\"version\"),\n",
    "                \"run_id\": info.get(\"run_id\")\n",
    "            }\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    # ---------- Retraining Methods -----------------------------------------\n",
    "    async def retrain_iris(self, n_trials: int = 50) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Fire-and-forget Optuna retrain for Iris RF + LR.\n",
    "        Returns best metric dict for immediate response.\n",
    "        \"\"\"\n",
    "        logger.info(f\"Starting Iris retrain with {n_trials} trials\")\n",
    "\n",
<<<<<<< HEAD
    "        if run_all_trainings is None:\n",
    "            logger.info(\"Training module not available in this deployment\")\n",
    "            raise RuntimeError(\n",
    "                \"Training module not available in this deployment. \"\n",
    "                \"This is expected in production environments.\"\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            run_all_trainings(n_trials=n_trials)\n",
    "            await self._promote_latest_to_production(\"iris_random_forest\")\n",
    "            await self._promote_latest_to_production(\"iris_logreg\")\n",
    "            await self._load_models()  # hot-reload cache\n",
    "\n",
    "            # Surface new metrics\n",
    "            metrics = await self.get_model_metrics()\n",
    "            iris_metrics = metrics.get(\"iris_rf\", {})\n",
    "            logger.info(f\"Iris retrain completed. New accuracy: {iris_metrics.get('accuracy')}\")\n",
    "            return iris_metrics\n",
=======
    "        try:\n",
    "            if run_all_trainings:\n",
    "                run_all_trainings(n_trials=n_trials)\n",
    "                await self._promote_latest_to_production(\"iris_random_forest\")\n",
    "                await self._promote_latest_to_production(\"iris_logreg\")\n",
    "                await self._load_models()  # hot-reload cache\n",
    "\n",
    "                # Surface new metrics\n",
    "                metrics = await self.get_model_metrics()\n",
    "                iris_metrics = metrics.get(\"iris_random_forest\", {})\n",
    "                logger.info(f\"Iris retrain completed. New accuracy: {iris_metrics.get('accuracy')}\")\n",
    "                return iris_metrics\n",
    "            else:\n",
    "                raise RuntimeError(\"run_all_trainings not available\")\n",
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Iris retrain failed: {e}\")\n",
    "            raise\n",
    "\n",
    "    async def retrain_cancer_bayes(\n",
    "        self,\n",
    "        draws: int = 800,\n",
    "        tune: int = 400,\n",
    "        target_accept: float = 0.9,\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Launch PyMC retrain with custom MCMC params.\n",
    "        \"\"\"\n",
    "        logger.info(f\"Starting Cancer Bayesian retrain: draws={draws}, tune={tune}, target_accept={target_accept}\")\n",
    "\n",
<<<<<<< HEAD
    "        if train_bayes_logreg is None:\n",
    "            logger.info(\"Bayesian training module not available in this deployment\")\n",
    "            raise RuntimeError(\n",
    "                \"Bayesian training module not available in this deployment. \"\n",
    "                \"This is expected in production environments.\"\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            train_bayes_logreg(\n",
    "                draws=draws,\n",
    "                tune=tune,\n",
    "                register=True,\n",
    "                target_accept=target_accept\n",
    "            )\n",
    "            await self._promote_latest_to_production(\"breast_cancer_bayes\")\n",
    "            await self._load_models()\n",
    "\n",
    "            metrics = await self.get_model_metrics()\n",
    "            cancer_metrics = metrics.get(\"breast_cancer_bayes\", {})\n",
    "            logger.info(f\"Cancer retrain completed. New accuracy: {cancer_metrics.get('accuracy')}\")\n",
    "            return cancer_metrics\n",
=======
    "        try:\n",
    "            if train_bayes_logreg:\n",
    "                train_bayes_logreg(\n",
    "                    draws=draws,\n",
    "                    tune=tune,\n",
    "                    register=True,\n",
    "                    target_accept=target_accept\n",
    "                )\n",
    "                await self._promote_latest_to_production(\"breast_cancer_bayes\")\n",
    "                await self._load_models()\n",
    "\n",
    "                metrics = await self.get_model_metrics()\n",
    "                cancer_metrics = metrics.get(\"breast_cancer_bayes\", {})\n",
    "                logger.info(f\"Cancer retrain completed. New accuracy: {cancer_metrics.get('accuracy')}\")\n",
    "                return cancer_metrics\n",
    "            else:\n",
    "                raise RuntimeError(\"train_bayes_logreg not available\")\n",
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Cancer retrain failed: {e}\")\n",
    "            raise\n",
    "\n",
<<<<<<< HEAD
    "    async def _load_fallback_models(self) -> None:\n",
    "        \"\"\"Load fallback in-memory models using scikit-learn datasets.\"\"\"\n",
    "        logger.info(\"Checking for missing models and loading fallbacks...\")\n",
    "\n",
    "        # Fallback for iris models\n",
    "        if \"iris_rf\" not in self.models:\n",
    "            logger.info(\"âš ï¸ Fallback: training in-memory iris_rf model using sklearn iris dataset\")\n",
    "            iris_data = load_iris()\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                iris_data.data, iris_data.target, test_size=0.2, random_state=42\n",
    "            )\n",
    "            \n",
    "            clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "            clf.fit(X_train, y_train)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            accuracy = accuracy_score(y_test, clf.predict(X_test))\n",
    "            \n",
    "            self.models[\"iris_rf\"] = clf\n",
    "            self.model_info[\"iris_rf\"] = {\n",
    "                \"name\": \"iris_rf\",\n",
    "                \"version\": \"fallback\",\n",
    "                \"status\": \"loaded\",\n",
    "                \"accuracy\": accuracy,\n",
    "                \"run_id\": \"fallback\",\n",
    "                \"created_at\": None,\n",
    "                \"metrics\": {\"accuracy\": accuracy}\n",
    "            }\n",
    "            logger.info(f\"âœ… Loaded fallback iris_rf model with accuracy: {accuracy:.3f}\")\n",
    "\n",
    "        if \"iris_logreg\" not in self.models:\n",
    "            logger.info(\"âš ï¸ Fallback: training in-memory iris_logreg model using sklearn iris dataset\")\n",
    "            iris_data = load_iris()\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                iris_data.data, iris_data.target, test_size=0.2, random_state=42\n",
    "            )\n",
    "            \n",
    "            clf = LogisticRegression(random_state=42, max_iter=1000)\n",
    "            clf.fit(X_train, y_train)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            accuracy = accuracy_score(y_test, clf.predict(X_test))\n",
    "            \n",
    "            self.models[\"iris_logreg\"] = clf\n",
    "            self.model_info[\"iris_logreg\"] = {\n",
    "                \"name\": \"iris_logreg\",\n",
    "                \"version\": \"fallback\",\n",
    "                \"status\": \"loaded\",\n",
    "                \"accuracy\": accuracy,\n",
    "                \"run_id\": \"fallback\",\n",
    "                \"created_at\": None,\n",
    "                \"metrics\": {\"accuracy\": accuracy}\n",
    "            }\n",
    "            logger.info(f\"âœ… Loaded fallback iris_logreg model with accuracy: {accuracy:.3f}\")\n",
    "\n",
    "        # Fallback for cancer models\n",
    "        if \"breast_cancer_bayes\" not in self.models:\n",
    "            logger.info(\"âš ï¸ Fallback: training in-memory breast_cancer_bayes model using sklearn breast cancer dataset\")\n",
    "            cancer_data = load_breast_cancer()\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                cancer_data.data, cancer_data.target, test_size=0.2, random_state=42\n",
    "            )\n",
    "            \n",
    "            # Use logistic regression as fallback for bayesian model\n",
    "            clf = LogisticRegression(random_state=42, max_iter=1000)\n",
    "            clf.fit(X_train, y_train)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            accuracy = accuracy_score(y_test, clf.predict(X_test))\n",
    "            \n",
    "            self.models[\"breast_cancer_bayes\"] = clf\n",
    "            self.model_info[\"breast_cancer_bayes\"] = {\n",
    "                \"name\": \"breast_cancer_bayes\",\n",
    "                \"version\": \"fallback\",\n",
    "                \"status\": \"loaded\",\n",
    "                \"accuracy\": accuracy,\n",
    "                \"run_id\": \"fallback\",\n",
    "                \"created_at\": None,\n",
    "                \"metrics\": {\"accuracy\": accuracy}\n",
    "            }\n",
    "            logger.info(f\"âœ… Loaded fallback breast_cancer_bayes model with accuracy: {accuracy:.3f}\")\n",
    "\n",
    "\n",
    "# Global model service instance\n",
    "model_service = ModelService()\n"
=======
    "\n",
    "# Global model service instance\n",
    "model_service = ModelService()\n",
    "\n"
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 259,
   "id": "6b718e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting backend/app/api/api_v1/endpoints/health.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile backend/app/api/api_v1/endpoints/health.py\n",
    "\"\"\"Health endpoints at /api/v1/health for frontend and Render compatibility.\"\"\"\n",
    "\n",
    "import logging\n",
    "from typing import Dict, Any\n",
    "from fastapi import APIRouter, HTTPException\n",
    "from fastapi.responses import Response\n",
    "from ....schemas.common import HealthResponse\n",
    "from ....services.ml.model_service import model_service\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "router = APIRouter()\n",
    "\n",
    "\n",
    "@router.get(\"/health\", response_model=Dict[str, Any], tags=[\"Health\"])\n",
    "async def api_v1_health():\n",
    "    \"\"\"\n",
    "    GET /api/v1/health - Detailed JSON health report.\n",
    "    \n",
    "    This endpoint provides comprehensive health information including:\n",
    "    - Overall service status\n",
    "    - Individual model status\n",
    "    - MLflow connection status\n",
    "    - Model metrics\n",
    "    \n",
    "    Used by frontend applications and monitoring systems.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return await model_service.get_health_status()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Health check error: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Health check failed\")\n",
    "\n",
    "\n",
    "@router.head(\"/health\", include_in_schema=False)\n",
    "async def api_v1_health_head():\n",
    "    \"\"\"\n",
    "    HEAD /api/v1/health - Lightweight probe for load balancers.\n",
    "    \n",
    "    Returns only HTTP status code without response body.\n",
    "    Used by Render health probes and other monitoring systems\n",
    "    that only need to verify service availability.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Just check if the service is initialized\n",
    "        health_status = await model_service.get_health_status()\n",
    "        # Return 200 if service is running, regardless of model status\n",
    "        return Response(status_code=200)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Health check HEAD error: {e}\")\n",
    "        return Response(status_code=503)  # Service Unavailable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
=======
   "execution_count": 56,
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
   "id": "1d87ca37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting backend/app/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile backend/app/main.py\n",
    "\"\"\"Main FastAPI application.\"\"\"\n",
    "\n",
    "import logging\n",
    "from contextlib import asynccontextmanager\n",
    "from fastapi import FastAPI\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "\n",
<<<<<<< HEAD
    "# ðŸ”§ Adjusted imports to use \"app.\" as the top-level package\n",
    "#     so that when rootDir=backend, Python can resolve modules.\n",
    "from app.core.config import settings\n",
    "from app.api.api_v1.api import api_router\n",
    "from app.services.ml.model_service import model_service\n",
    "import os\n",
=======
    "from backend.app.core.config import settings\n",
    "from backend.app.api.api_v1.api import api_router\n",
    "from backend.app.services.ml.model_service import model_service\n",
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "@asynccontextmanager\n",
    "async def lifespan(app: FastAPI):\n",
    "    \"\"\"Manage application lifespan - startup and shutdown.\"\"\"\n",
<<<<<<< HEAD
    "    logger.info(f\"ðŸŒ€ PORT env inside container: {os.getenv('PORT')}\")\n",
    "    logger.info(\"ðŸ”§ Initializing ML model service...\")\n",
    "    logger.info(\"ðŸŒ€ [lifespan] Entering lifespanâ€”about to init services\")\n",
=======
    "    logger.info(\"ðŸš€ Starting ML Full Stack API...\")\n",
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
    "\n",
    "    try:\n",
    "        # Initialize ML service\n",
    "        logger.info(\"ðŸ”§ Initializing ML model service...\")\n",
    "        await model_service.initialize()\n",
    "        logger.info(\"âœ… ML model service initialized successfully\")\n",
    "\n",
    "        yield\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ Failed to initialize ML service: {e}\")\n",
    "        raise\n",
    "    finally:\n",
<<<<<<< HEAD
    "        logger.info(\"ðŸŒ€ [lifespan] After yieldâ€”now shutting down\")\n",
=======
    "        logger.info(\"ðŸ”„ Shutting down ML Full Stack API...\")\n",
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
    "        await model_service.cleanup()\n",
    "\n",
    "\n",
    "def create_application() -> FastAPI:\n",
    "    \"\"\"Create and configure the FastAPI application.\"\"\"\n",
    "    app = FastAPI(\n",
    "        title=settings.PROJECT_NAME,\n",
    "        description=\"Full Stack ML API with Iris and Cancer prediction models\",\n",
    "        version=settings.VERSION,\n",
    "        openapi_url=f\"{settings.API_V1_STR}/openapi.json\",\n",
    "        docs_url=f\"{settings.API_V1_STR}/docs\",\n",
    "        redoc_url=f\"{settings.API_V1_STR}/redoc\",\n",
    "        lifespan=lifespan\n",
    "    )\n",
    "\n",
    "    # Add CORS middleware\n",
    "    app.add_middleware(\n",
    "        CORSMiddleware,\n",
    "        allow_origins=settings.BACKEND_CORS_ORIGINS,\n",
    "        allow_credentials=True,\n",
    "        allow_methods=[\"*\"],\n",
    "        allow_headers=[\"*\"],\n",
    "    )\n",
    "\n",
    "    # Include API router\n",
    "    app.include_router(api_router, prefix=settings.API_V1_STR)\n",
    "\n",
    "    @app.get(\"/\")\n",
    "    async def root():\n",
    "        \"\"\"Root endpoint.\"\"\"\n",
    "        return {\n",
    "            \"message\": \"ML Full Stack API\",\n",
    "            \"version\": settings.VERSION,\n",
    "            \"docs_url\": f\"{settings.API_V1_STR}/docs\",\n",
    "            \"health_url\": f\"{settings.API_V1_STR}/health\"\n",
    "        }\n",
    "\n",
<<<<<<< HEAD
    "    @app.head(\"/\", include_in_schema=False)\n",
    "    async def root_head():\n",
    "        \"\"\"Root HEAD endpoint for Render probes.\"\"\"\n",
    "        from fastapi.responses import Response\n",
    "        return Response(status_code=200)\n",
    "\n",
=======
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
    "    @app.get(\"/health\")\n",
    "    async def health():\n",
    "        \"\"\"Health check endpoint.\"\"\"\n",
    "        return await model_service.get_health_status()\n",
    "\n",
<<<<<<< HEAD
    "    @app.head(\"/health\", include_in_schema=False)\n",
    "    async def health_head():\n",
    "        \"\"\"Health check HEAD endpoint for Render probes.\"\"\"\n",
    "        from fastapi.responses import Response\n",
    "        return Response(status_code=200)\n",
    "\n",
=======
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
    "    return app\n",
    "\n",
    "\n",
    "# Create the application instance\n",
<<<<<<< HEAD
    "app = create_application()\n",
    "\n"
=======
    "app = create_application()\n"
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eac4382",
   "metadata": {},
   "source": [
    "# Frontend"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 261,
=======
   "execution_count": 57,
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
   "id": "ebec606e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting frontend/package.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile frontend/package.json\n",
    "{\n",
    "  \"name\": \"ml-fullstack-frontend\",\n",
    "  \"private\": true,\n",
    "  \"version\": \"1.0.0\",\n",
    "  \"type\": \"module\",\n",
    "  \"scripts\": {\n",
    "    \"dev\": \"vite\",\n",
    "    \"build\": \"vite build\",\n",
    "    \"lint\": \"eslint . --ext js,jsx --report-unused-disable-directives --max-warnings 0\",\n",
    "    \"preview\": \"vite preview\"\n",
    "  },\n",
    "  \"dependencies\": {\n",
    "    \"react\": \"^18.2.0\",\n",
    "    \"react-dom\": \"^18.2.0\",\n",
    "    \"axios\": \"^1.6.0\",\n",
    "    \"react-router-dom\": \"^6.8.0\",\n",
    "    \"recharts\": \"^2.8.0\",\n",
    "    \"@headlessui/react\": \"^1.7.17\",\n",
    "    \"@heroicons/react\": \"^2.0.18\",\n",
    "    \"lucide-react\": \"^0.453.0\"\n",
    "  },\n",
    "  \"devDependencies\": {\n",
    "    \"@types/react\": \"^18.2.37\",\n",
    "    \"@types/react-dom\": \"^18.2.15\",\n",
    "    \"@vitejs/plugin-react\": \"^4.1.1\",\n",
    "    \"eslint\": \"^8.53.0\",\n",
    "    \"eslint-plugin-react\": \"^7.33.2\",\n",
    "    \"eslint-plugin-react-hooks\": \"^4.6.0\",\n",
    "    \"eslint-plugin-react-refresh\": \"^0.4.4\",\n",
    "    \"autoprefixer\": \"^10.4.16\",\n",
    "    \"postcss\": \"^8.4.31\",\n",
    "    \"tailwindcss\": \"^3.3.5\",\n",
    "    \"vite\": \"^4.5.0\"\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 262,
=======
   "execution_count": 58,
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
   "id": "7e8150b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting frontend/postcss.config.js\n"
     ]
    }
   ],
   "source": [
    "%%writefile frontend/postcss.config.js\n",
    "export default {\n",
    "  plugins: {\n",
    "    tailwindcss: {},\n",
    "    autoprefixer: {},\n",
    "  },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 263,
=======
   "execution_count": 59,
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
   "id": "c3974081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting frontend/tailwind.config.js\n"
     ]
    }
   ],
   "source": [
    "%%writefile frontend/tailwind.config.js\n",
    "/** @type {import('tailwindcss').Config} */\n",
    "export default {\n",
    "  content: [\n",
    "    \"./index.html\",\n",
    "    \"./src/**/*.{js,ts,jsx,tsx}\",\n",
    "  ],\n",
    "  theme: {\n",
    "    extend: {\n",
    "      colors: {\n",
    "        primary: {\n",
    "          50: '#f0f9ff',\n",
    "          500: '#3b82f6',\n",
    "          600: '#2563eb',\n",
    "          700: '#1d4ed8',\n",
    "        },\n",
    "        secondary: {\n",
    "          50: '#f9fafb',\n",
    "          100: '#f3f4f6',\n",
    "          500: '#6b7280',\n",
    "          600: '#4b5563',\n",
    "          700: '#374151',\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "  },\n",
    "  plugins: [],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 264,
=======
   "execution_count": 60,
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
   "id": "d1a46149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting frontend/vite.config.js\n"
     ]
    }
   ],
   "source": [
    "%%writefile frontend/vite.config.js\n",
    "import { defineConfig, loadEnv } from 'vite'\n",
    "import react from '@vitejs/plugin-react'\n",
    "\n",
    "// https://vitejs.dev/config/\n",
    "export default defineConfig(({ mode }) => {\n",
    "  const env = loadEnv(mode, process.cwd(), '')\n",
<<<<<<< HEAD
    "      const API_URL = env.VITE_API_URL || 'http://127.0.0.1:8000'   // Default to local FastAPI\n",
=======
    "  const API_URL = env.VITE_API_URL || 'http://localhost:8000'   // Default to local FastAPI\n",
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
    "\n",
    "  return {\n",
    "    plugins: [react()],\n",
    "    define: {\n",
    "      __API_URL__: JSON.stringify(API_URL),\n",
    "    },\n",
    "    server: {\n",
    "      host: '0.0.0.0',\n",
    "      port: 5173,\n",
    "      proxy: {\n",
    "        '/api/v1': {\n",
    "          target: API_URL,\n",
    "          changeOrigin: true,\n",
    "          secure: false,\n",
    "          xfwd: true,\n",
    "        },\n",
    "      },\n",
    "    },\n",
    "    build: { outDir: 'dist' },\n",
    "  }\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 265,
=======
   "execution_count": 61,
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
   "id": "2dcd9330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting frontend/index.html\n"
     ]
    }
   ],
   "source": [
    "%%writefile frontend/index.html\n",
    "<!doctype html>\n",
    "<html lang=\"en\">\n",
    "  <head>\n",
    "    <meta charset=\"UTF-8\" />\n",
    "    <link rel=\"icon\" type=\"image/svg+xml\" href=\"/vite.svg\" />\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n",
    "    <title>ML Full Stack - Iris & Cancer Predictions</title>\n",
    "    <meta name=\"description\" content=\"Full stack ML application with iris and breast cancer prediction models\" />\n",
    "  </head>\n",
    "  <body>\n",
    "    <div id=\"root\"></div>\n",
    "    <script type=\"module\" src=\"/src/main.jsx\"></script>\n",
    "  </body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 266,
=======
   "execution_count": 62,
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
   "id": "239c57ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting frontend/src/index.css\n"
     ]
    }
   ],
   "source": [
    "%%writefile frontend/src/index.css\n",
    "@tailwind base;\n",
    "@tailwind components;\n",
    "@tailwind utilities;\n",
    "\n",
    "/* Custom styles */\n",
    "@layer base {\n",
    "  body {\n",
    "    @apply bg-gray-50 text-gray-900;\n",
    "  }\n",
    "}\n",
    "\n",
    "@layer components {\n",
    "  .btn-primary {\n",
    "    @apply inline-flex items-center px-4 py-2 border border-transparent text-sm font-medium rounded-md shadow-sm text-white bg-primary-600 hover:bg-primary-700 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-primary-500;\n",
    "  }\n",
    "\n",
    "  .btn-secondary {\n",
    "    @apply inline-flex items-center px-4 py-2 border border-gray-300 text-sm font-medium rounded-md shadow-sm text-gray-700 bg-white hover:bg-gray-50 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-primary-500;\n",
    "  }\n",
    "\n",
    "  .card {\n",
    "    @apply bg-white shadow rounded-lg p-6;\n",
    "  }\n",
    "\n",
    "  .form-input {\n",
    "    @apply block w-full px-3 py-2 border border-gray-300 rounded-md shadow-sm placeholder-gray-400 focus:outline-none focus:ring-primary-500 focus:border-primary-500;\n",
    "  }\n",
    "\n",
    "  .form-label {\n",
    "    @apply block text-sm font-medium text-gray-700 mb-1;\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 267,
=======
   "execution_count": 63,
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
   "id": "d0c210d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting frontend/src/services/api.js\n"
     ]
    }
   ],
   "source": [
    "%%writefile frontend/src/services/api.js\n",
    "import axios from 'axios';\n",
    "\n",
<<<<<<< HEAD
    "const api = axios.create({\n",
    "  baseURL: import.meta.env.VITE_API_URL || '/api/v1',\n",
=======
    "// Create axios instance with base configuration\n",
    "const api = axios.create({\n",
    "  baseURL: '/api/v1', // Use proxy\n",
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
    "  timeout: 30000,\n",
    "  headers: {\n",
    "    'Content-Type': 'application/json',\n",
    "  },\n",
    "});\n",
    "\n",
    "// Request interceptor\n",
    "api.interceptors.request.use(\n",
    "  (config) => {\n",
    "    console.log(`API Request: ${config.method?.toUpperCase()} ${config.url}`);\n",
    "    return config;\n",
    "  },\n",
    "  (error) => {\n",
    "    console.error('API Request Error:', error);\n",
    "    return Promise.reject(error);\n",
    "  }\n",
    ");\n",
    "\n",
    "// Response interceptor\n",
    "api.interceptors.response.use(\n",
    "  (response) => {\n",
    "    console.log(`API Response: ${response.status} ${response.config.url}`);\n",
    "    return response;\n",
    "  },\n",
    "  (error) => {\n",
    "    console.error('API Response Error:', {\n",
    "      status: error.response?.status,\n",
    "      url: error.config?.url,\n",
    "      message: error.response?.data?.detail || error.message,\n",
    "    });\n",
    "    return Promise.reject(error);\n",
    "  }\n",
    ");\n",
    "\n",
    "// API methods\n",
    "export const apiService = {\n",
    "  // Health and system endpoints\n",
    "  async getHealth() {\n",
    "    const response = await api.get('/models/health');\n",
    "    return response.data;\n",
    "  },\n",
    "\n",
    "  async getModelMetrics() {\n",
    "    const response = await api.get('/models/metrics');\n",
    "    return response.data;\n",
    "  },\n",
    "\n",
    "  async listModels() {\n",
    "    const response = await api.get('/models/list');\n",
    "    return response.data;\n",
    "  },\n",
    "\n",
    "  async reloadModels() {\n",
    "    const response = await api.post('/models/reload');\n",
    "    return response.data;\n",
    "  },\n",
    "\n",
    "  // Iris endpoints\n",
    "  async predictIris(data) {\n",
<<<<<<< HEAD
    "    // Convert rows to samples if needed for backward compatibility\n",
    "    const payload = {\n",
    "      model_type: data.model_type || 'rf',\n",
    "      samples: data.rows || data.samples\n",
    "    };\n",
    "    const response = await api.post('/iris/predict', payload);\n",
=======
    "    const response = await api.post('/iris/predict', data);\n",
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
    "    return response.data;\n",
    "  },\n",
    "\n",
    "  async getIrisModels() {\n",
    "    const response = await api.get('/iris/models');\n",
    "    return response.data;\n",
    "  },\n",
    "\n",
    "  async getIrisSampleData() {\n",
    "    const response = await api.get('/iris/sample-data');\n",
    "    return response.data;\n",
    "  },\n",
    "\n",
    "  // Cancer endpoints\n",
    "  async predictCancer(data) {\n",
    "    const response = await api.post('/cancer/predict', data);\n",
    "    return response.data;\n",
    "  },\n",
    "\n",
    "  async getCancerModels() {\n",
    "    const response = await api.get('/cancer/models');\n",
    "    return response.data;\n",
    "  },\n",
    "\n",
    "  async getCancerSampleData() {\n",
    "    const response = await api.get('/cancer/sample-data');\n",
    "    return response.data;\n",
    "  },\n",
    "\n",
    "  // Retraining endpoints\n",
    "  async retrainIris(hyperparameters = {}) {\n",
    "    const response = await api.post('/iris/retrain', {\n",
    "      model_type: 'rf',\n",
    "      hyperparameters\n",
    "    });\n",
    "    return response.data;\n",
    "  },\n",
    "\n",
    "  async retrainCancer(hyperparameters = {}) {\n",
    "    const response = await api.post('/cancer/retrain', {\n",
    "      model_type: 'bayes',\n",
    "      hyperparameters\n",
    "    });\n",
    "    return response.data;\n",
    "  },\n",
    "\n",
    "  // Utility methods for polling\n",
    "  async waitForMetrics(modelKey, maxAttempts = 60) {\n",
    "    let attempts = 0;\n",
    "    while (attempts < maxAttempts) {\n",
    "      try {\n",
    "        const metrics = await this.getModelMetrics();\n",
    "        if (metrics[modelKey]?.accuracy !== undefined) {\n",
    "          return metrics[modelKey];\n",
    "        }\n",
    "        await new Promise(resolve => setTimeout(resolve, 5000)); // Wait 5 seconds\n",
    "        attempts++;\n",
    "      } catch (error) {\n",
    "        console.error('Error polling metrics:', error);\n",
    "        attempts++;\n",
    "      }\n",
    "    }\n",
    "    throw new Error(`Timeout waiting for metrics for ${modelKey}`);\n",
    "  }\n",
    "};\n",
    "\n",
<<<<<<< HEAD
    "export default apiService;\n",
    "\n"
=======
    "export default apiService;\n"
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 268,
=======
   "execution_count": 64,
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
   "id": "be68addb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting frontend/src/App.jsx\n"
     ]
    }
   ],
   "source": [
    "%%writefile frontend/src/App.jsx\n",
    "import React, { useState, useEffect } from 'react';\n",
    "import { LineChart, Line, XAxis, YAxis, CartesianGrid, Tooltip, Legend, ResponsiveContainer, BarChart, Bar, ScatterChart, Scatter, PieChart, Pie, Cell } from 'recharts';\n",
    "import { Activity, Brain, Database, TrendingUp, AlertCircle, CheckCircle, Play, Upload, Download, Settings, BarChart3, Target } from 'lucide-react';\n",
    "import apiService from './services/api';\n",
    "\n",
    "const MLModelFrontend = () => {\n",
    "  const [activeTab, setActiveTab] = useState('dashboard');\n",
    "  const [selectedDataset, setSelectedDataset] = useState('iris');\n",
    "  const [modelStatus, setModelStatus] = useState('idle');\n",
    "  const [predictions, setPredictions] = useState([]);\n",
    "  const [trainingHistory, setTrainingHistory] = useState([]);\n",
    "  const [apiHealth, setApiHealth] = useState('unknown');\n",
    "  const [inputData, setInputData] = useState({});\n",
    "  const [modelMetrics, setModelMetrics] = useState(null);\n",
    "  const [predictionResults, setPredictionResults] = useState(null);\n",
    "\n",
    "  // Training parameters\n",
    "  const [trainingParams, setTrainingParams] = useState({\n",
    "    iris: { n_trials: 50 },\n",
    "    cancer: { draws: 800, tune: 400, target_accept: 0.9 }\n",
    "  });\n",
    "\n",
    "  // API Base URL - adjust this to match your backend\n",
    "  const API_BASE = '' // unused â€“ kept for backward compat\n",
    "\n",
    "  // --- Real API helper  ---------------------------------------------------\n",
    "  const callApi = async (path, payload = null, opts = {}) => {\n",
    "    /*\n",
<<<<<<< HEAD
    "      Direct frontendâ†’FastAPI communication using environment variable.\n",
    "      In development: uses Vite proxy to localhost:8000\n",
    "      In production: uses VITE_API_URL environment variable\n",
    "    */\n",
    "    const API_BASE = import.meta.env.VITE_API_URL || '/api/v1'\n",
    "    const url = path.startsWith('/api/v1') ? \n",
    "      `${API_BASE.replace('/api/v1', '')}${path}` : \n",
    "      `${API_BASE}${path}`\n",
=======
    "      Direct frontendâ†’FastAPI communication using the `/api/v1` prefix.\n",
    "      Vite dev server proxies `/api/v1` calls directly to FastAPI.\n",
    "      This eliminates the Express proxy layer.\n",
    "    */\n",
    "    const url = path.startsWith('/api/v1') ? path : `/api/v1${path}`\n",
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
    "\n",
    "    // ðŸ” DEBUG: Log the exact URL being called\n",
    "    console.log('ðŸ” callApi DEBUG:', {\n",
    "      originalPath: path,\n",
    "      finalUrl: url,\n",
    "      hasPayload: payload !== null,\n",
    "      method: payload !== null ? 'POST' : 'GET'\n",
    "    })\n",
    "\n",
    "    const cfg = payload !== null\n",
    "      ? {\n",
    "          method: 'POST',\n",
    "          headers: { 'Content-Type': 'application/json' },\n",
    "          body: JSON.stringify(payload),\n",
    "          ...opts,\n",
    "        }\n",
    "      : { method: 'GET', ...opts }\n",
    "\n",
    "    try {\n",
    "      const res = await fetch(url, cfg)\n",
    "      console.log('ðŸ” callApi Response:', {\n",
    "        status: res.status,\n",
    "        ok: res.ok,\n",
    "        url: res.url\n",
    "      })\n",
    "\n",
    "      if (!res.ok) {\n",
    "        const txt = await res.text()\n",
    "        console.error('âŒ callApi Error:', {\n",
    "          status: res.status,\n",
    "          statusText: res.statusText,\n",
    "          body: txt,\n",
    "          url: url\n",
    "        })\n",
    "        throw new Error(`API ${res.status} â€” ${txt}`)\n",
    "      }\n",
    "      return res.json()\n",
    "    } catch (error) {\n",
    "      console.error('âŒ callApi Exception:', {\n",
    "        error: error.message,\n",
    "        url: url,\n",
    "        payload: payload\n",
    "      })\n",
    "      throw error\n",
    "    }\n",
    "  }\n",
    "\n",
    "  // Dataset configurations\n",
    "  const datasets = {\n",
    "    iris: {\n",
    "      name: 'Iris Dataset',\n",
    "      description: 'Classic iris flower classification (Setosa vs Others)',\n",
    "      features: ['sepal_length', 'sepal_width', 'petal_length', 'petal_width'],\n",
    "      featureLabels: ['Sepal Length (cm)', 'Sepal Width (cm)', 'Petal Length (cm)', 'Petal Width (cm)'],\n",
    "      targetClasses: ['Setosa', 'Non-Setosa'],\n",
    "      color: '#8884d8'\n",
    "    },\n",
    "    breast_cancer: {\n",
    "      name: 'Breast Cancer Dataset',\n",
    "      description: 'Breast cancer diagnosis prediction (Malignant vs Benign)',\n",
    "      features: ['mean_radius', 'mean_texture', 'mean_perimeter', 'mean_area', 'mean_smoothness'],\n",
    "      featureLabels: ['Mean Radius', 'Mean Texture', 'Mean Perimeter', 'Mean Area', 'Mean Smoothness'],\n",
    "      targetClasses: ['Malignant', 'Benign'],\n",
    "      color: '#82ca9d'\n",
    "    }\n",
    "  };\n",
    "\n",
    "  // Check API health\n",
    "  useEffect(() => {\n",
    "    const checkHealth = async () => {\n",
    "      try {\n",
    "        console.log('ðŸ” checkHealth DEBUG: Starting health check')\n",
<<<<<<< HEAD
    "        const response = await callApi('/api/v1/models/health');\n",
    "        console.log('ðŸ” checkHealth DEBUG: Health check successful:', response)\n",
    "        // Accept both healthy and degraded as OK states\n",
    "        setApiHealth(response.status === 'degraded' ? 'healthy' : response.status);\n",
=======
    "        const response = await callApi('/health');\n",
    "        console.log('ðŸ” checkHealth DEBUG: Health check successful:', response)\n",
    "        setApiHealth(response.status);\n",
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
    "      } catch (error) {\n",
    "        console.error('âŒ checkHealth DEBUG: Health check failed:', error)\n",
    "        setApiHealth('error');\n",
    "      }\n",
    "    };\n",
    "\n",
    "    checkHealth();\n",
    "    const interval = setInterval(checkHealth, 30000);\n",
    "    return () => clearInterval(interval);\n",
    "  }, []);\n",
    "\n",
    "  // Initialize input data when dataset changes\n",
    "  useEffect(() => {\n",
    "    const initData = {};\n",
    "    datasets[selectedDataset].features.forEach(feature => {\n",
    "      initData[feature] = '';\n",
    "    });\n",
    "    setInputData(initData);\n",
    "  }, [selectedDataset]);\n",
    "\n",
    "  // Train model\n",
    "  const handleTrainModel = async () => {\n",
    "    setModelStatus('training');\n",
    "    try {\n",
    "      console.log('ðŸ” handleTrainModel DEBUG: Starting training for dataset:', selectedDataset);\n",
    "\n",
    "      let result;\n",
    "      if (selectedDataset === 'iris') {\n",
    "        result = await apiService.retrainIris(trainingParams.iris);\n",
    "        console.log('ðŸ” handleTrainModel DEBUG: Iris training started:', result);\n",
    "\n",
    "        // Poll for completion\n",
    "        try {\n",
    "          const newMetrics = await apiService.waitForMetrics('iris_random_forest');\n",
    "          setModelMetrics(newMetrics);\n",
    "          console.log('ðŸ” handleTrainModel DEBUG: Training completed with metrics:', newMetrics);\n",
    "        } catch (error) {\n",
    "          console.warn('Training may still be in progress:', error.message);\n",
    "        }\n",
    "\n",
    "      } else if (selectedDataset === 'breast_cancer') {\n",
    "        result = await apiService.retrainCancer(trainingParams.cancer);\n",
    "        console.log('ðŸ” handleTrainModel DEBUG: Cancer training started:', result);\n",
    "\n",
    "        // Poll for completion\n",
    "        try {\n",
    "          const newMetrics = await apiService.waitForMetrics('breast_cancer_bayes');\n",
    "          setModelMetrics(newMetrics);\n",
    "          console.log('ðŸ” handleTrainModel DEBUG: Training completed with metrics:', newMetrics);\n",
    "        } catch (error) {\n",
    "          console.warn('Training may still be in progress:', error.message);\n",
    "        }\n",
    "\n",
    "      } else {\n",
    "        alert('Training is only supported for Iris and Breast Cancer models.');\n",
    "        setModelStatus('idle');\n",
    "        return;\n",
    "      }\n",
    "\n",
    "      // Update training history\n",
    "      setTrainingHistory((prev) => [\n",
    "        ...prev,\n",
    "        {\n",
    "          timestamp: new Date().toISOString(),\n",
    "          dataset: selectedDataset,\n",
    "          parameters: trainingParams[selectedDataset],\n",
    "          status: 'completed'\n",
    "        },\n",
    "      ]);\n",
    "\n",
    "      setModelStatus('trained');\n",
    "    } catch (error) {\n",
    "      console.error('âŒ handleTrainModel Error:', error);\n",
    "      setModelStatus('error');\n",
    "      alert(`Training failed: ${error.message}`);\n",
    "    }\n",
    "  };\n",
    "\n",
    "  // Make prediction\n",
    "  const handlePredict = async () => {\n",
<<<<<<< HEAD
    "    let result; // âš ï¸ Declare variable to avoid strict-mode error\n",
    "    try {\n",
    "      console.log('ðŸ” handlePredict DEBUG: Starting prediction for dataset:', selectedDataset)\n",
    "      \n",
    "      // Validate input ranges for iris dataset\n",
    "      if (selectedDataset === 'iris') {\n",
    "        const validationRules = {\n",
    "          sepal_length: { min: 4.0, max: 8.0 },\n",
    "          sepal_width: { min: 2.0, max: 4.5 },\n",
    "          petal_length: { min: 1.0, max: 7.0 },\n",
    "          petal_width: { min: 0.1, max: 2.5 }\n",
    "        };\n",
    "\n",
    "        const errors = [];\n",
    "        Object.entries(validationRules).forEach(([field, { min, max }]) => {\n",
    "          const value = parseFloat(inputData[field]);\n",
    "          if (isNaN(value)) {\n",
    "            errors.push(`${field} must be a number`);\n",
    "          } else if (value < min || value > max) {\n",
    "            errors.push(`${field} must be between ${min} and ${max}`);\n",
    "          }\n",
    "        });\n",
    "\n",
    "        if (errors.length > 0) {\n",
    "          alert(`Please fix the following errors:\\n${errors.join('\\n')}`);\n",
    "          return;\n",
    "        }\n",
    "\n",
    "        const payload = {\n",
    "          model_type: 'rf',\n",
    "          samples: [\n",
    "            {\n",
    "              sepal_length: parseFloat(inputData.sepal_length),\n",
    "              sepal_width: parseFloat(inputData.sepal_width),\n",
    "              petal_length: parseFloat(inputData.petal_length),\n",
    "              petal_width: parseFloat(inputData.petal_width)\n",
    "            }\n",
    "          ]\n",
    "        };\n",
    "        console.log('ðŸ” handlePredict DEBUG: Calling iris prediction with payload:', payload)\n",
    "        result = await callApi('/api/v1/iris/predict', payload)\n",
    "        console.log('ðŸ” handlePredict DEBUG: Iris prediction result:', result)\n",
    "        result.class_name = result.predicted_classes[0] || 'Unknown'\n",
    "        result.probability = 1\n",
    "        result.confidence = 1\n",
    "      } else {\n",
    "        const features = Object.values(inputData).map(val => parseFloat(val) || 0);\n",
    "        const values = features\n",
    "        const payload = { model_type: 'bayes', samples: [{ values }], posterior_samples: 100 }\n",
    "        console.log('ðŸ” handlePredict DEBUG: Calling cancer prediction with payload:', payload)\n",
    "        result = await callApi('/api/v1/cancer/predict', payload)\n",
    "        console.log('ðŸ” handlePredict DEBUG: Cancer prediction result:', result)\n",
    "        result.class_name = result.predicted_classes[0] || 'Unknown'\n",
=======
    "    try {\n",
    "      console.log('ðŸ” handlePredict DEBUG: Starting prediction for dataset:', selectedDataset)\n",
    "      const features = Object.values(inputData).map(val => parseFloat(val) || 0);\n",
    "      let result\n",
    "      if (selectedDataset === 'iris') {\n",
    "        const payload = {\n",
    "          rows: [\n",
    "            {\n",
    "              sepal_length: parseFloat(inputData.sepal_length) || 0,\n",
    "              sepal_width: parseFloat(inputData.sepal_width) || 0,\n",
    "              petal_length: parseFloat(inputData.petal_length) || 0,\n",
    "              petal_width: parseFloat(inputData.petal_width) || 0,\n",
    "            },\n",
    "          ],\n",
    "        }\n",
    "        console.log('ðŸ” handlePredict DEBUG: Calling iris prediction with payload:', payload)\n",
    "        result = await callApi('/iris/predict', payload)\n",
    "        console.log('ðŸ” handlePredict DEBUG: Iris prediction result:', result)\n",
    "        result.class_name = result.predictions[0] === 0 ? 'Setosa' : 'Non-Setosa'\n",
    "        result.probability = 1\n",
    "        result.confidence = 1\n",
    "      } else {\n",
    "        const values = features\n",
    "        const payload = { rows: [{ values }], posterior_samples: 100 }\n",
    "        console.log('ðŸ” handlePredict DEBUG: Calling cancer prediction with payload:', payload)\n",
    "        result = await callApi('/cancer/predict', payload)\n",
    "        console.log('ðŸ” handlePredict DEBUG: Cancer prediction result:', result)\n",
    "        result.class_name = result.predictions[0] > 0.5 ? 'Malignant' : 'Benign'\n",
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
    "        result.probability = result.predictions[0]\n",
    "        result.confidence = 1\n",
    "      }\n",
    "\n",
    "      setPredictionResults(result);\n",
    "      setPredictions(prev => [...prev, {\n",
    "        id: Date.now(),\n",
    "        input: { ...inputData },\n",
    "        result: result,\n",
    "        timestamp: new Date().toISOString()\n",
    "      }]);\n",
    "    } catch (error) {\n",
    "      console.error('âŒ handlePredict Error:', error);\n",
<<<<<<< HEAD
    "      if (error.response?.data?.detail?.valid_ranges) {\n",
    "        const ranges = error.response.data.detail.valid_ranges;\n",
    "        alert(`Invalid input ranges. Please use these ranges:\\n${\n",
    "          Object.entries(ranges)\n",
    "            .map(([field, range]) => `${field}: ${range}`)\n",
    "            .join('\\n')\n",
    "        }`);\n",
    "      } else {\n",
    "        alert(`Prediction failed: ${error.message}`);\n",
    "      }\n",
=======
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
    "    }\n",
    "  };\n",
    "\n",
    "  // Generate sample data for visualization\n",
    "  const generateSampleData = () => {\n",
    "    return Array.from({ length: 100 }, (_, i) => ({\n",
    "      x: Math.random() * 10,\n",
    "      y: Math.random() * 10,\n",
    "      class: Math.random() > 0.5 ? 'Class A' : 'Class B'\n",
    "    }));\n",
    "  };\n",
    "\n",
    "  const sampleData = generateSampleData();\n",
    "\n",
    "  // --- Background Bayesian retrain ---------------------------------------\n",
    "  const handleRetrainBayes = async () => {\n",
    "    setModelStatus('training');\n",
    "    try {\n",
    "      console.log('ðŸ” handleRetrainBayes DEBUG: Starting Bayesian retrain');\n",
    "\n",
    "      const result = await apiService.retrainCancer(trainingParams.cancer);\n",
    "      console.log('ðŸ” handleRetrainBayes DEBUG: Bayesian retrain started:', result);\n",
    "\n",
    "      // Poll for completion\n",
    "      try {\n",
    "        const newMetrics = await apiService.waitForMetrics('breast_cancer_bayes');\n",
    "        setModelMetrics(newMetrics);\n",
    "        console.log('ðŸ” handleRetrainBayes DEBUG: Retrain completed with metrics:', newMetrics);\n",
    "      } catch (error) {\n",
    "        console.warn('Retraining may still be in progress:', error.message);\n",
    "      }\n",
    "\n",
    "      setModelStatus('trained');\n",
    "    } catch (error) {\n",
    "      console.error('âŒ handleRetrainBayes Error:', error);\n",
    "      setModelStatus('error');\n",
    "      alert(`Retraining failed: ${error.message}`);\n",
    "    }\n",
    "  };\n",
    "\n",
    "  return (\n",
    "    <div className=\"min-h-screen bg-gray-50\">\n",
    "      {/* Header */}\n",
    "      <header className=\"bg-white shadow-sm border-b\">\n",
    "        <div className=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8\">\n",
    "          <div className=\"flex justify-between items-center py-4\">\n",
    "            <div className=\"flex items-center\">\n",
    "              <Brain className=\"h-8 w-8 text-blue-600 mr-3\" />\n",
    "              <h1 className=\"text-2xl font-bold text-gray-900\">ML Model Dashboard</h1>\n",
    "            </div>\n",
    "            <div className=\"flex items-center space-x-4\">\n",
    "              <div className=\"flex items-center\">\n",
    "                <div className={`w-3 h-3 rounded-full mr-2 ${\n",
    "                  apiHealth === 'healthy' ? 'bg-green-500' :\n",
    "                  apiHealth === 'error' ? 'bg-red-500' : 'bg-yellow-500'\n",
    "                }`} />\n",
    "                <span className=\"text-sm text-gray-600\">API Status: {apiHealth}</span>\n",
    "              </div>\n",
    "              <select\n",
    "                value={selectedDataset}\n",
    "                onChange={(e) => setSelectedDataset(e.target.value)}\n",
    "                className=\"px-3 py-2 border border-gray-300 rounded-md focus:outline-none focus:ring-2 focus:ring-blue-500\"\n",
    "              >\n",
    "                {Object.entries(datasets).map(([key, dataset]) => (\n",
    "                  <option key={key} value={key}>{dataset.name}</option>\n",
    "                ))}\n",
    "              </select>\n",
    "            </div>\n",
    "          </div>\n",
    "        </div>\n",
    "      </header>\n",
    "\n",
    "      {/* Navigation */}\n",
    "      <nav className=\"bg-white border-b\">\n",
    "        <div className=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8\">\n",
    "          <div className=\"flex space-x-8\">\n",
    "            {[\n",
    "              { id: 'dashboard', label: 'Dashboard', icon: Activity },\n",
    "              { id: 'training', label: 'Training', icon: Brain },\n",
    "              { id: 'prediction', label: 'Prediction', icon: TrendingUp },\n",
    "              { id: 'analysis', label: 'Analysis', icon: BarChart3 }\n",
    "            ].map(({ id, label, icon: Icon }) => (\n",
    "              <button\n",
    "                key={id}\n",
    "                onClick={() => setActiveTab(id)}\n",
    "                className={`flex items-center px-3 py-4 text-sm font-medium border-b-2 ${\n",
    "                  activeTab === id\n",
    "                    ? 'border-blue-500 text-blue-600'\n",
    "                    : 'border-transparent text-gray-500 hover:text-gray-700'\n",
    "                }`}\n",
    "              >\n",
    "                <Icon className=\"h-4 w-4 mr-2\" />\n",
    "                {label}\n",
    "              </button>\n",
    "            ))}\n",
    "          </div>\n",
    "        </div>\n",
    "      </nav>\n",
    "\n",
    "      {/* Main Content */}\n",
    "      <main className=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8\">\n",
    "        {/* Dashboard Tab */}\n",
    "        {activeTab === 'dashboard' && (\n",
    "          <div className=\"space-y-6\">\n",
    "            {/* Dataset Info Card */}\n",
    "            <div className=\"bg-white rounded-lg shadow p-6\">\n",
    "              <h2 className=\"text-lg font-semibold mb-4\">Current Dataset: {datasets[selectedDataset].name}</h2>\n",
    "              <p className=\"text-gray-600 mb-4\">{datasets[selectedDataset].description}</p>\n",
    "              <div className=\"grid grid-cols-1 md:grid-cols-3 gap-4\">\n",
    "                <div className=\"flex items-center\">\n",
    "                  <Database className=\"h-5 w-5 text-blue-500 mr-2\" />\n",
    "                  <span className=\"text-sm\">Features: {datasets[selectedDataset].features.length}</span>\n",
    "                </div>\n",
    "                <div className=\"flex items-center\">\n",
    "                  <Target className=\"h-5 w-5 text-green-500 mr-2\" />\n",
    "                  <span className=\"text-sm\">Classes: {datasets[selectedDataset].targetClasses.join(', ')}</span>\n",
    "                </div>\n",
    "                <div className=\"flex items-center\">\n",
    "                  <Settings className=\"h-5 w-5 text-purple-500 mr-2\" />\n",
    "                  <span className=\"text-sm\">Model: Bayesian LogReg</span>\n",
    "                </div>\n",
    "              </div>\n",
    "            </div>\n",
    "\n",
    "            {/* Quick Stats */}\n",
    "            <div className=\"grid grid-cols-1 md:grid-cols-4 gap-6\">\n",
    "              <div className=\"bg-white rounded-lg shadow p-6\">\n",
    "                <div className=\"flex items-center\">\n",
    "                  <CheckCircle className=\"h-8 w-8 text-green-500\" />\n",
    "                  <div className=\"ml-4\">\n",
    "                    <p className=\"text-sm text-gray-600\">Model Status</p>\n",
    "                    <p className=\"text-lg font-semibold capitalize\">{modelStatus}</p>\n",
    "                  </div>\n",
    "                </div>\n",
    "              </div>\n",
    "              <div className=\"bg-white rounded-lg shadow p-6\">\n",
    "                <div className=\"flex items-center\">\n",
    "                  <TrendingUp className=\"h-8 w-8 text-blue-500\" />\n",
    "                  <div className=\"ml-4\">\n",
    "                    <p className=\"text-sm text-gray-600\">Predictions Made</p>\n",
    "                    <p className=\"text-lg font-semibold\">{predictions.length}</p>\n",
    "                  </div>\n",
    "                </div>\n",
    "              </div>\n",
    "              <div className=\"bg-white rounded-lg shadow p-6\">\n",
    "                <div className=\"flex items-center\">\n",
    "                  <Activity className=\"h-8 w-8 text-purple-500\" />\n",
    "                  <div className=\"ml-4\">\n",
    "                    <p className=\"text-sm text-gray-600\">Training Runs</p>\n",
    "                    <p className=\"text-lg font-semibold\">{trainingHistory.length}</p>\n",
    "                  </div>\n",
    "                </div>\n",
    "              </div>\n",
    "              <div className=\"bg-white rounded-lg shadow p-6\">\n",
    "                <div className=\"flex items-center\">\n",
    "                  <BarChart3 className=\"h-8 w-8 text-orange-500\" />\n",
    "                  <div className=\"ml-4\">\n",
    "                    <p className=\"text-sm text-gray-600\">Accuracy</p>\n",
    "                    <p className=\"text-lg font-semibold\">\n",
    "                      {modelMetrics ? `${(modelMetrics.accuracy * 100).toFixed(1)}%` : 'N/A'}\n",
    "                    </p>\n",
    "                  </div>\n",
    "                </div>\n",
    "              </div>\n",
    "            </div>\n",
    "\n",
    "            {/* Recent Activity */}\n",
    "            <div className=\"bg-white rounded-lg shadow p-6\">\n",
    "              <h3 className=\"text-lg font-semibold mb-4\">Recent Activity</h3>\n",
    "              <div className=\"space-y-3\">\n",
    "                {[...trainingHistory, ...predictions].slice(-5).map((item, index) => (\n",
    "                  <div key={index} className=\"flex items-center justify-between py-2 border-b border-gray-100\">\n",
    "                    <div className=\"flex items-center\">\n",
    "                      {item.dataset ? (\n",
    "                        <Brain className=\"h-4 w-4 text-blue-500 mr-2\" />\n",
    "                      ) : (\n",
    "                        <TrendingUp className=\"h-4 w-4 text-green-500 mr-2\" />\n",
    "                      )}\n",
    "                      <span className=\"text-sm\">\n",
    "                        {item.dataset ? `Trained ${item.dataset} model` : 'Made prediction'}\n",
    "                      </span>\n",
    "                    </div>\n",
    "                    <span className=\"text-xs text-gray-500\">\n",
    "                      {new Date(item.timestamp).toLocaleTimeString()}\n",
    "                    </span>\n",
    "                  </div>\n",
    "                ))}\n",
    "              </div>\n",
    "            </div>\n",
    "          </div>\n",
    "        )}\n",
    "\n",
    "        {/* Training Tab */}\n",
    "        {activeTab === 'training' && (\n",
    "          <div className=\"space-y-6\">\n",
    "            <div className=\"bg-white rounded-lg shadow p-6\">\n",
    "              <h2 className=\"text-lg font-semibold mb-4\">Model Training</h2>\n",
    "\n",
    "              {/* Training Parameters */}\n",
    "              <div className=\"mb-6 p-4 bg-gray-50 rounded-lg\">\n",
    "                <h3 className=\"text-md font-medium mb-3\">Training Parameters</h3>\n",
    "\n",
    "                {selectedDataset === 'iris' && (\n",
    "                  <div className=\"space-y-3\">\n",
    "                    <div>\n",
    "                      <label className=\"block text-sm font-medium text-gray-700 mb-1\">\n",
    "                        Optuna Trials: {trainingParams.iris.n_trials}\n",
    "                      </label>\n",
    "                      <input\n",
    "                        type=\"range\"\n",
    "                        min=\"10\"\n",
    "                        max=\"200\"\n",
    "                        value={trainingParams.iris.n_trials}\n",
    "                        onChange={(e) => setTrainingParams(prev => ({\n",
    "                          ...prev,\n",
    "                          iris: { ...prev.iris, n_trials: parseInt(e.target.value) }\n",
    "                        }))}\n",
    "                        className=\"w-full\"\n",
    "                      />\n",
    "                      <div className=\"flex justify-between text-xs text-gray-500\">\n",
    "                        <span>10 (Fast)</span>\n",
    "                        <span>200 (Thorough)</span>\n",
    "                      </div>\n",
    "                    </div>\n",
    "                  </div>\n",
    "                )}\n",
    "\n",
    "                {selectedDataset === 'breast_cancer' && (\n",
    "                  <div className=\"grid grid-cols-1 md:grid-cols-3 gap-4\">\n",
    "                    <div>\n",
    "                      <label className=\"block text-sm font-medium text-gray-700 mb-1\">\n",
    "                        MCMC Draws: {trainingParams.cancer.draws}\n",
    "                      </label>\n",
    "                      <input\n",
    "                        type=\"range\"\n",
    "                        min=\"100\"\n",
    "                        max=\"2000\"\n",
    "                        step=\"100\"\n",
    "                        value={trainingParams.cancer.draws}\n",
    "                        onChange={(e) => setTrainingParams(prev => ({\n",
    "                          ...prev,\n",
    "                          cancer: { ...prev.cancer, draws: parseInt(e.target.value) }\n",
    "                        }))}\n",
    "                        className=\"w-full\"\n",
    "                      />\n",
    "                    </div>\n",
    "                    <div>\n",
    "                      <label className=\"block text-sm font-medium text-gray-700 mb-1\">\n",
    "                        Tune Steps: {trainingParams.cancer.tune}\n",
    "                      </label>\n",
    "                      <input\n",
    "                        type=\"range\"\n",
    "                        min=\"100\"\n",
    "                        max=\"1000\"\n",
    "                        step=\"100\"\n",
    "                        value={trainingParams.cancer.tune}\n",
    "                        onChange={(e) => setTrainingParams(prev => ({\n",
    "                          ...prev,\n",
    "                          cancer: { ...prev.cancer, tune: parseInt(e.target.value) }\n",
    "                        }))}\n",
    "                        className=\"w-full\"\n",
    "                      />\n",
    "                    </div>\n",
    "                    <div>\n",
    "                      <label className=\"block text-sm font-medium text-gray-700 mb-1\">\n",
    "                        Target Accept: {trainingParams.cancer.target_accept}\n",
    "                      </label>\n",
    "                      <input\n",
    "                        type=\"range\"\n",
    "                        min=\"0.7\"\n",
    "                        max=\"0.99\"\n",
    "                        step=\"0.01\"\n",
    "                        value={trainingParams.cancer.target_accept}\n",
    "                        onChange={(e) => setTrainingParams(prev => ({\n",
    "                          ...prev,\n",
    "                          cancer: { ...prev.cancer, target_accept: parseFloat(e.target.value) }\n",
    "                        }))}\n",
    "                        className=\"w-full\"\n",
    "                      />\n",
    "                    </div>\n",
    "                  </div>\n",
    "                )}\n",
    "              </div>\n",
    "\n",
    "              <div className=\"mb-6\">\n",
    "                <button\n",
    "                  onClick={handleTrainModel}\n",
    "                  disabled={modelStatus === 'training'}\n",
    "                  className={`flex items-center px-4 py-2 rounded-md ${\n",
    "                    modelStatus === 'training'\n",
    "                      ? 'bg-gray-400 cursor-not-allowed'\n",
    "                      : 'bg-blue-600 hover:bg-blue-700'\n",
    "                  } text-white`}\n",
    "                >\n",
    "                  <Play className=\"h-4 w-4 mr-2\" />\n",
    "                  {modelStatus === 'training' ? 'Training...' : 'Train Model'}\n",
    "                </button>\n",
    "\n",
    "                {/* NEW â€“ retrain Bayesian button */}\n",
    "                {selectedDataset === 'breast_cancer' && (\n",
    "                  <button\n",
    "                    onClick={handleRetrainBayes}\n",
    "                    disabled={modelStatus === 'training'}\n",
    "                    className={`ml-4 flex items-center px-4 py-2 rounded-md ${\n",
    "                      modelStatus === 'training'\n",
    "                        ? 'bg-gray-400 cursor-not-allowed'\n",
    "                        : 'bg-orange-600 hover:bg-orange-700'\n",
    "                    } text-white`}\n",
    "                  >\n",
    "                    <Upload className=\"h-4 w-4 mr-2\" />\n",
    "                    Retrain Cancer Model\n",
    "                  </button>\n",
    "                )}\n",
    "              </div>\n",
    "\n",
    "              {modelMetrics && (\n",
    "                <div className=\"grid grid-cols-2 md:grid-cols-4 gap-4 mb-6\">\n",
    "                  {Object.entries(modelMetrics).map(([metric, value]) => (\n",
    "                    <div key={metric} className=\"text-center p-4 bg-gray-50 rounded-lg\">\n",
    "                      <p className=\"text-sm text-gray-600 capitalize\">{metric.replace('_', ' ')}</p>\n",
    "                      <p className=\"text-lg font-semibold\">{(value * 100).toFixed(1)}%</p>\n",
    "                    </div>\n",
    "                  ))}\n",
    "                </div>\n",
    "              )}\n",
    "\n",
    "              {trainingHistory.length > 0 && (\n",
    "                <div className=\"h-64\">\n",
    "                  <h3 className=\"text-md font-semibold mb-2\">Training History</h3>\n",
    "                  <ResponsiveContainer width=\"100%\" height=\"100%\">\n",
    "                    <LineChart data={trainingHistory}>\n",
    "                      <CartesianGrid strokeDasharray=\"3 3\" />\n",
    "                      <XAxis dataKey=\"timestamp\" tickFormatter={(value) => new Date(value).toLocaleTimeString()} />\n",
    "                      <YAxis />\n",
    "                      <Tooltip labelFormatter={(value) => new Date(value).toLocaleString()} />\n",
    "                      <Legend />\n",
    "                      <Line type=\"monotone\" dataKey=\"accuracy\" stroke=\"#8884d8\" name=\"Accuracy\" />\n",
    "                    </LineChart>\n",
    "                  </ResponsiveContainer>\n",
    "                </div>\n",
    "              )}\n",
    "            </div>\n",
    "          </div>\n",
    "        )}\n",
    "\n",
    "        {/* Prediction Tab */}\n",
    "        {activeTab === 'prediction' && (\n",
    "          <div className=\"space-y-6\">\n",
    "            <div className=\"bg-white rounded-lg shadow p-6\">\n",
    "              <h2 className=\"text-lg font-semibold mb-4\">Make Prediction</h2>\n",
    "              <div className=\"grid grid-cols-1 md:grid-cols-2 gap-4 mb-6\">\n",
    "                {datasets[selectedDataset].features.map((feature, index) => (\n",
    "                  <div key={feature}>\n",
    "                    <label className=\"block text-sm font-medium text-gray-700 mb-1\">\n",
    "                      {datasets[selectedDataset].featureLabels[index]}\n",
    "                    </label>\n",
    "                    <input\n",
    "                      type=\"number\"\n",
    "                      step=\"0.1\"\n",
    "                      value={inputData[feature] || ''}\n",
    "                      onChange={(e) => setInputData(prev => ({ ...prev, [feature]: e.target.value }))}\n",
    "                      className=\"w-full px-3 py-2 border border-gray-300 rounded-md focus:outline-none focus:ring-2 focus:ring-blue-500\"\n",
    "                      placeholder={`Enter ${feature}`}\n",
    "                    />\n",
    "                  </div>\n",
    "                ))}\n",
    "              </div>\n",
    "              <button\n",
    "                onClick={handlePredict}\n",
    "                className=\"flex items-center px-4 py-2 bg-green-600 hover:bg-green-700 text-white rounded-md\"\n",
    "              >\n",
    "                <TrendingUp className=\"h-4 w-4 mr-2\" />\n",
    "                Predict\n",
    "              </button>\n",
    "\n",
    "              {predictionResults && (\n",
    "                <div className=\"mt-6 p-4 bg-blue-50 rounded-lg\">\n",
    "                  <h3 className=\"font-semibold mb-2\">Prediction Results</h3>\n",
    "                  <div className=\"grid grid-cols-1 md:grid-cols-3 gap-4\">\n",
    "                    <div>\n",
    "                      <p className=\"text-sm text-gray-600\">Predicted Class</p>\n",
    "                      <p className=\"text-lg font-semibold\">{predictionResults.class_name}</p>\n",
    "                    </div>\n",
    "                    <div>\n",
    "                      <p className=\"text-sm text-gray-600\">Probability</p>\n",
    "                      <p className=\"text-lg font-semibold\">{(predictionResults.probability * 100).toFixed(1)}%</p>\n",
    "                    </div>\n",
    "                    <div>\n",
    "                      <p className=\"text-sm text-gray-600\">Confidence</p>\n",
    "                      <p className=\"text-lg font-semibold\">{(predictionResults.confidence * 100).toFixed(1)}%</p>\n",
    "                    </div>\n",
    "                  </div>\n",
    "                </div>\n",
    "              )}\n",
    "            </div>\n",
    "\n",
    "            {predictions.length > 0 && (\n",
    "              <div className=\"bg-white rounded-lg shadow p-6\">\n",
    "                <h3 className=\"text-lg font-semibold mb-4\">Prediction History</h3>\n",
    "                <div className=\"overflow-x-auto\">\n",
    "                  <table className=\"min-w-full table-auto\">\n",
    "                    <thead>\n",
    "                      <tr className=\"bg-gray-50\">\n",
    "                        <th className=\"px-4 py-2 text-left text-sm font-medium text-gray-700\">Timestamp</th>\n",
    "                        <th className=\"px-4 py-2 text-left text-sm font-medium text-gray-700\">Prediction</th>\n",
    "                        <th className=\"px-4 py-2 text-left text-sm font-medium text-gray-700\">Confidence</th>\n",
    "                      </tr>\n",
    "                    </thead>\n",
    "                    <tbody>\n",
    "                      {predictions.slice(-10).map((pred) => (\n",
    "                        <tr key={pred.id} className=\"border-b border-gray-200\">\n",
    "                          <td className=\"px-4 py-2 text-sm text-gray-600\">\n",
    "                            {new Date(pred.timestamp).toLocaleString()}\n",
    "                          </td>\n",
    "                          <td className=\"px-4 py-2 text-sm font-medium\">\n",
    "                            {pred.result.class_name}\n",
    "                          </td>\n",
    "                          <td className=\"px-4 py-2 text-sm\">\n",
    "                            {(pred.result.confidence * 100).toFixed(1)}%\n",
    "                          </td>\n",
    "                        </tr>\n",
    "                      ))}\n",
    "                    </tbody>\n",
    "                  </table>\n",
    "                </div>\n",
    "              </div>\n",
    "            )}\n",
    "          </div>\n",
    "        )}\n",
    "\n",
    "        {/* Analysis Tab */}\n",
    "        {activeTab === 'analysis' && (\n",
    "          <div className=\"space-y-6\">\n",
    "            <div className=\"grid grid-cols-1 lg:grid-cols-2 gap-6\">\n",
    "              <div className=\"bg-white rounded-lg shadow p-6\">\n",
    "                <h3 className=\"text-lg font-semibold mb-4\">Feature Distribution</h3>\n",
    "                <div className=\"h-64\">\n",
    "                  <ResponsiveContainer width=\"100%\" height=\"100%\">\n",
    "                    <BarChart data={datasets[selectedDataset].features.map((feature, index) => ({\n",
    "                      name: feature,\n",
    "                      value: Math.random() * 100,\n",
    "                      fill: `hsl(${index * 60}, 70%, 50%)`\n",
    "                    }))}>\n",
    "                      <CartesianGrid strokeDasharray=\"3 3\" />\n",
    "                      <XAxis dataKey=\"name\" angle={-45} textAnchor=\"end\" height={100} />\n",
    "                      <YAxis />\n",
    "                      <Tooltip />\n",
    "                      <Bar dataKey=\"value\" />\n",
    "                    </BarChart>\n",
    "                  </ResponsiveContainer>\n",
    "                </div>\n",
    "              </div>\n",
    "\n",
    "              <div className=\"bg-white rounded-lg shadow p-6\">\n",
    "                <h3 className=\"text-lg font-semibold mb-4\">Class Distribution</h3>\n",
    "                <div className=\"h-64\">\n",
    "                  <ResponsiveContainer width=\"100%\" height=\"100%\">\n",
    "                    <PieChart>\n",
    "                      <Pie\n",
    "                        data={datasets[selectedDataset].targetClasses.map((className, index) => ({\n",
    "                          name: className,\n",
    "                          value: 50 + Math.random() * 50,\n",
    "                          fill: ['#8884d8', '#82ca9d', '#ffc658', '#ff7300'][index % 4]\n",
    "                        }))}\n",
    "                        cx=\"50%\"\n",
    "                        cy=\"50%\"\n",
    "                        labelLine={false}\n",
    "                        label={({ name, percent }) => `${name}: ${(percent * 100).toFixed(0)}%`}\n",
    "                        outerRadius={80}\n",
    "                        fill=\"#8884d8\"\n",
    "                        dataKey=\"value\"\n",
    "                      >\n",
    "                        {datasets[selectedDataset].targetClasses.map((entry, index) => (\n",
    "                          <Cell key={`cell-${index}`} fill={['#8884d8', '#82ca9d', '#ffc658', '#ff7300'][index % 4]} />\n",
    "                        ))}\n",
    "                      </Pie>\n",
    "                      <Tooltip />\n",
    "                    </PieChart>\n",
    "                  </ResponsiveContainer>\n",
    "                </div>\n",
    "              </div>\n",
    "            </div>\n",
    "\n",
    "            <div className=\"bg-white rounded-lg shadow p-6\">\n",
    "              <h3 className=\"text-lg font-semibold mb-4\">Data Visualization</h3>\n",
    "              <div className=\"h-64\">\n",
    "                <ResponsiveContainer width=\"100%\" height=\"100%\">\n",
    "                  <ScatterChart data={sampleData}>\n",
    "                    <CartesianGrid strokeDasharray=\"3 3\" />\n",
    "                    <XAxis type=\"number\" dataKey=\"x\" />\n",
    "                    <YAxis type=\"number\" dataKey=\"y\" />\n",
    "                    <Tooltip cursor={{ strokeDasharray: '3 3' }} />\n",
    "                    <Scatter\n",
    "                      name=\"Class A\"\n",
    "                      data={sampleData.filter(d => d.class === 'Class A')}\n",
    "                      fill=\"#8884d8\"\n",
    "                    />\n",
    "                    <Scatter\n",
    "                      name=\"Class B\"\n",
    "                      data={sampleData.filter(d => d.class === 'Class B')}\n",
    "                      fill=\"#82ca9d\"\n",
    "                    />\n",
    "                    <Legend />\n",
    "                  </ScatterChart>\n",
    "                </ResponsiveContainer>\n",
    "              </div>\n",
    "            </div>\n",
    "          </div>\n",
    "        )}\n",
    "      </main>\n",
    "    </div>\n",
    "  );\n",
    "};\n",
    "\n",
<<<<<<< HEAD
    "export default MLModelFrontend;\n",
    "\n"
=======
    "export default MLModelFrontend;\n"
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 269,
=======
   "execution_count": 65,
>>>>>>> 6249ca8e2e33f82548eb02009a16df6446138120
   "id": "42be02d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting frontend/src/main.jsx\n"
     ]
    }
   ],
   "source": [
    "%%writefile frontend/src/main.jsx\n",
    "import React from 'react'\n",
    "import ReactDOM from 'react-dom/client'\n",
    "import App from './App.jsx'\n",
    "import './index.css'\n",
    "\n",
    "ReactDOM.createRoot(document.getElementById('root')).render(\n",
    "  <React.StrictMode>\n",
    "    <App />\n",
    "  </React.StrictMode>,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
