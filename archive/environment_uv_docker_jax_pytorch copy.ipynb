{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensure no other containers are running for dev container, if they are stop and remove them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .devcontainer/.dockerignore\n"
     ]
    }
   ],
   "source": [
    "%%writefile .devcontainer/.dockerignore\n",
    "**/.git\n",
    "**/.vscode\n",
    "**/.idea\n",
    "**/__pycache__\n",
    "**/*.pyc\n",
    "\n",
    "**/*.pyo\n",
    "**/*.pyd\n",
    "**/*.swp\n",
    "**/.venv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .dockerignore\n"
     ]
    }
   ],
   "source": [
    "%%writefile .dockerignore\n",
    "**/.git\n",
    "**/.vscode\n",
    "**/.idea\n",
    "**/__pycache__\n",
    "**/*.pyc\n",
    "**/*.pyo\n",
    "**/*.pyd\n",
    "**/*.swp\n",
    "**/venv\n",
    "**/env\n",
    ".env\n",
    "*.code-workspace\n",
    "data/\n",
    "notebooks/**/*.ipynb_checkpoints\n",
    "*.log\n",
    ".DS_Store\n",
    "Thumbs.db "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .env.template\n"
     ]
    }
   ],
   "source": [
    "%%writefile .env.template \n",
    "ENV_NAME=docker_dev_template \n",
    "CUDA_TAG=12.8.0          # default; override via invoke up --cuda-tag …\n",
    "\n",
    "# Fixed ports you actually care about\n",
    "HOST_JUPYTER_PORT=8890\n",
    "\n",
    "# Leave blank → Docker picks a free host port\n",
    "HOST_TENSORBOARD_PORT=\n",
    "HOST_EXPLAINER_PORT=\n",
    "HOST_STREAMLIT_PORT=\n",
    "\n",
    "# JAX/GPU Configuration\n",
    "PYTHON_VER=3.10\n",
    "JAX_PLATFORM_NAME=gpu\n",
    "XLA_PYTHON_CLIENT_PREALLOCATE=true\n",
    "XLA_PYTHON_CLIENT_ALLOCATOR=platform\n",
    "XLA_PYTHON_CLIENT_MEM_FRACTION=0.95\n",
    "XLA_FLAGS=--xla_force_host_platform_device_count=1\n",
    "JAX_DISABLE_JIT=false\n",
    "JAX_ENABLE_X64=false\n",
    "TF_FORCE_GPU_ALLOW_GROWTH=false\n",
    "JAX_PREALLOCATION_SIZE_LIMIT_BYTES=8589934592\n",
    "\n",
    "# Code Executor\n",
    "CODE_STORAGE_DIR=code_executor_storage\n",
    "ENV_NAME=docker_dev_template\n",
    "\n",
    "# Snowflake\n",
    "SNOWFLAKE_ACCOUNT=your_account\n",
    "SNOWFLAKE_USER=your_user\n",
    "SNOWFLAKE_PASSWORD=your_password\n",
    "SNOWFLAKE_ROLE=your_role\n",
    "SNOWFLAKE_WAREHOUSE=your_warehouse\n",
    "SNOWFLAKE_DATABASE=your_database\n",
    "SNOWFLAKE_SCHEMA=your_schema\n",
    "\n",
    "# Jupyter\n",
    "JUPYTER_URL=http://host.docker.internal:8890\n",
    "JUPYTER_TOKEN=insert_token         # must match token used in jupyter lab command\n",
    "NOTEBOOK_PATH=notebooks/demo.ipynb\n",
    "# OracleDB\n",
    "ORACLE_CONNECTION_STRING=username/password@//host:port/service\n",
    "TARGET_SCHEMA=your_schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .devcontainer/devcontainer.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile .devcontainer/devcontainer.json\n",
    "{\n",
    "  \"name\": \"docker_dev_template_uv\",\n",
    "  \"dockerComposeFile\": \"../docker-compose.yml\",\n",
    "  \"service\": \"datascience\",\n",
    "  \"workspaceFolder\": \"/workspace\",\n",
    "  \"shutdownAction\": \"stopCompose\",\n",
    "  \"runArgs\": [\n",
    "    \"--gpus\", \"all\",\n",
    "    \"--env-file\", \".devcontainer/devcontainer.env\"\n",
    "  ],\n",
    "  \"customizations\": {\n",
    "    \"vscode\": {\n",
    "      \"settings\": {\n",
    "        \"python.defaultInterpreterPath\": \"/app/.venv/bin/python\",\n",
    "        \"python.pythonPath\": \"/app/.venv/bin/python\"\n",
    "      },\n",
    "      \"extensions\": [\n",
    "        \"ms-python.python\",\n",
    "        \"ms-toolsai.jupyter\",\n",
    "        \"GitHub.copilot\",\n",
    "        \"ms-azuretools.vscode-docker\"\n",
    "      ]\n",
    "    }\n",
    "  },\n",
    "  \"remoteEnv\": {\n",
    "      \"MY_VAR\": \"${localEnv:MY_VAR:test_var}\",\n",
    "      \"UV_PROJECT_ENVIRONMENT\": \"/app/.venv\"\n",
    "  },\n",
    "  \"overrideCommand\": false,\n",
    "  \"postCreateCommand\": [\n",
    "    \"bash\", \"-c\", \n",
    "    \"set -euo pipefail && echo '## uv diagnostics ##' && uv sync --active && echo 'Dependencies synced to /app/.venv ✔' && uv --version && echo '## python ##' && which python && python -V && python -c 'import encodings, sys; print(\\\"🟢 encodings OK\\\", sys.executable)' && python -c 'import jupyterlab; print(\\\"🟢 jupyterlab OK\\\")' && python -c 'import torch; print(\\\"🟢 torch\\\", torch.__version__, \\\"CUDA:\\\", torch.cuda.is_available())' && python -c 'import jax; print(\\\"🟢 jax\\\", jax.__version__, \\\"devices:\\\", jax.devices())' && echo '🎉 All imports successful!'\"\n",
    "  ]\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .devcontainer/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile .devcontainer/Dockerfile\n",
    "# .devcontainer/Dockerfile — uv‑based replacement for the previous Conda image\n",
    "# -----------------------------------------------------------------------------\n",
    "# CUDA + cuDNN base with drivers already installed --------------------------------\n",
    "ARG CUDA_TAG=12.8.0              # <── single source of truth\n",
    "FROM nvidia/cuda:${CUDA_TAG}-cudnn-devel-ubuntu22.04\n",
    "\n",
    "# ---------- build-time ARGs ---------------------------------------------------\n",
    "ARG PYTHON_VER=3.10\n",
    "ARG ENV_NAME=docker_dev_template\n",
    "ARG JAX_PREALLOCATE=true\n",
    "ARG JAX_MEM_FRAC=0.95\n",
    "ARG JAX_ALLOCATOR=platform\n",
    "ARG JAX_PREALLOC_LIMIT=8589934592\n",
    "ENV DEBIAN_FRONTEND=noninteractive\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 1) Core OS deps, build tools, & Python (system) -----------------------------\n",
    "RUN --mount=type=cache,target=/var/cache/apt \\\n",
    "    --mount=type=cache,target=/var/lib/apt \\\n",
    "    apt-get update && apt-get install -y --no-install-recommends \\\n",
    "        curl ca-certificates git procps htop util-linux build-essential \\\n",
    "        python3 python3-venv python3-pip python3-dev \\\n",
    "        autoconf automake libtool m4 cmake pkg-config \\\n",
    "        jags \\\n",
    "        && pkg-config --modversion jags \\\n",
    "        && apt-get clean && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 2) Copy a *pinned* uv & uvx binary pair from the official distroless image --\n",
    "COPY --from=ghcr.io/astral-sh/uv:0.7.12 /uv /uvx /bin/\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 3) Create project dir & copy only the lock/manifest for best layer‑caching --\n",
    "WORKDIR /app\n",
    "COPY pyproject.toml uv.lock* ./\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 4) Create an in‑project venv & sync dependencies (no project‑source yet) ----\n",
    "RUN --mount=type=cache,target=/root/.cache/uv \\\n",
    "    uv venv .venv --python \"${PYTHON_VER}\" --prompt \"${ENV_NAME}\" && \\\n",
    "    # If the lockfile is stale, regenerate it & continue – keeps CI green\n",
    "    (uv sync --locked || (echo \"⚠️  Lock drift detected – regenerating\" \\\n",
    "        && uv lock --upgrade --quiet && uv sync))\n",
    "\n",
    "# Promote venv for all later layers ------------------------------------------------\n",
    "ENV VIRTUAL_ENV=/app/.venv\n",
    "ENV PATH=\"/app/.venv/bin:${PATH}\"\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 5) ---------- CUDA wheels -------------------------------------------------------\n",
    "RUN --mount=type=cache,target=/root/.cache/uv \\\n",
    "    uv pip install --pre --no-cache-dir \\\n",
    "        torch torchvision torchaudio \\\n",
    "        --index-url https://download.pytorch.org/whl/nightly/cu128 && \\\n",
    "    uv pip install --no-cache-dir \\\n",
    "        \"jax[cuda12]==0.6.0\" \\\n",
    "        -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
    "\n",
    "# --- CUDA toolkit sanity check (robust for runtime *and* devel images) ------\n",
    "RUN set -e; \\\n",
    "    # 1️⃣ First try: any cuda-<ver> folder?\n",
    "    CUDA_REAL=\"$(ls -d /usr/local/cuda-* 2>/dev/null | sort -V | tail -n1 || true)\"; \\\n",
    "    # 2️⃣ Fallback: flat layout shipped by some runtime images\n",
    "    if [ -z \"$CUDA_REAL\" ] && [ -d /usr/local/cuda ]; then \\\n",
    "        CUDA_REAL=\"/usr/local/cuda\"; \\\n",
    "    fi; \\\n",
    "    # 3️⃣ Bail if still empty\n",
    "    if [ -z \"$CUDA_REAL\" ]; then \\\n",
    "        echo '❌  No CUDA toolkit folder found — aborting.' >&2; exit 1; \\\n",
    "    fi; \\\n",
    "    # 4️⃣ Refresh the canonical symlink only when needed\n",
    "    if [ \"$CUDA_REAL\" != \"/usr/local/cuda\" ]; then \\\n",
    "        echo \"🔧  Linking /usr/local/cuda -> $CUDA_REAL\"; \\\n",
    "        ln -sfn \"$CUDA_REAL\" /usr/local/cuda; \\\n",
    "    fi; \\\n",
    "    echo \"🟢  CUDA toolkit detected at $CUDA_REAL\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 6) Install PyJAGS with the cstdint header work‑around -----------------------\n",
    "RUN CPPFLAGS=\"-include cstdint\" uv pip install --no-build-isolation pyjags==1.3.8\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 7) Copy *rest* of the project after deps → fast rebuild when code changes ---\n",
    "COPY . /app\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 8) GPU‑tuning env vars (carried forward from Conda‑based image) -------------\n",
    "ENV XLA_PYTHON_CLIENT_PREALLOCATE=${JAX_PREALLOCATE}\n",
    "ENV XLA_PYTHON_CLIENT_MEM_FRACTION=${JAX_MEM_FRAC}\n",
    "ENV XLA_PYTHON_CLIENT_ALLOCATOR=${JAX_ALLOCATOR}\n",
    "ENV JAX_PLATFORM_NAME=gpu\n",
    "ENV XLA_FLAGS=\"--xla_force_host_platform_device_count=1\"\n",
    "ENV JAX_DISABLE_JIT=false\n",
    "ENV JAX_ENABLE_X64=false\n",
    "ENV TF_FORCE_GPU_ALLOW_GROWTH=false\n",
    "ENV JAX_PREALLOCATION_SIZE_LIMIT_BYTES=${JAX_PREALLOC_LIMIT}\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 9) Library path so PyJAGS & CUDA libs resolve correctly ---------------------\n",
    "ENV LD_LIBRARY_PATH=\"/app/.venv/lib:${LD_LIBRARY_PATH}\"\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 10) Final working directory & default command ------------------------------\n",
    "WORKDIR /workspace\n",
    "CMD [\"bash\"]\n",
    "\n",
    "# 11) Force login shells & VS Code terminals to land in /workspace\n",
    "RUN echo 'cd /workspace' > /etc/profile.d/99-workspace-cd.sh\n",
    "\n",
    "# 12) Force every IPython / Jupyter kernel to start in /workspace\n",
    "RUN mkdir -p /root/.ipython/profile_default/startup && \\\n",
    "    printf \"import os, sys\\nos.chdir('/workspace')\\nsys.path.append('/workspace')\\n\" \\\n",
    "      > /root/.ipython/profile_default/startup/00-cd-workspace.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .devcontainer/gpu_verify.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile .devcontainer/gpu_verify.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Fail-fast GPU sanity check for both PyTorch and JAX.\n",
    "Exit code 0 = OK, 1 = warning (GPU absent), 2 = hard failure.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import platform\n",
    "import sys\n",
    "\n",
    "\n",
    "def _torch_check():\n",
    "    import torch\n",
    "    ok = torch.cuda.is_available()\n",
    "    info = {\n",
    "        \"torch\": torch.__version__,\n",
    "        \"cuda_available\": ok,\n",
    "        \"cuda_devices\": torch.cuda.device_count() if ok else 0,\n",
    "        \"capabilities\": platform.platform(),\n",
    "    }\n",
    "    return ok, info\n",
    "\n",
    "\n",
    "def _jax_check():\n",
    "    import jax\n",
    "    try:\n",
    "        devs = jax.devices()\n",
    "        return any(d.platform == \"gpu\" for d in devs), [str(d) for d in devs]\n",
    "    except Exception as e:\n",
    "        return False, str(e)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    t_ok, t_info = _torch_check()\n",
    "    j_ok, j_info = _jax_check()\n",
    "\n",
    "    print(\"TORCH:\", json.dumps(t_info, indent=2))\n",
    "    print(\"JAX :\", json.dumps(j_info, indent=2))\n",
    "\n",
    "    if t_ok and j_ok:\n",
    "        sys.exit(0)\n",
    "    elif t_ok or j_ok:\n",
    "        sys.exit(1)\n",
    "    else:\n",
    "        sys.exit(2) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .devcontainer/jags_verify.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile .devcontainer/jags_verify.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Verify that PyJAGS is correctly installed and working.\n",
    "This script is used by the Docker container health check.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "try:\n",
    "    import pyjags\n",
    "    print(f\"PyJAGS version: {pyjags.__version__}\")\n",
    "    \n",
    "    # Create a simple model to verify that PyJAGS works\n",
    "    code = \"\"\"\n",
    "    model {\n",
    "        # Likelihood\n",
    "        y ~ dnorm(mu, 1/sigma^2)\n",
    "        \n",
    "        # Priors\n",
    "        mu ~ dnorm(0, 0.001)\n",
    "        sigma ~ dunif(0, 100)\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sample data\n",
    "    data = {'y': 0.5}\n",
    "    \n",
    "    # Initialize model with data\n",
    "    model = pyjags.Model(code, data=data, chains=1, adapt=100)\n",
    "    print(\"JAGS model initialized successfully!\")\n",
    "    \n",
    "    # Sample from the model\n",
    "    samples = model.sample(200, vars=['mu', 'sigma'])\n",
    "    print(\"JAGS sampling completed successfully!\")\n",
    "    \n",
    "    # Verify the samples\n",
    "    mu_samples = samples['mu']\n",
    "    sigma_samples = samples['sigma']\n",
    "    print(f\"mu mean: {mu_samples.mean():.4f}\")\n",
    "    print(f\"sigma mean: {sigma_samples.mean():.4f}\")\n",
    "    \n",
    "    print(\"PyJAGS verification completed successfully!\")\n",
    "    sys.exit(0)\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"PyJAGS not found!\")\n",
    "    sys.exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"Error during PyJAGS verification: {e}\")\n",
    "    sys.exit(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .devcontainer/pyjags_patch.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile .devcontainer/pyjags_patch.py\n",
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def patch_pyjags_sources():\n",
    "    print(\"Downloading and patching PyJAGS source...\")\n",
    "    os.system(\"pip download --no-binary :all: pyjags==1.3.8\")\n",
    "    os.system(\"tar -xzf pyjags-1.3.8.tar.gz\")\n",
    "    os.chdir(\"pyjags-1.3.8\")\n",
    "    \n",
    "    # Add cstdint include to all cpp files\n",
    "    for root, dirs, files in os.walk(\"src\"):\n",
    "        for file in files:\n",
    "            if file.endswith(\".cpp\") or file.endswith(\".h\"):\n",
    "                filepath = os.path.join(root, file)\n",
    "                with open(filepath, 'r') as f:\n",
    "                    content = f.read()\n",
    "                if \"#include <cstdint>\" not in content:\n",
    "                    with open(filepath, 'w') as f:\n",
    "                        f.write(\"#include <cstdint>\\n\" + content)\n",
    "                    print(f\"Patched {filepath}\")\n",
    "    \n",
    "    # Build and install\n",
    "    os.system(\"pip install --no-build-isolation .\")\n",
    "    print(\"PyJAGS installation complete!\")\n",
    "    return 0\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sys.exit(patch_pyjags_sources()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .pre-commit-config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile .pre-commit-config.yaml\n",
    "repos:\n",
    "  - repo: https://github.com/astral-sh/uv-pre-commit\n",
    "    rev: 0.5.7  # Use the ref you want to point at\n",
    "    hooks:\n",
    "      - id: uv-lock\n",
    "      - id: uv-export\n",
    "      - id: uv-lock\n",
    "        stages: [push]\n",
    "        args: [\"--check\"]\n",
    "  - repo: https://github.com/pre-commit/pre-commit-hooks\n",
    "    rev: v4.6.0\n",
    "    hooks:\n",
    "      - id: trailing-whitespace\n",
    "      - id: end-of-file-fixer\n",
    "      - id: check-yaml\n",
    "      - id: check-added-large-files \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting docker-compose.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile docker-compose.yml\n",
    "# docker-compose.yml\n",
    "name: ${ENV_NAME:-docker_dev_template}\n",
    "\n",
    "services:\n",
    "  datascience:\n",
    "    build:\n",
    "      context: .\n",
    "      dockerfile: .devcontainer/Dockerfile\n",
    "      args:\n",
    "        PYTHON_VER: ${PYTHON_VER:-3.10}\n",
    "        ENV_NAME: ${ENV_NAME:-docker_dev_template}\n",
    "        JAX_PREALLOCATE: ${XLA_PYTHON_CLIENT_PREALLOCATE:-true}\n",
    "        JAX_MEM_FRAC: ${XLA_PYTHON_CLIENT_MEM_FRACTION:-0.95}\n",
    "        JAX_ALLOCATOR: ${XLA_PYTHON_CLIENT_ALLOCATOR:-platform}\n",
    "        JAX_PREALLOC_LIMIT: ${JAX_PREALLOCATION_SIZE_LIMIT_BYTES:-8589934592}\n",
    "\n",
    "    # (Removed explicit container_name to avoid \"already in use\" conflicts.)\n",
    "\n",
    "    # Enhanced restart policy to handle port conflicts\n",
    "    restart: unless-stopped\n",
    "\n",
    "    gpus: all\n",
    "\n",
    "    env_file:\n",
    "      - .env.template     # acts as the \"defaults\" layer\n",
    "\n",
    "    environment:\n",
    "      - PYTHON_VER=${PYTHON_VER}\n",
    "      - NVIDIA_VISIBLE_DEVICES=all\n",
    "      - NVIDIA_DRIVER_CAPABILITIES=compute,utility,graphics,display\n",
    "      - JAX_PLATFORM_NAME=${JAX_PLATFORM_NAME}\n",
    "      - XLA_PYTHON_CLIENT_PREALLOCATE=${XLA_PYTHON_CLIENT_PREALLOCATE}\n",
    "      - XLA_PYTHON_CLIENT_ALLOCATOR=${XLA_PYTHON_CLIENT_ALLOCATOR}\n",
    "      - XLA_PYTHON_CLIENT_MEM_FRACTION=${XLA_PYTHON_CLIENT_MEM_FRACTION}\n",
    "      - XLA_FLAGS=${XLA_FLAGS}\n",
    "      - JAX_DISABLE_JIT=${JAX_DISABLE_JIT}\n",
    "      - JAX_ENABLE_X64=${JAX_ENABLE_X64}\n",
    "      - TF_FORCE_GPU_ALLOW_GROWTH=${TF_FORCE_GPU_ALLOW_GROWTH}\n",
    "      - JAX_PREALLOCATION_SIZE_LIMIT_BYTES=${JAX_PREALLOCATION_SIZE_LIMIT_BYTES}\n",
    "\n",
    "    volumes:\n",
    "      - .:/workspace\n",
    "\n",
    "    ports:\n",
    "      # Enhanced port configuration with fallback options\n",
    "      - \"${HOST_JUPYTER_PORT:-8890}:8888\"\n",
    "      - \"${HOST_TENSORBOARD_PORT:-}:6008\"\n",
    "      - \"${HOST_EXPLAINER_PORT:-}:8050\"\n",
    "      - \"${HOST_STREAMLIT_PORT:-}:8501\"\n",
    "\n",
    "    # Add debugging and conflict prevention\n",
    "    command: >\n",
    "      bash -c \"\n",
    "      echo '=== Docker Dev Template Container Starting ===' &&\n",
    "      echo 'Checking port availability...' &&\n",
    "      if netstat -tulpn 2>/dev/null | grep -q :8888; then\n",
    "        echo 'WARNING: Port 8888 is already in use inside container!'\n",
    "      fi &&\n",
    "      cd /workspace &&\n",
    "      echo 'Python version:' &&\n",
    "      python -c \\\"import jax; print('JAX version:', jax.__version__)\\\" &&\n",
    "      echo \\\"Jupyter will be available at: http://localhost:${HOST_JUPYTER_PORT:-8890}\\\" &&\n",
    "      echo \\\"TensorBoard mapped to \\$(hostname -i):6008 (host port auto-assigned)\\\" &&\n",
    "      echo 'Container ready for dev work. Ports configured:' &&\n",
    "      echo '  - Jupyter: ${HOST_JUPYTER_PORT:-8890} -> 8888' &&\n",
    "      echo '  - TensorBoard: ${HOST_TENSORBOARD_PORT:-auto} -> 6008' &&\n",
    "      echo '  - Explainer: ${HOST_EXPLAINER_PORT:-auto} -> 8050' &&\n",
    "      echo '  - Streamlit: ${HOST_STREAMLIT_PORT:-auto} -> 8501' &&\n",
    "      echo 'To prevent port conflicts, modify HOST_*_PORT variables in dev.env' &&\n",
    "      tail -f /dev/null\n",
    "      \"\n",
    "\n",
    "    healthcheck:\n",
    "      test: [\"CMD-SHELL\",\n",
    "             \"python /app/.devcontainer/jags_verify.py \\\n",
    "              && curl -f http://localhost:8888\"]\n",
    "      interval: 30s\n",
    "      timeout: 10s\n",
    "      retries: 3\n",
    "      # pick the larger of your two start_periods so both get a fair chance\n",
    "      start_period: 60s\n",
    "\n",
    "    labels:\n",
    "      - \"com.docker.compose.project=${ENV_NAME:-docker_dev_template}\"\n",
    "      - \"com.docker.compose.service=datascience\"\n",
    "      - \"description=AI/ML Development Environment with GPU Support\"\n",
    "\n",
    "    depends_on:\n",
    "      mlflow:\n",
    "        condition: service_healthy\n",
    "\n",
    "  mlflow:\n",
    "    image: ghcr.io/mlflow/mlflow:latest\n",
    "    command: >\n",
    "      mlflow server\n",
    "      --host 0.0.0.0\n",
    "      --port 5000\n",
    "      --backend-store-uri sqlite:///mlflow.db\n",
    "      --default-artifact-root /mlflow_artifacts\n",
    "    environment:\n",
    "      MLFLOW_EXPERIMENTS_DEFAULT_ARTIFACT_LOCATION: /mlflow_artifacts\n",
    "    volumes:\n",
    "      - ./mlruns:/mlflow_artifacts\n",
    "      - ./mlflow_db:/mlflow_db\n",
    "    ports:\n",
    "      - \"${HOST_MLFLOW_PORT:-5000}:5000\"\n",
    "    restart: unless-stopped\n",
    "    healthcheck:\n",
    "      test: [\"CMD-SHELL\", \"python -c \\\"import requests; requests.get('http://localhost:5000/health').raise_for_status()\\\"\"]\n",
    "      interval: 10s\n",
    "      timeout: 3s\n",
    "      retries: 5\n",
    "      start_period: 30s\n",
    "\n",
    "volumes:\n",
    "  data:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pyproject.toml\n"
     ]
    }
   ],
   "source": [
    "%%writefile pyproject.toml\n",
    "[project]\n",
    "name = \"docker_dev_template\"\n",
    "version = \"0.1.0\"\n",
    "description = \"Pytorch and Jax GPU docker container\"\n",
    "authors = [\n",
    "  { name = \"Geoffrey Hadfield\" },\n",
    "]\n",
    "license = \"MIT\"\n",
    "readme = \"README.md\"\n",
    "\n",
    "# ─── Restrict to Python 3.10–3.12 ──────────────────────────────\n",
    "requires-python = \">=3.10,<3.13\"\n",
    "\n",
    "dependencies = [\n",
    "  \"pandas>=1.2.0\",\n",
    "  \"numpy>=1.20.0\",\n",
    "  \"matplotlib>=3.4.0\",\n",
    "  \"scikit-learn>=1.4.2\",\n",
    "  \"pymc>=5.0.0\",\n",
    "  \"arviz>=0.14.0\",\n",
    "  \"statsmodels>=0.13.0\",\n",
    "  \"jupyterlab>=3.0.0\",\n",
    "  \"seaborn>=0.11.0\",\n",
    "  \"tabulate>=0.9.0\",\n",
    "  \"shap>=0.40.0\",\n",
    "  \"xgboost>=1.5.0\",\n",
    "  \"lightgbm>=3.3.0\",\n",
    "  \"catboost>=1.0.0\",\n",
    "  \"scipy>=1.7.0\",\n",
    "  \"shapash[report]>=2.3.0\",\n",
    "  \"shapiq>=0.1.0\",\n",
    "  \"explainerdashboard>=0.3.0\",\n",
    "  \"ipywidgets>=8.0.0\",\n",
    "  \"nutpie>=0.7.1\",   # new: nutpie backend for shapash\n",
    "  \"tqdm>=4.66.6\",    # relaxed version constraint\n",
    "  \"torch==2.3.1\",    # switched to stable version\n",
    "  \"torchvision==0.18.1\",\n",
    "  \"torchaudio==2.3.1\",\n",
    "  \"flax>=0.8.1\",\n",
    "  \"optax>=0.1.9\",\n",
    "  \"orbax-checkpoint>=0.4.8\",\n",
    "  \"mlflow>=2.10.2\",\n",
    "  \"mlflow-skinny>=2.10.2\",\n",
    "  \"pytest>=7.4.4\",\n",
    "  \"pytest-cov>=4.1.0\",\n",
    "  \"pytest-xdist>=3.5.0\",\n",
    "  \"pytest-timeout>=2.2.0\",\n",
    "  \"pytest-sugar>=1.0.0\",\n",
    "  \"pytest-html>=4.1.1\",\n",
    "  \"pytest-reportlog>=0.3.0\",\n",
    "  \"pytest-rerunfailures>=13.0\",\n",
    "  \"pytest-randomly>=3.15.0\",\n",
    "]\n",
    "\n",
    "[build-system]\n",
    "requires = [\"hatchling\"]\n",
    "build-backend = \"hatchling.build\"\n",
    "\n",
    "[project.optional-dependencies]\n",
    "cuda = []\n",
    "\n",
    "[[tool.uv.index]]\n",
    "name = \"pytorch-cu128\"\n",
    "url = \"https://download.pytorch.org/whl/cu128\"\n",
    "explicit = true  # critical: prevents accidental bleed-through\n",
    "\n",
    "[tool.hatch.build.targets.wheel]\n",
    "packages = [\".\"]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tasks.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tasks.py\n",
    "# tasks.py  ── invoke ≥2.2\n",
    "from invoke import task, Context, UnexpectedExit\n",
    "from typing import List, Optional\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "import tempfile\n",
    "import datetime as _dt\n",
    "import atexit\n",
    "import socket\n",
    "\n",
    "\n",
    "BASE_ENV = pathlib.Path(__file__).parent\n",
    "\n",
    "\n",
    "# Port helper functions\n",
    "def _is_port_free(port: int) -> bool:\n",
    "    \"\"\"Return True iff *port* is unused on localhost.\"\"\"\n",
    "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n",
    "        return sock.connect_ex((\"127.0.0.1\", port)) != 0   # non-0 = free\n",
    "\n",
    "def _find_free_port() -> int:\n",
    "    \"\"\"Ask the OS for an ephemeral port and return it.\"\"\"\n",
    "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n",
    "        sock.bind((\"\", 0))\n",
    "        return sock.getsockname()[1]\n",
    "\n",
    "\n",
    "# Track temporary env files for cleanup\n",
    "_saved_env_files: List[str] = []\n",
    "\n",
    "\n",
    "def _write_envfile(name: str, ports: Optional[dict[str, int]] = None) -> pathlib.Path:\n",
    "    \"\"\"Generate an .env file customised for this run & return its path.\"\"\"\n",
    "    env_lines = [f\"ENV_NAME={name}\"]\n",
    "    mapping = {\n",
    "        \"jupyter\": \"HOST_JUPYTER_PORT\",\n",
    "        \"tensorboard\": \"HOST_TENSORBOARD_PORT\",\n",
    "        \"explainer\": \"HOST_EXPLAINER_PORT\",\n",
    "        \"streamlit\": \"HOST_STREAMLIT_PORT\",\n",
    "        \"mlflow\": \"HOST_MLFLOW_PORT\",\n",
    "    }\n",
    "    for svc, var in mapping.items():\n",
    "        if ports and svc in ports:\n",
    "            env_lines.append(f\"{var}={ports[svc]}\")\n",
    "    # fall back to template defaults for everything else\n",
    "    env_lines.append(f\"# generated {_dt.datetime.now().isoformat()}\")\n",
    "    tmp = tempfile.NamedTemporaryFile(\n",
    "        \"w\", \n",
    "        delete=False, \n",
    "        prefix=\".env.\",\n",
    "        dir=BASE_ENV\n",
    "    )\n",
    "    tmp.write(\"\\n\".join(env_lines))\n",
    "    tmp.close()\n",
    "    _saved_env_files.append(tmp.name)\n",
    "    return pathlib.Path(tmp.name)\n",
    "\n",
    "\n",
    "# Register cleanup function\n",
    "def _cleanup_env_files() -> None:\n",
    "    \"\"\"Remove all temporary env files.\"\"\"\n",
    "    for path in _saved_env_files:\n",
    "        try:\n",
    "            os.remove(path)\n",
    "        except OSError:\n",
    "            pass\n",
    "\n",
    "\n",
    "atexit.register(_cleanup_env_files)\n",
    "\n",
    "\n",
    "def _compose(\n",
    "    c: Context,\n",
    "    cmd: str,\n",
    "    name: str,\n",
    "    rebuild: bool = False,\n",
    "    force_pty: bool = False,\n",
    "    ports: Optional[dict[str, int]] = None,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Wrapper around `docker compose` that\n",
    "\n",
    "    • Injects ENV_NAME and COMPOSE_PROJECT_NAME so both build-time (*args*)\n",
    "      and runtime (*docker-compose.yml* env) use one canonical name.\n",
    "    • Injects UV_PROJECT_ENVIRONMENT so every uv invocation\n",
    "      points at the baked venv `/app/.venv`, avoiding the mismatch\n",
    "      warning/error.\n",
    "    • Passes `-p <name>` so images / volumes share that namespace.\n",
    "    • Falls back gracefully when PTYs are unavailable (Windows CI).\n",
    "    • Allows custom port configuration via *ports* dict.\n",
    "    \"\"\"\n",
    "    env = {\n",
    "        **os.environ,\n",
    "        \"ENV_NAME\": name,\n",
    "        \"COMPOSE_PROJECT_NAME\": name,\n",
    "        \"UV_PROJECT_ENVIRONMENT\": \"/app/.venv\",  # Critical fix for uv sync\n",
    "    }\n",
    "\n",
    "    # propagate any port overrides\n",
    "    if ports:\n",
    "        port_vars = {\n",
    "            \"jupyter\": \"HOST_JUPYTER_PORT\",\n",
    "            \"tensorboard\": \"HOST_TENSORBOARD_PORT\",\n",
    "            \"explainer\": \"HOST_EXPLAINER_PORT\",\n",
    "            \"streamlit\": \"HOST_STREAMLIT_PORT\",\n",
    "            \"mlflow\": \"HOST_MLFLOW_PORT\",\n",
    "        }\n",
    "        env.update({port_vars[s]: str(p) for s, p in ports.items() if s in port_vars})\n",
    "\n",
    "    use_pty = force_pty or (os.name != \"nt\" and sys.stdin.isatty())\n",
    "    if not use_pty and not getattr(_compose, \"_warned\", False):\n",
    "        print(\"ℹ️  PTY not supported – running without TTY.\")\n",
    "        _compose._warned = True  # type: ignore[attr-defined]\n",
    "\n",
    "    base = f\"docker compose -p {name}\"\n",
    "    full_cmd = f\"{base} {cmd} --build --pull\" if rebuild else f\"{base} {cmd}\"\n",
    "    c.run(full_cmd, env=env, pty=use_pty)\n",
    "\n",
    "\n",
    "def _ensure_lock(c):\n",
    "    \"\"\"Raise if uv.lock is out of sync.\"\"\"\n",
    "    try:\n",
    "        c.run(\"uv lock --check\", hide=True)\n",
    "    except UnexpectedExit:\n",
    "        raise RuntimeError(\"uv.lock is stale. Run `invoke lock`.\")\n",
    "\n",
    "\n",
    "@task\n",
    "def lock(c):\n",
    "    \"\"\"Verify or regenerate uv.lock.\"\"\"\n",
    "    print(\"🔍  Checking lock drift…\")\n",
    "    res = c.run(\"uv lock --check\", warn=True, pty=False)\n",
    "    if res.ok:\n",
    "        print(\"✅  uv.lock is in sync\")\n",
    "    else:\n",
    "        print(\"⚠️  Lock drift detected – regenerating\")\n",
    "        c.run(\"uv lock --upgrade\")\n",
    "\n",
    "\n",
    "@task(\n",
    "    help={\n",
    "        \"name\": \"Project/venv name (defaults to folder name)\",\n",
    "        \"use_pty\": \"Force PTY even on non-POSIX hosts\",\n",
    "        \"jupyter_port\": \"Jupyter Lab port (default: 8890)\",\n",
    "        \"tensorboard_port\": \"TensorBoard port (default: auto-assigned)\",\n",
    "        \"explainer_port\": \"Explainer Dashboard port (default: auto-assigned)\", \n",
    "        \"streamlit_port\": \"Streamlit port (default: auto-assigned)\",\n",
    "        \"mlflow_port\": \"MLflow UI port (default: 5000, auto-assigns if busy)\",\n",
    "    }\n",
    ")\n",
    "def up(\n",
    "    c,\n",
    "    name: Optional[str] = None,\n",
    "    rebuild: bool = False,\n",
    "    detach: bool = True,\n",
    "    use_pty: bool = False,\n",
    "    jupyter_port: Optional[int] = None,\n",
    "    tensorboard_port: Optional[int] = None,\n",
    "    explainer_port: Optional[int] = None,\n",
    "    streamlit_port: Optional[int] = None,\n",
    "    mlflow_port: Optional[int] = None,\n",
    ") -> None:\n",
    "    \"\"\"Build (optionally --rebuild) & start the container with custom ports.\"\"\"\n",
    "    _ensure_lock(c)  # Check lock status before proceeding\n",
    "    name = name or BASE_ENV.name\n",
    "    \n",
    "    # Build ports dict from provided arguments\n",
    "    ports = {}\n",
    "    if jupyter_port is not None:\n",
    "        ports[\"jupyter\"] = jupyter_port\n",
    "    if tensorboard_port is not None:\n",
    "        ports[\"tensorboard\"] = tensorboard_port\n",
    "    if explainer_port is not None:\n",
    "        ports[\"explainer\"] = explainer_port\n",
    "    if streamlit_port is not None:\n",
    "        ports[\"streamlit\"] = streamlit_port\n",
    "        \n",
    "    # Handle MLflow port - use 5000 if free, otherwise find a free port\n",
    "    if mlflow_port is None:\n",
    "        mlflow_port = 5000 if _is_port_free(5000) else _find_free_port()\n",
    "    elif not _is_port_free(mlflow_port):\n",
    "        print(f\"❌ Port {mlflow_port} is already in use!\")\n",
    "        sys.exit(1)\n",
    "    ports[\"mlflow\"] = mlflow_port\n",
    "    print(f\"🔌 MLflow host-port → {mlflow_port}\")\n",
    "    \n",
    "    # Generate environment file\n",
    "    env_path = _write_envfile(name, ports)\n",
    "    env_file_flag = f\"--env-file {env_path}\"\n",
    "    compose_cmd = \"up -d\" if detach else \"up\"\n",
    "\n",
    "    _compose(\n",
    "        c,\n",
    "        f\"{env_file_flag} {compose_cmd}\",\n",
    "        name,\n",
    "        rebuild=rebuild,\n",
    "        force_pty=use_pty,\n",
    "        ports=ports if ports else None,\n",
    "    )\n",
    "\n",
    "\n",
    "@task(\n",
    "    help={\n",
    "        \"name\": \"Project/venv name (defaults to folder name)\",\n",
    "    }\n",
    ")\n",
    "def stop(c, name: Optional[str] = None) -> None:\n",
    "    \"\"\"Stop and remove dev container (keeps volumes).\"\"\"\n",
    "    name = name or BASE_ENV.name\n",
    "    cmd = f\"docker compose -p {name} down\"\n",
    "    try:\n",
    "        c.run(cmd)\n",
    "        print(f\"\\n🛑 Stopped and removed project '{name}'\")\n",
    "    except Exception:\n",
    "        print(f\"❌ No running containers found for project '{name}'\")\n",
    "\n",
    "\n",
    "@task\n",
    "def shell(c, name: str | None = None) -> None:\n",
    "    \"\"\"Open an interactive shell inside the running container.\"\"\"\n",
    "    name = name or BASE_ENV.name\n",
    "    cmd = f\"docker compose -p {name} ps -q datascience\"\n",
    "    cid = c.run(cmd, hide=True).stdout.strip()\n",
    "    c.run(f\"docker exec -it {cid} bash\", env={\"ENV_NAME\": name}, pty=False)\n",
    "\n",
    "\n",
    "@task\n",
    "def clean(c) -> None:\n",
    "    \"\"\"Prune stopped containers + dangling images.\"\"\"\n",
    "    c.run(\"docker system prune -f\")\n",
    "\n",
    "\n",
    "@task\n",
    "def ports(c, name: str | None = None) -> None:\n",
    "    \"\"\"Show current port mappings for the named project.\"\"\"\n",
    "    name = name or BASE_ENV.name\n",
    "    cmd = f\"docker compose -p {name} ps --format table\"\n",
    "    try:\n",
    "        c.run(cmd, hide=False)\n",
    "        print(f\"\\n📊 Port mappings for project '{name}':\")\n",
    "        print(\"=\" * 50)\n",
    "    except Exception:\n",
    "        print(f\"❌ No running containers found for project '{name}'\")\n",
    "        print(\"\\n💡 Usage examples:\")\n",
    "        print(\"  invoke up --name myproject --jupyter-port 8891\")\n",
    "        print(\"  invoke up --name myproject --jupyter-port 8892 \\\\\")\n",
    "        print(\"    --tensorboard-port 6009\")\n",
    "\n",
    "\n",
    "# --- utilities ---------------------------------------------------------------\n",
    "def _norm(path: str | pathlib.Path) -> str:\n",
    "    \"\"\"Return a lower-case, forward-slash, no-trailing-slash version of *path*.\"\"\"\n",
    "    p = str(path).replace(\"\\\\\", \"/\").rstrip(\"/\").lower()\n",
    "    return p\n",
    "\n",
    "def _docker_projects_from_this_repo() -> set[str]:\n",
    "    \"\"\"\n",
    "    Discover every Compose *project name* whose working_dir label ends with\n",
    "    the current repo path.\n",
    "\n",
    "    Works across Windows ↔ WSL ↔ macOS because we do suffix-match on a\n",
    "    normalised path.\n",
    "    \"\"\"\n",
    "    here_tail = _norm(pathlib.Path(__file__).parent.resolve())\n",
    "    cmd = (\n",
    "        \"docker container ls -a \"\n",
    "        \"--format '{{.Label \\\"com.docker.compose.project\\\"}} \"\n",
    "        \"{{.Label \\\"com.docker.compose.project.working_dir\\\"}}' \"\n",
    "        \"--filter label=com.docker.compose.project\"\n",
    "    )\n",
    "    projects: set[str] = set()\n",
    "    for line in os.popen(cmd).read().strip().splitlines():\n",
    "        try:\n",
    "            proj, wd = line.split(maxsplit=1)\n",
    "        except ValueError:\n",
    "            continue\n",
    "        if _norm(wd).endswith(here_tail):\n",
    "            projects.add(proj)\n",
    "    return projects\n",
    "\n",
    "# --- task --------------------------------------------------------------------\n",
    "@task(\n",
    "    help={\n",
    "        \"name\": \"Project name (defaults to folder). Ignored with --all.\",\n",
    "        \"all\":  \"Remove *all* projects launched from this repo.\",\n",
    "        \"rmi\":  \"Image-removal policy: all | local | none (default: local).\",\n",
    "    }\n",
    ")\n",
    "def down(c, name: str | None = None, all: bool = False, rmi: str = \"local\"):\n",
    "    \"\"\"Stop and remove dev container(s) with optional image cleanup.\"\"\"\n",
    "    if rmi not in {\"all\", \"local\", \"none\"}:\n",
    "        raise ValueError(\"--rmi must be all | local | none\")\n",
    "\n",
    "    targets = _docker_projects_from_this_repo() if all else {name or BASE_ENV.name}\n",
    "    flags = \"-v --remove-orphans\"\n",
    "    if rmi != \"none\":\n",
    "        flags += f\" --rmi {rmi}\"\n",
    "\n",
    "    for proj in targets:\n",
    "        try:\n",
    "            c.run(f\"docker compose -p {proj} down {flags}\")\n",
    "            print(f\"🗑️  Removed project '{proj}'\")\n",
    "        except Exception:\n",
    "            print(f\"⚠️  Nothing to remove for '{proj}'\")\n",
    "\n",
    "\n",
    "@task\n",
    "def diag(c):\n",
    "    \"\"\"Run deep container diagnostics inside the running dev container.\"\"\"\n",
    "    name = BASE_ENV.name\n",
    "    cmd = f\"docker compose -p {name} ps -q datascience\"\n",
    "    cid = c.run(cmd, hide=True).stdout.strip()\n",
    "    cmd = f\"docker exec {cid} python /app/tests/diagnose_devcontainer.py\"\n",
    "    c.run(cmd, pty=False)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tests/diagnose_devcontainer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/diagnose_devcontainer.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Deep container diagnostics for development environment.\n",
    "Checks GPU availability, CUDA configuration, and environment setup.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import platform\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def check_environment():\n",
    "    \"\"\"Check environment variables and paths.\"\"\"\n",
    "    env_info = {\n",
    "        \"python\": sys.version,\n",
    "        \"platform\": platform.platform(),\n",
    "        \"cuda_home\": os.getenv(\"CUDA_HOME\", \"Not set\"),\n",
    "        \"cuda_path\": os.getenv(\"CUDA_PATH\", \"Not set\"),\n",
    "        \"ld_library_path\": os.getenv(\"LD_LIBRARY_PATH\", \"Not set\"),\n",
    "        \"virtual_env\": os.getenv(\"VIRTUAL_ENV\", \"Not set\"),\n",
    "    }\n",
    "    return env_info\n",
    "\n",
    "\n",
    "def check_cuda():\n",
    "    \"\"\"Check CUDA toolkit installation.\"\"\"\n",
    "    cuda_info = {}\n",
    "    \n",
    "    # Check CUDA symlink\n",
    "    cuda_link = Path(\"/usr/local/cuda\")\n",
    "    if cuda_link.exists():\n",
    "        cuda_info[\"symlink\"] = str(cuda_link.resolve())\n",
    "    else:\n",
    "        cuda_info[\"symlink\"] = \"Not found\"\n",
    "    \n",
    "    # Check nvcc version\n",
    "    try:\n",
    "        import subprocess\n",
    "        result = subprocess.run(\n",
    "            [\"nvcc\", \"--version\"],\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "        cuda_info[\"nvcc\"] = result.stdout.strip()\n",
    "    except Exception as e:\n",
    "        cuda_info[\"nvcc\"] = f\"Error: {str(e)}\"\n",
    "    \n",
    "    return cuda_info\n",
    "\n",
    "\n",
    "def check_gpu_frameworks():\n",
    "    \"\"\"Check PyTorch and JAX GPU support.\"\"\"\n",
    "    gpu_info = {}\n",
    "    \n",
    "    # PyTorch\n",
    "    try:\n",
    "        import torch\n",
    "        gpu_info[\"torch\"] = {\n",
    "            \"version\": torch.__version__,\n",
    "            \"cuda_available\": torch.cuda.is_available(),\n",
    "            \"device_count\": torch.cuda.device_count() if torch.cuda.is_available() else 0,\n",
    "        }\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_info[\"torch\"][\"device_name\"] = torch.cuda.get_device_name(0)\n",
    "    except ImportError:\n",
    "        gpu_info[\"torch\"] = \"Not installed\"\n",
    "    \n",
    "    # JAX\n",
    "    try:\n",
    "        import jax\n",
    "        devices = jax.devices()\n",
    "        gpu_info[\"jax\"] = {\n",
    "            \"version\": jax.__version__,\n",
    "            \"devices\": [str(d) for d in devices],\n",
    "            \"gpu_devices\": len([d for d in devices if d.platform == \"gpu\"]),\n",
    "        }\n",
    "    except ImportError:\n",
    "        gpu_info[\"jax\"] = \"Not installed\"\n",
    "    \n",
    "    return gpu_info\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run all diagnostics and print results.\"\"\"\n",
    "    results = {\n",
    "        \"environment\": check_environment(),\n",
    "        \"cuda\": check_cuda(),\n",
    "        \"gpu_frameworks\": check_gpu_frameworks(),\n",
    "    }\n",
    "    \n",
    "    print(\"\\n🔍 Container Diagnostics\")\n",
    "    print(\"=\" * 50)\n",
    "    print(json.dumps(results, indent=2))\n",
    "    \n",
    "    # Check for critical issues\n",
    "    critical = False\n",
    "    warnings = []\n",
    "    \n",
    "    gpu_info = results[\"gpu_frameworks\"]\n",
    "    if isinstance(gpu_info[\"torch\"], dict):\n",
    "        if not gpu_info[\"torch\"][\"cuda_available\"]:\n",
    "            critical = True\n",
    "            warnings.append(\"❌ PyTorch CUDA not available\")\n",
    "    \n",
    "    if isinstance(gpu_info[\"jax\"], dict):\n",
    "        if gpu_info[\"jax\"][\"gpu_devices\"] == 0:\n",
    "            critical = True\n",
    "            warnings.append(\"❌ JAX GPU devices not found\")\n",
    "    \n",
    "    if warnings:\n",
    "        print(\"\\n⚠️  Warnings:\")\n",
    "        for w in warnings:\n",
    "            print(f\"  {w}\")\n",
    "    \n",
    "    sys.exit(1 if critical else 0)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tests/test_pytorch_jax_gpu.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/test_pytorch_jax_gpu.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Test script to verify that PyTorch and JAX can access the GPU,\n",
    "and that PyJAGS is working correctly.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "def test_pytorch_gpu():\n",
    "    \"\"\"Test PyTorch GPU availability and basic operations.\"\"\"\n",
    "    print(\"\\n=== Testing PyTorch GPU ===\")\n",
    "    try:\n",
    "        import torch\n",
    "        print(f\"PyTorch version: {torch.__version__}\")\n",
    "        print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "        \n",
    "        if not torch.cuda.is_available():\n",
    "            print(\"❌ PyTorch CUDA not available!\")\n",
    "            return False\n",
    "        \n",
    "        print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "        print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "        print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "        \n",
    "        # Run a simple test computation\n",
    "        x = torch.rand(1000, 1000).cuda()\n",
    "        y = torch.rand(1000, 1000).cuda()\n",
    "        start = torch.cuda.Event(enable_timing=True)\n",
    "        end = torch.cuda.Event(enable_timing=True)\n",
    "        \n",
    "        start.record()\n",
    "        z = torch.matmul(x, y)\n",
    "        end.record()\n",
    "        \n",
    "        # Wait for GPU computation to finish\n",
    "        torch.cuda.synchronize()\n",
    "        print(f\"Matrix multiplication time: {start.elapsed_time(end):.2f} ms\")\n",
    "        print(f\"Result shape: {z.shape}\")\n",
    "        print(\"✅ PyTorch GPU test passed!\")\n",
    "        return True\n",
    "    \n",
    "    except ImportError:\n",
    "        print(\"❌ PyTorch not found!\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during PyTorch GPU test: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def test_jax_gpu():\n",
    "    \"\"\"Test JAX GPU availability and basic operations.\"\"\"\n",
    "    print(\"\\n=== Testing JAX GPU ===\")\n",
    "    try:\n",
    "        import jax\n",
    "        import jax.numpy as jnp\n",
    "        \n",
    "        print(f\"JAX version: {jax.__version__}\")\n",
    "        \n",
    "        # Force GPU platform\n",
    "        jax.config.update('jax_platform_name', 'gpu')\n",
    "        \n",
    "        # Get device count and details\n",
    "        devices = jax.devices()\n",
    "        device_count = len(devices)\n",
    "        print(f\"Available devices: {device_count}\")\n",
    "        \n",
    "        for i, device in enumerate(devices):\n",
    "            print(f\"Device {i}: {device}\")\n",
    "        \n",
    "        if device_count == 0 or 'cuda' not in str(devices[0]).lower():\n",
    "            print(\"❌ No GPU devices found by JAX!\")\n",
    "            return False\n",
    "        \n",
    "        # Check CUDA configuration\n",
    "        jit_info = jax.config.values\n",
    "        print(f\"JAX configuration: {jit_info}\")\n",
    "        \n",
    "        # Run a simple GPU computation\n",
    "        print(\"Running a test computation on GPU...\")\n",
    "        try:\n",
    "            x = jnp.ones((1000, 1000))\n",
    "            y = jnp.ones((1000, 1000))\n",
    "            \n",
    "            # Use JIT compilation for better performance\n",
    "            @jax.jit\n",
    "            def matmul(a, b):\n",
    "                return jnp.matmul(a, b)\n",
    "            \n",
    "            result = matmul(x, y)\n",
    "            print(f\"Result shape: {result.shape}\")\n",
    "            \n",
    "            print(\"✅ JAX GPU test passed!\")\n",
    "            return True\n",
    "        except RuntimeError as e:\n",
    "            if \"ptxas too old\" in str(e):\n",
    "                print(f\"⚠️ JAX GPU detected but CUDA compatibility issue: {e}\")\n",
    "                print(\"⚠️ JAX can see the GPU but there's a CUDA version compatibility issue.\")\n",
    "                print(\"⚠️ This is considered a partial success since the GPU is detected.\")\n",
    "                return True\n",
    "            else:\n",
    "                raise\n",
    "    \n",
    "    except ImportError:\n",
    "        print(\"❌ JAX not found!\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during JAX GPU test: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def test_pyjags():\n",
    "    \"\"\"Test PyJAGS installation and basic functionality.\"\"\"\n",
    "    print(\"\\n=== Testing PyJAGS ===\")\n",
    "    try:\n",
    "        import pyjags\n",
    "        print(f\"PyJAGS version: {pyjags.__version__}\")\n",
    "        \n",
    "        # Create a simple model to verify that PyJAGS works\n",
    "        code = \"\"\"\n",
    "        model {\n",
    "            # Likelihood\n",
    "            y ~ dnorm(mu, 1/sigma^2)\n",
    "            \n",
    "            # Priors\n",
    "            mu ~ dnorm(0, 0.001)\n",
    "            sigma ~ dunif(0, 100)\n",
    "        }\n",
    "        \"\"\"\n",
    "        \n",
    "        # Sample data\n",
    "        data = {'y': 0.5}\n",
    "        \n",
    "        # Initialize model with data\n",
    "        model = pyjags.Model(code, data=data, chains=1, adapt=100)\n",
    "        print(\"JAGS model initialized successfully!\")\n",
    "        \n",
    "        # Sample from the model\n",
    "        samples = model.sample(200, vars=['mu', 'sigma'])\n",
    "        print(\"JAGS sampling completed successfully!\")\n",
    "        \n",
    "        # Verify the samples\n",
    "        mu_samples = samples['mu']\n",
    "        sigma_samples = samples['sigma']\n",
    "        print(f\"mu mean: {mu_samples.mean():.4f}\")\n",
    "        print(f\"sigma mean: {sigma_samples.mean():.4f}\")\n",
    "        \n",
    "        print(\"✅ PyJAGS test passed!\")\n",
    "        return True\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"❌ PyJAGS not found!\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during PyJAGS test: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Running GPU and PyJAGS verification tests...\")\n",
    "    \n",
    "    pytorch_success = test_pytorch_gpu()\n",
    "    jax_success = test_jax_gpu()\n",
    "    pyjags_success = test_pyjags()\n",
    "    \n",
    "    print(\"\\n=== Test Summary ===\")\n",
    "    print(f\"PyTorch GPU: {'✅ PASS' if pytorch_success else '❌ FAIL'}\")\n",
    "    print(f\"JAX GPU: {'✅ PASS' if jax_success else '❌ FAIL'}\")\n",
    "    print(f\"PyJAGS: {'✅ PASS' if pyjags_success else '❌ FAIL'}\")\n",
    "    \n",
    "    if pytorch_success and jax_success and pyjags_success:\n",
    "        print(\"\\n🎉 All tests passed! The container is working correctly.\")\n",
    "        sys.exit(0)\n",
    "    else:\n",
    "        print(\"\\n❌ Some tests failed. Please check the output for details.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
