{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/backend/ML/mlops/config.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/backend/ML/mlops/config.py\n",
        "\"\"\"Central MLflow configuration for consistent experiment tracking.\"\"\"\n",
        "import os\n",
        "\n",
        "# ─── MLflow configuration ──────────────────────────────────────────────────\n",
        "# Use Docker service-name so this works inside the compose network\n",
        "# Falls back to local file store for standalone usage\n",
        "TRACKING_URI = os.getenv(\"MLFLOW_TRACKING_URI\", \"http://mlflow:5000\")\n",
        "EXPERIMENT_NAME = \"iris_classification\"\n",
        "ARTIFACT_ROOT = os.getenv(\"MLFLOW_ARTIFACT_ROOT\", \"./mlruns\")\n",
        "\n",
        "# ─── Model registry ────────────────────────────────────────────────────────\n",
        "MODEL_NAME = \"iris_classifier\"\n",
        "MODEL_STAGE_PRODUCTION = \"Production\"\n",
        "MODEL_STAGE_STAGING = \"Staging\"\n",
        "\n",
        "# ─── Dataset defaults ──────────────────────────────────────────────────────\n",
        "RANDOM_STATE = 42\n",
        "TEST_SIZE = 0.2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/backend/ML/mlops/logging.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/backend/ML/mlops/logging.py\n",
        "\"\"\"\n",
        "Extended MLflow logging helpers.\n",
        "\"\"\"\n",
        "from __future__ import annotations\n",
        "import mlflow\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Dict, Any, Sequence, Optional\n",
        "from matplotlib.figure import Figure\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_recall_fscore_support,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    roc_auc_score,\n",
        "    log_loss,\n",
        "    matthews_corrcoef,\n",
        ")\n",
        "\n",
        "\n",
        "def _log_fig(fig: Figure, name: str) -> None:\n",
        "    \"\"\"Log a Matplotlib figure directly without temp files.\"\"\"\n",
        "    mlflow.log_figure(fig, artifact_file=name)\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "def log_full_metrics(\n",
        "    y_true, y_pred, *, label_list: Optional[Sequence[int]] = None, prefix: str = \"\"\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Compute & log *all* useful classification metrics.\n",
        "\n",
        "    Returns a flat dict so callers can unit-test easily.\n",
        "\n",
        "    Args:\n",
        "        y_true: True labels\n",
        "        y_pred: Predicted labels\n",
        "        label_list: Optional list of label integers (for compatibility)\n",
        "        prefix: Optional prefix for metric names\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of all calculated metrics\n",
        "    \"\"\"\n",
        "    # (1) macro metrics ------------------------------------------------------\n",
        "    macro = precision_recall_fscore_support(\n",
        "        y_true, y_pred, average=\"macro\", zero_division=\"warn\"\n",
        "    )\n",
        "    metrics: Dict[str, float] = {\n",
        "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
        "        \"precision_macro\": float(macro[0]),\n",
        "        \"recall_macro\": float(macro[1]),\n",
        "        \"f1_macro\": float(macro[2]),\n",
        "    }\n",
        "\n",
        "    # (2) per-class ----------------------------------------------------------\n",
        "    report = classification_report(y_true, y_pred, output_dict=True, zero_division=\"warn\")\n",
        "    if isinstance(report, dict):\n",
        "        for klass, d in report.items():\n",
        "            if isinstance(klass, str) and klass.isdigit():  # skip 'accuracy', 'macro avg', …\n",
        "                k = int(klass)\n",
        "                if isinstance(d, dict):\n",
        "                    precision_val = d.get(\"precision\", 0.0)\n",
        "                    recall_val = d.get(\"recall\", 0.0)\n",
        "                    f1_val = d.get(\"f1-score\", 0.0)\n",
        "                    support_val = d.get(\"support\", 0.0)\n",
        "\n",
        "                    metrics[f\"precision_{k}\"] = float(precision_val) if precision_val is not None else 0.0\n",
        "                    metrics[f\"recall_{k}\"] = float(recall_val) if recall_val is not None else 0.0\n",
        "                    metrics[f\"f1_{k}\"] = float(f1_val) if f1_val is not None else 0.0\n",
        "                    metrics[f\"support_{k}\"] = float(support_val) if support_val is not None else 0.0\n",
        "\n",
        "    # (3) derived – try/except so we never crash ----------------------------\n",
        "    try:\n",
        "        metrics[\"roc_auc_ovr_weighted\"] = roc_auc_score(\n",
        "            y_true, pd.get_dummies(y_pred), multi_class=\"ovr\", average=\"weighted\"\n",
        "        )\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        metrics[\"log_loss\"] = log_loss(y_true, pd.get_dummies(y_pred))\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        metrics[\"mcc\"] = matthews_corrcoef(y_true, y_pred)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # (4) optional prefix for nested CV, etc. -------------------------------\n",
        "    if prefix:\n",
        "        metrics = {f\"{prefix}_{k}\": v for k, v in metrics.items()}\n",
        "\n",
        "    mlflow.log_metrics(metrics)\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def log_confusion_matrix(\n",
        "    y_true, y_pred, *, class_names: Optional[Sequence[str]] = None, artifact_name: str = \"confusion_matrix.png\"\n",
        ") -> None:\n",
        "    \"\"\"Create + log confusion matrix using mlflow.log_figure.\"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    sns.heatmap(\n",
        "        cm,\n",
        "        annot=True,\n",
        "        fmt=\"d\",\n",
        "        cmap=\"Blues\",\n",
        "        xticklabels=class_names if class_names is not None else \"auto\",\n",
        "        yticklabels=class_names if class_names is not None else \"auto\",\n",
        "        ax=ax,\n",
        "    )\n",
        "    ax.set_xlabel(\"Predicted\")\n",
        "    ax.set_ylabel(\"Actual\")\n",
        "    ax.set_title(\"Confusion Matrix\")\n",
        "    _log_fig(fig, artifact_name)\n",
        "\n",
        "\n",
        "def log_feature_importance(\n",
        "    feature_names: list, importances: list, artifact_name: str = \"feature_importance.png\"\n",
        "):\n",
        "    \"\"\"Bar plot logged via mlflow.log_figure (no disk I/O).\"\"\"\n",
        "    imp_df = (\n",
        "        pd.DataFrame({\"feature\": feature_names, \"importance\": importances})\n",
        "        .sort_values(\"importance\")\n",
        "    )\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    sns.barplot(data=imp_df, x=\"importance\", y=\"feature\", ax=ax)\n",
        "    ax.set_title(\"Feature Importances\")\n",
        "    _log_fig(fig, artifact_name)\n",
        "\n",
        "\n",
        "def log_parameters(params: Dict[str, Any]) -> None:\n",
        "    \"\"\"\n",
        "    Log parameters to MLflow.\n",
        "\n",
        "    Args:\n",
        "        params: Dictionary of parameter names and values\n",
        "    \"\"\"\n",
        "    mlflow.log_params(params)\n",
        "\n",
        "\n",
        "def log_dataset_info(X_train, X_test, y_train, y_test) -> None:\n",
        "    \"\"\"\n",
        "    Log dataset information as parameters.\n",
        "\n",
        "    Args:\n",
        "        X_train: Training features\n",
        "        X_test: Test features\n",
        "        y_train: Training labels\n",
        "        y_test: Test labels\n",
        "    \"\"\"\n",
        "    dataset_params = {\n",
        "        \"train_size\": len(X_train),\n",
        "        \"test_size\": len(X_test),\n",
        "        \"n_features\": (X_train.shape[1] if hasattr(X_train, \"shape\") else len(X_train[0])),\n",
        "        \"n_classes\": (len(set(y_train)) if hasattr(y_train, \"__iter__\") else 1),\n",
        "    }\n",
        "\n",
        "    log_parameters(dataset_params)\n",
        "\n",
        "\n",
        "# Legacy compatibility - keep old function name as alias\n",
        "log_model_metrics = log_full_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/backend/ML/mlops/experiment_utils.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/backend/ML/mlops/experiment_utils.py\n",
        "\"\"\"MLflow experiment utilities.\"\"\"\n",
        "import os\n",
        "import pathlib\n",
        "import mlflow\n",
        "import mlflow.tracking\n",
        "from typing import Optional, Dict, Any\n",
        "import requests\n",
        "\n",
        "from src.backend.ML.mlops.config import EXPERIMENT_NAME, TRACKING_URI\n",
        "\n",
        "import re, shutil, logging\n",
        "from src.backend.ML.mlops.config import ARTIFACT_ROOT\n",
        "\n",
        "# ─── PATCH: src/backend/ML/mlops/experiment_utils.py ──────────────────────────\n",
        "def _patch_yaml_for_mlflow() -> None:\n",
        "    \"\"\"\n",
        "    Register PyYAML representers for MLflow entity objects (*Metric*, *Param*,\n",
        "    *RunTag*) **once per interpreter** so that MLflow's FileStore can write a\n",
        "    valid ``meta.yaml`` even when it uses ``yaml.safe_dump`` / ``YamlSafeDumper``.\n",
        "\n",
        "    The original implementation raised::\n",
        "\n",
        "        TypeError: mappingproxy() argument must be a mapping, not set\n",
        "\n",
        "    because a *set* was passed to ``MappingProxyType`` (which requires a *dict*).\n",
        "    The new version uses a **tuple** and skips the proxy altogether—immutability\n",
        "    is already guaranteed by the tuple itself.\n",
        "    \"\"\"\n",
        "    import yaml\n",
        "    from types import MappingProxyType    # stdlib\n",
        "    try:\n",
        "        from mlflow.entities import Metric, Param, RunTag\n",
        "        from mlflow.utils.yaml_utils import YamlSafeDumper as _ML_DUMPER\n",
        "    except Exception:\n",
        "        # MLflow may not be imported yet → safe no-op\n",
        "        return\n",
        "\n",
        "    def _as_dict(dumper: yaml.Dumper, obj) -> yaml.Node:\n",
        "        \"\"\"Serialise MLflow entity via its public ``to_dict`` method.\"\"\"\n",
        "        return dumper.represent_dict(obj.to_dict())\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # 1. Target dumpers we care about (SafeDumper **and** MLflow's custom one)\n",
        "    #    Using a tuple avoids the previous MappingProxyType misuse.\n",
        "    _TARGET_DUMPERS = (yaml.SafeDumper, _ML_DUMPER)\n",
        "\n",
        "    # 2. Entity classes requiring custom representers\n",
        "    _ENTITY_CLASSES = (Metric, Param, RunTag)\n",
        "\n",
        "    # 3. Register representers idempotently\n",
        "    for dumper_cls in _TARGET_DUMPERS:\n",
        "        for cls in _ENTITY_CLASSES:\n",
        "            if cls not in dumper_cls.yaml_representers:\n",
        "                yaml.add_representer(cls, _as_dict, Dumper=dumper_cls)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "\n",
        "def _is_http_uri(uri: str) -> bool:\n",
        "    return uri.startswith(\"http\")\n",
        "\n",
        "def _local_path_from_uri(uri: str) -> pathlib.Path:\n",
        "    return pathlib.Path(uri.replace(\"file:\", \"\", 1)) if uri.startswith(\"file:\") else pathlib.Path(uri)\n",
        "\n",
        "_HEALTH_ENDPOINTS = (\"/health\", \"/version\")\n",
        "_hex32 = re.compile(r\"^[0-9a-f]{32}$\", re.I)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def _ping_tracking_server(uri: str, timeout: float = 2.0) -> bool:\n",
        "    \"\"\"Return True iff an HTTP MLflow server is reachable at *uri*.\"\"\"\n",
        "    if not uri.startswith(\"http\"):\n",
        "        return False                        # file store – nothing to ping\n",
        "    try:\n",
        "        # Use new health endpoints\n",
        "        for ep in _HEALTH_ENDPOINTS:\n",
        "            response = requests.get(uri.rstrip(\"/\") + ep, timeout=timeout)\n",
        "            response.raise_for_status()\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "\n",
        "# ── experiment_utils.py ───────────────────────────────────────────────────\n",
        "import pathlib, shutil, logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def _ensure_trash(root: pathlib.Path) -> pathlib.Path:\n",
        "    \"\"\"Return the path to <root>/.trash, creating it if needed.\"\"\"\n",
        "    trash_path = root / \".trash\"\n",
        "    trash_path.mkdir(exist_ok=True)\n",
        "    return trash_path\n",
        "\n",
        "\n",
        "def _sanitize_mlruns_dir(root: pathlib.Path) -> None:\n",
        "    \"\"\"\n",
        "    Remove **only** invalid directories while preserving MLflow's mandatory\n",
        "    '.trash' folder.  Idempotent and safe to call repeatedly.\n",
        "    \"\"\"\n",
        "    trash_root = _ensure_trash(root)\n",
        "\n",
        "    # 1️⃣  Clean *inside* .trash (broken experiments that never got meta.yaml)\n",
        "    for p in list(trash_root.iterdir()):\n",
        "        if p.is_dir() and (not p.name.isdigit() or not (p / \"meta.yaml\").exists()):\n",
        "            logger.warning(\"🧹 Purging corrupt trash dir %s\", p)\n",
        "            shutil.rmtree(p, ignore_errors=True)\n",
        "\n",
        "    # 2️⃣  Sweep root level (skip .trash itself)\n",
        "    for p in list(root.iterdir()):\n",
        "        if p == trash_root:\n",
        "            continue\n",
        "        remove = (\n",
        "            p.is_dir()\n",
        "            and (\n",
        "                p.name == \"artifacts\"                   # stray artifact root\n",
        "                or not p.name.isdigit()                 # junk\n",
        "                or not (p / \"meta.yaml\").exists()       # corrupt exp\n",
        "            )\n",
        "        )\n",
        "        if remove:\n",
        "            logger.warning(\"🧹 Removing stray MLflow dir %s\", p)\n",
        "            shutil.rmtree(p, ignore_errors=True)\n",
        "\n",
        "\n",
        "def _fallback_uri() -> str:\n",
        "    \"\"\"Local file store outside default ./mlruns to avoid collisions.\"\"\"\n",
        "    local = pathlib.Path.cwd() / \"mlruns_local\"\n",
        "    local.mkdir(exist_ok=True)\n",
        "    _sanitize_mlruns_dir(local)\n",
        "    _ensure_trash(local)          # make doubly sure\n",
        "    return f\"file:{local}\"\n",
        "\n",
        "\n",
        "# ── src/backend/ML/mlops/experiment_utils.py ──\n",
        "_resolved_uri: str | None = None         # module-level cache\n",
        "\n",
        "\n",
        "\n",
        "def setup_mlflow_experiment(experiment_name: str | None = None) -> None:\n",
        "    \"\"\"\n",
        "    Initialise MLflow tracking & experiment **safely**, no matter whether a\n",
        "    remote server is reachable or we fall back to the local file store.\n",
        "    \"\"\"\n",
        "    _patch_yaml_for_mlflow()\n",
        "    global _resolved_uri\n",
        "    exp_name = experiment_name or EXPERIMENT_NAME\n",
        "\n",
        "    if _resolved_uri is None:\n",
        "        uri = TRACKING_URI\n",
        "        if not _ping_tracking_server(uri):\n",
        "            uri = _fallback_uri()\n",
        "            logger.warning(\"⚠️  MLflow server unreachable – using local store %s\", uri)\n",
        "        mlflow.set_tracking_uri(uri)\n",
        "        _resolved_uri = uri\n",
        "    else:\n",
        "        mlflow.set_tracking_uri(_resolved_uri)\n",
        "\n",
        "    # --- NEW: always clean the store when it's file-based ------------------\n",
        "    if not _is_http_uri(_resolved_uri):\n",
        "        root_path = _local_path_from_uri(_resolved_uri)\n",
        "        _sanitize_mlruns_dir(root_path)\n",
        "    # ----------------------------------------------------------------------\n",
        "\n",
        "    # decide on artifact root only if explicitly configured\n",
        "    artifact_loc = ARTIFACT_ROOT.strip() or None\n",
        "\n",
        "    if mlflow.get_experiment_by_name(exp_name) is None:\n",
        "        mlflow.create_experiment(exp_name, artifact_location=artifact_loc)\n",
        "    mlflow.set_experiment(exp_name)\n",
        "    logger.info(\"🗂 Experiment '%s' @ %s\", exp_name, _resolved_uri)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_best_run(\n",
        "    experiment_name: Optional[str] = None,\n",
        "    metric_key: str = \"accuracy\",\n",
        "    maximize: bool = True,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Return a *shallow* dict with run_id, metrics.*, and params.* keys\n",
        "    so downstream code can use predictable dotted paths.\n",
        "    \"\"\"\n",
        "    exp_name = experiment_name or EXPERIMENT_NAME\n",
        "    setup_mlflow_experiment(exp_name)\n",
        "\n",
        "    client = mlflow.tracking.MlflowClient()\n",
        "    exp = mlflow.get_experiment_by_name(exp_name)\n",
        "    if exp is None:\n",
        "        raise ValueError(f\"Experiment '{exp_name}' not found\")\n",
        "\n",
        "    order = \"DESC\" if maximize else \"ASC\"\n",
        "    run = client.search_runs(\n",
        "        [exp.experiment_id],\n",
        "        order_by=[f\"metrics.{metric_key} {order}\"],\n",
        "        max_results=1,\n",
        "    )[0]\n",
        "\n",
        "    # Build a *flat* mapping -------------------------------------------------\n",
        "    flat: Dict[str, Any] = {\"run_id\": run.info.run_id}\n",
        "\n",
        "    # Metrics\n",
        "    for k, v in run.data.metrics.items():\n",
        "        flat[f\"metrics.{k}\"] = v\n",
        "\n",
        "    # Params\n",
        "    for k, v in run.data.params.items():\n",
        "        flat[f\"params.{k}\"] = v\n",
        "\n",
        "    # Tags (optional but handy)\n",
        "    for k, v in run.data.tags.items():\n",
        "        flat[f\"tags.{k}\"] = v\n",
        "\n",
        "    return flat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/backend/ML/mlops/experiment.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/backend/ML/mlops/experiment.py\n",
        "\n",
        "\"\"\"Training utilities with MLflow integration.\"\"\"\n",
        "import mlflow\n",
        "import optuna\n",
        "from typing import Optional\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from .config import RANDOM_STATE, TEST_SIZE\n",
        "from .experiment_utils import setup_mlflow_experiment\n",
        "\n",
        "# Re-export for convenience\n",
        "__all__ = ['setup_mlflow_experiment', 'load_and_prepare_iris_data',\n",
        "           'train_logistic_regression', 'train_random_forest_with_optimization']\n",
        "from .logging import (\n",
        "    log_model_metrics,\n",
        "    log_confusion_matrix,\n",
        "    log_feature_importance,\n",
        "    log_dataset_info,\n",
        "    log_parameters\n",
        ")\n",
        "\n",
        "\n",
        "def load_and_prepare_iris_data(\n",
        "    test_size: float = TEST_SIZE,\n",
        "    random_state: int = RANDOM_STATE\n",
        ") -> DatasetTuple:\n",
        "    iris = load_iris()\n",
        "    X, y = iris.data, iris.target\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=random_state\n",
        "    )\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled  = scaler.transform(X_test)\n",
        "\n",
        "    # ✅ re-wrap as DataFrame so feature names propagate downstream\n",
        "    import pandas as pd\n",
        "    feat_names = iris.feature_names\n",
        "    X_train_df = pd.DataFrame(X_train_scaled, columns=feat_names)\n",
        "    X_test_df  = pd.DataFrame(X_test_scaled,  columns=feat_names)\n",
        "\n",
        "    return (X_train_df, X_test_df, y_train, y_test,\n",
        "            feat_names, list(iris.target_names), scaler)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/backend/ML/mlops/model_registry.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/backend/ML/mlops/model_registry.py\n",
        "\"\"\"MLflow model registry utilities.\"\"\"\n",
        "import mlflow\n",
        "from typing import Optional, Dict, Any\n",
        "from .config import MODEL_NAME, MODEL_STAGE_PRODUCTION\n",
        "\n",
        "\n",
        "def register_model(model_uri: str,\n",
        "                   model_name: Optional[str] = None,\n",
        "                   description: Optional[str] = None) -> str:\n",
        "    \"\"\"\n",
        "    Register a model in the MLflow model registry using the fluent client API.\n",
        "\n",
        "    Args:\n",
        "        model_uri: URI of the model to register\n",
        "        model_name: Name for the registered model\n",
        "        description: Optional description\n",
        "\n",
        "    Returns:\n",
        "        Model version\n",
        "    \"\"\"\n",
        "    name = model_name or MODEL_NAME\n",
        "    client = mlflow.tracking.MlflowClient()\n",
        "\n",
        "    try:\n",
        "        # Create registered model if it doesn't exist\n",
        "        if not client.get_registered_model(name, silent=True):\n",
        "            client.create_registered_model(name)\n",
        "            print(f\"Created new registered model: {name}\")\n",
        "\n",
        "        # Create new version\n",
        "        mv = client.create_model_version(\n",
        "            name=name,\n",
        "            source=model_uri,\n",
        "            description=description\n",
        "        )\n",
        "        print(f\"Created version {mv.version} of model {name}\")\n",
        "        return mv.version\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to register model: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def promote_model_to_stage(model_name: Optional[str] = None,\n",
        "                           version: Optional[str] = None,\n",
        "                           stage: str = MODEL_STAGE_PRODUCTION) -> None:\n",
        "    \"\"\"\n",
        "    Promote a model version to a specific stage using the fluent client.\n",
        "\n",
        "    Args:\n",
        "        model_name: Name of the registered model\n",
        "        version: Version to promote (if None, promotes latest)\n",
        "        stage: Target stage\n",
        "    \"\"\"\n",
        "    name = model_name or MODEL_NAME\n",
        "    client = mlflow.tracking.MlflowClient()\n",
        "\n",
        "    try:\n",
        "        # Get latest version if not specified\n",
        "        if version is None:\n",
        "            latest = client.get_latest_versions(name, stages=[\"None\"])\n",
        "            if not latest:\n",
        "                raise ValueError(f\"No versions found for model {name}\")\n",
        "            version = latest[0].version\n",
        "\n",
        "        # Transition to stage\n",
        "        client.transition_model_version_stage(\n",
        "            name=name,\n",
        "            version=version,\n",
        "            stage=stage\n",
        "        )\n",
        "        print(f\"Promoted model {name} version {version} to {stage}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to promote model: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def load_model_from_registry(model_name: Optional[str] = None,\n",
        "                             stage: str = MODEL_STAGE_PRODUCTION):\n",
        "    \"\"\"\n",
        "    Load a model from the registry by name and stage.\n",
        "\n",
        "    Args:\n",
        "        model_name: Name of the registered model\n",
        "        stage: Stage to load from\n",
        "\n",
        "    Returns:\n",
        "        Loaded model\n",
        "    \"\"\"\n",
        "    name = model_name or MODEL_NAME\n",
        "    model_uri = f\"models:/{name}/{stage}\"\n",
        "\n",
        "    try:\n",
        "        model = mlflow.sklearn.load_model(model_uri)\n",
        "        print(f\"Loaded model {name} from {stage} stage\")\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load model from registry: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def load_model_from_run(run_id: str, artifact_path: str = \"model\"):\n",
        "    \"\"\"\n",
        "    Load a model from a specific run.\n",
        "\n",
        "    Args:\n",
        "        run_id: MLflow run ID\n",
        "        artifact_path: Path to the model artifact\n",
        "\n",
        "    Returns:\n",
        "        Loaded model\n",
        "    \"\"\"\n",
        "    model_uri = f\"runs:/{run_id}/{artifact_path}\"\n",
        "\n",
        "    try:\n",
        "        model = mlflow.sklearn.load_model(model_uri)\n",
        "        print(f\"Loaded model from run {run_id}\")\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load model from run: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def get_model_info(model_name: Optional[str] = None,\n",
        "                   stage: str = MODEL_STAGE_PRODUCTION) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Get information about a registered model using the fluent client.\n",
        "\n",
        "    Args:\n",
        "        model_name: Name of the registered model\n",
        "        stage: Stage to get info for\n",
        "\n",
        "    Returns:\n",
        "        Model information dictionary\n",
        "    \"\"\"\n",
        "    name = model_name or MODEL_NAME\n",
        "    client = mlflow.tracking.MlflowClient()\n",
        "\n",
        "    try:\n",
        "        model_version = client.get_latest_versions(name, stages=[stage])[0]\n",
        "\n",
        "        return {\n",
        "            \"name\": model_version.name,\n",
        "            \"version\": model_version.version,\n",
        "            \"stage\": model_version.current_stage,\n",
        "            \"description\": model_version.description,\n",
        "            \"creation_timestamp\": model_version.creation_timestamp,\n",
        "            \"last_updated_timestamp\": model_version.last_updated_timestamp,\n",
        "            \"run_id\": model_version.run_id\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to get model info: {e}\")\n",
        "        raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/backend/ML/mlops/training.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/backend/ML/mlops/training.py\n",
        "\"\"\"Training utilities with MLflow integration.\"\"\"\n",
        "import mlflow\n",
        "from mlflow import sklearn  # type: ignore\n",
        "from mlflow import models  # type: ignore\n",
        "import optuna\n",
        "from optuna.integration.mlflow import MLflowCallback\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Optional, Tuple, List, Callable, cast, Any, Dict, TypeAlias\n",
        "from numpy.typing import NDArray\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.utils import Bunch\n",
        "from sklearn.pipeline import Pipeline  # NEW\n",
        "\n",
        "from src.backend.ML.mlops.config import RANDOM_STATE, TEST_SIZE\n",
        "from src.backend.ML.mlops.experiment_utils import setup_mlflow_experiment\n",
        "from src.backend.ML.mlops.logging import (\n",
        "    log_full_metrics,\n",
        "    log_confusion_matrix,\n",
        "    log_feature_importance,\n",
        "    log_dataset_info,\n",
        "    log_parameters\n",
        ")\n",
        "from src.backend.ML.mlops.shapiq_utils import log_shapiq_interactions\n",
        "\n",
        "# Type aliases for complex types\n",
        "FloatArray: TypeAlias = NDArray[np.float64]\n",
        "IntArray: TypeAlias = NDArray[np.int64]\n",
        "DatasetTuple: TypeAlias = Tuple[FloatArray, FloatArray, IntArray, IntArray, List[str], List[str], StandardScaler]\n",
        "\n",
        "\n",
        "def load_and_prepare_iris_data(\n",
        "    test_size: float = TEST_SIZE,\n",
        "    random_state: int = RANDOM_STATE\n",
        ") -> DatasetTuple:\n",
        "    \"\"\"\n",
        "    Load and prepare the Iris dataset.\n",
        "\n",
        "    Args:\n",
        "        test_size: Fraction of data to use for testing\n",
        "        random_state: Random state for reproducibility\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (X_train_scaled, X_test_scaled, y_train, y_test,\n",
        "                 feature_names, target_names, scaler)\n",
        "    \"\"\"\n",
        "    # Load dataset\n",
        "    iris: Any = load_iris()\n",
        "    X: NDArray[np.float64] = cast(NDArray[np.float64], iris.data)\n",
        "    y: NDArray[np.int64] = cast(NDArray[np.int64], iris.target)\n",
        "    feature_names: List[str] = list(iris.feature_names)\n",
        "    target_names: List[str] = list(iris.target_names)\n",
        "\n",
        "    # Split data\n",
        "    X_train: NDArray[np.float64]\n",
        "    X_test: NDArray[np.float64]\n",
        "    y_train: NDArray[np.int64]\n",
        "    y_test: NDArray[np.int64]\n",
        "    X_train, X_test, y_train, y_test = cast(\n",
        "        Tuple[NDArray[np.float64], NDArray[np.float64], NDArray[np.int64], NDArray[np.int64]],\n",
        "        train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
        "    )\n",
        "\n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled: NDArray[np.float64] = cast(NDArray[np.float64], scaler.fit_transform(X_train))\n",
        "    X_test_scaled: NDArray[np.float64] = cast(NDArray[np.float64], scaler.transform(X_test))\n",
        "\n",
        "    return (X_train_scaled, X_test_scaled, y_train, y_test,\n",
        "            feature_names, target_names, scaler)\n",
        "\n",
        "\n",
        "# === (A) LOGISTIC REGRESSION (training only, NO dashboard) ================\n",
        "def train_logistic_regression(\n",
        "    X_train, y_train, X_test, y_test, feature_names, target_names,\n",
        "    *, run_name: str = \"lr_baseline\", register: bool = True\n",
        ") -> str:\n",
        "    \"\"\"Train logistic regression model without dashboard integration.\"\"\"\n",
        "    setup_mlflow_experiment()\n",
        "    mlflow.sklearn.autolog(log_models=True)\n",
        "\n",
        "    with mlflow.start_run(run_name=run_name) as run:\n",
        "        log_dataset_info(X_train, X_test, y_train, y_test)\n",
        "        model = LogisticRegression(random_state=RANDOM_STATE, max_iter=1_000).fit(\n",
        "            X_train, y_train\n",
        "        )\n",
        "\n",
        "        y_pred = model.predict(X_test)\n",
        "        log_full_metrics(y_test, y_pred)\n",
        "        log_confusion_matrix(y_test, y_pred, class_names=target_names)\n",
        "\n",
        "        signature = mlflow.models.infer_signature(X_train, model.predict(X_train))\n",
        "        sklearn.log_model(\n",
        "            model, \"model\",\n",
        "            registered_model_name=\"iris_logreg\" if register else None,\n",
        "            signature=signature, input_example=X_test[:5],\n",
        "        )\n",
        "\n",
        "        # SHAP-IQ: compute & log feature interaction values\n",
        "        X_test_df = pd.DataFrame(X_test, columns=feature_names)\n",
        "        log_shapiq_interactions(model, X_test_df, feature_names, max_order=2)\n",
        "\n",
        "        return run.info.run_id\n",
        "\n",
        "\n",
        "def _create_rf_objective(X_train, y_train, X_test, y_test) -> Callable[[optuna.trial.Trial], float]:\n",
        "    \"\"\"Create Optuna objective function for Random Forest optimization.\"\"\"\n",
        "    def objective(trial: optuna.trial.Trial) -> float:\n",
        "        params = {\n",
        "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 10, 200),\n",
        "            \"max_depth\": trial.suggest_int(\"max_depth\", 2, 20),\n",
        "            \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 20),\n",
        "            \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 10),\n",
        "            \"random_state\": RANDOM_STATE,\n",
        "        }\n",
        "        m = RandomForestClassifier(**params).fit(X_train, y_train)\n",
        "        return float(accuracy_score(y_test, m.predict(X_test)))\n",
        "    return objective\n",
        "\n",
        "\n",
        "# === (B) RANDOM-FOREST + Optuna (training only) ===========================\n",
        "def train_random_forest_optimized(\n",
        "    X_train, y_train, X_test, y_test, feature_names, target_names,\n",
        "    *, n_trials: int = 50, run_name: str = \"rf_optimized\", register: bool = True\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Train an Optuna-tuned Random-Forest inside a Pipeline(StandardScaler→RF)\n",
        "    and log the entire pipeline to MLflow so scaling is reproduced at inference.\n",
        "    \"\"\"\n",
        "    setup_mlflow_experiment()\n",
        "    mlflow.sklearn.autolog(disable=True)\n",
        "\n",
        "    with mlflow.start_run(run_name=run_name) as run:\n",
        "        log_dataset_info(X_train, X_test, y_train, y_test)\n",
        "\n",
        "        study = optuna.create_study(direction=\"maximize\")\n",
        "        study.optimize(\n",
        "            _create_rf_objective(X_train, y_train, X_test, y_test),\n",
        "            n_trials=n_trials,\n",
        "            callbacks=[MLflowCallback(\n",
        "                tracking_uri=mlflow.get_tracking_uri(),\n",
        "                metric_name=\"accuracy\", mlflow_kwargs={\"nested\": True}\n",
        "            )],\n",
        "        )\n",
        "\n",
        "        # 🟢 Pipeline with scaler\n",
        "        best_rf = RandomForestClassifier(**study.best_params, random_state=RANDOM_STATE)\n",
        "        pipeline = Pipeline([(\"scaler\", StandardScaler()), (\"rf\", best_rf)])\n",
        "        pipeline.fit(X_train, y_train)\n",
        "\n",
        "        y_pred = pipeline.predict(X_test)\n",
        "        log_full_metrics(y_test, y_pred)\n",
        "        log_confusion_matrix(y_test, y_pred, class_names=target_names)\n",
        "        log_feature_importance(feature_names, best_rf.feature_importances_)\n",
        "        mlflow.log_metric(\"best_accuracy\", study.best_value)\n",
        "\n",
        "        signature = mlflow.models.infer_signature(X_train, pipeline.predict(X_train))\n",
        "        sklearn.log_model(\n",
        "            pipeline, \"model\",\n",
        "            registered_model_name=\"iris_random_forest\" if register else None,\n",
        "            signature=signature, input_example=X_test[:5],\n",
        "        )\n",
        "\n",
        "        X_test_df = pd.DataFrame(X_test, columns=feature_names)\n",
        "        log_shapiq_interactions(best_rf, X_test_df, feature_names, max_order=2)\n",
        "\n",
        "        return run.info.run_id\n",
        "\n",
        "\n",
        "# === (C) ONE-STOP helper: train both models ===============================\n",
        "def run_all_trainings(*,\n",
        "    test_size: float = TEST_SIZE, random_state: int = RANDOM_STATE, n_trials: int = 50) -> None:\n",
        "    \"\"\"Train both logistic regression and random forest models.\"\"\"\n",
        "    X_tr, X_te, y_tr, y_te, feats, tgts, _ = load_and_prepare_iris_data(\n",
        "        test_size, random_state\n",
        "    )\n",
        "    train_logistic_regression(\n",
        "        X_tr, y_tr, X_te, y_te, feats, tgts, run_name=\"lr_baseline\"\n",
        "    )\n",
        "    train_random_forest_optimized(\n",
        "        X_tr, y_tr, X_te, y_te, feats, tgts,\n",
        "        n_trials=n_trials, run_name=\"rf_optimized\"\n",
        "    )\n",
        "\n",
        "\n",
        "# === (D) Robust comparator ===============================================\n",
        "def compare_models(\n",
        "    experiment_name: Optional[str] = None,\n",
        "    metric_key: str = \"accuracy\",\n",
        "    maximize: bool = True,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Print the best run according to *metric_key* while gracefully\n",
        "    falling-back to common alternates when the preferred key is missing.\n",
        "    \"\"\"\n",
        "    from .experiment_utils import get_best_run\n",
        "\n",
        "    fallback_keys = [\"accuracy_score\", \"best_accuracy\"]\n",
        "    try:\n",
        "        best = get_best_run(experiment_name, metric_key, maximize)\n",
        "        rid = best[\"run_id\"]\n",
        "\n",
        "        # choose first key that exists\n",
        "        score = best.get(f\"metrics.{metric_key}\")\n",
        "        if score is None:\n",
        "            for alt in fallback_keys:\n",
        "                score = best.get(f\"metrics.{alt}\")\n",
        "                if score is not None:\n",
        "                    metric_key = alt\n",
        "                    break\n",
        "\n",
        "        model_type = best.get(\"params.model_type\", \"unknown\")\n",
        "        print(f\"🏆 Best run: {rid}\")\n",
        "        print(f\"📈 {metric_key}: {score if score is not None else 'N/A'}\")\n",
        "        print(f\"🔖 Model type: {model_type}\")\n",
        "    except Exception as err:\n",
        "        print(f\"❌ Error comparing models: {err}\")\n",
        "\n",
        "\n",
        "# Legacy compatibility functions (with dashboard support)\n",
        "train_logistic_regression_autolog = train_logistic_regression\n",
        "train_random_forest_with_optimization = train_random_forest_optimized\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_all_trainings()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/backend/ML/mlops/utils_training_guards.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/backend/ML/mlops/utils_training_guards.py\n",
        "# src/backend/ML/mlops/utils_training_guards.py\n",
        "\n",
        "import logging\n",
        "import shutil\n",
        "\n",
        "try:\n",
        "    # PyTensor exposes its config flags in the configdefaults module\n",
        "    from pytensor.configdefaults import config as pconfig\n",
        "except ImportError as exc:\n",
        "    # If PyTensor isn’t installed or the API changed, disable C-ops fallback\n",
        "    logging.getLogger(__name__).warning(\n",
        "        \"Could not import pytensor.configdefaults.config; \"\n",
        "        \"skipping compiler checks.\"\n",
        "    )\n",
        "    pconfig = None\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def ensure_compiler() -> None:\n",
        "    \"\"\"\n",
        "    Check for the configured C++ compiler (config.cxx).\n",
        "    If it's set but not on PATH, disable C-ops to force Python fallback.\n",
        "    \"\"\"\n",
        "    if not pconfig:\n",
        "        # Nothing to do if config isn't available\n",
        "        return\n",
        "\n",
        "    cxx = getattr(pconfig, \"cxx\", \"\")\n",
        "    if cxx and shutil.which(cxx) is None:\n",
        "        logger.warning(\n",
        "            f\"Configured compiler '{cxx}' not found in PATH. \"\n",
        "            \"Disabling PyTensor C-ops (falling back to pure-Python).\"\n",
        "        )\n",
        "        # Disable C++ compilation\n",
        "        pconfig.cxx = \"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/backend/ML/mlops/training_bayes.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/backend/ML/mlops/training_bayes.py\n",
        "from __future__ import annotations\n",
        "import os\n",
        "import shutil\n",
        "import logging\n",
        "\n",
        "try:\n",
        "    import pymc as pm\n",
        "    import arviz as az\n",
        "except ImportError as exc:\n",
        "    raise ImportError(\"PyMC and ArviZ are required for Bayesian model training\") from exc\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from src.backend.ML.mlops.utils_training_guards import ensure_compiler\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "_BAYES_MODEL_NAME = \"iris_bayes_logreg\"\n",
        "\n",
        "def train_bayes_logreg(\n",
        "    debug: bool = False,\n",
        "    *,\n",
        "    draws: int = 1000,\n",
        "    tune: int = 500,\n",
        "    run_name: str = \"bayes_logreg\",\n",
        "    register: bool = True,\n",
        ") -> tuple[pm.Model, \"az.InferenceData\", str]:\n",
        "    \"\"\"Train a *Bayesian* logistic-regression and log a custom **pyfunc** model.\n",
        "\n",
        "    The original implementation attempted to pass\n",
        "    ``pm.sample_posterior_predictive`` directly as *python_model*, which\n",
        "    violates MLflow's requirement that the callable accepts **exactly one**\n",
        "    positional argument.  We now wrap the PyMC model and its posterior inside\n",
        "    a lightweight :class:`mlflow.pyfunc.PythonModel` so that the logged model\n",
        "    exposes a single-argument ``predict()`` API compatible with MLflow's\n",
        "    runtime.\n",
        "    \"\"\"\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # 0️⃣  House-keeping & experiment setup\n",
        "    # ------------------------------------------------------------------\n",
        "    from src.backend.ML.mlops.experiment_utils import setup_mlflow_experiment\n",
        "    import mlflow\n",
        "\n",
        "    ensure_compiler()  # gracefully disable C-ops if compiler missing\n",
        "    setup_mlflow_experiment()\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # 1️⃣  Data loading & standardisation (Breast-Cancer – binary)\n",
        "    # ------------------------------------------------------------------\n",
        "    data = load_breast_cancer()\n",
        "    X_raw = data.data.astype(float)\n",
        "    y_raw = data.target.astype(int)\n",
        "\n",
        "    X_std = (X_raw - X_raw.mean(axis=0)) / X_raw.std(axis=0)\n",
        "    n_feat = X_std.shape[1]\n",
        "\n",
        "    # Prior centred on empirical log-odds to aid convergence\n",
        "    p0, p1 = (y_raw == 0).mean(), (y_raw == 1).mean()\n",
        "    intercept_prior = float(\"-inf\") if p0 == 0 else np.log(p1 / p0)\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # 2️⃣  Model definition + sampling\n",
        "    # ------------------------------------------------------------------\n",
        "    with mlflow.start_run(run_name=run_name) as run:\n",
        "        with pm.Model() as model:\n",
        "            pm.Data(\"X_shared\", X_std)\n",
        "            pm.Data(\"y_obs\", y_raw)\n",
        "\n",
        "            alpha = pm.Normal(\"alpha\", mu=intercept_prior, sigma=2)\n",
        "            beta  = pm.Normal(\"beta\",  mu=0, sigma=1, shape=n_feat)\n",
        "\n",
        "            logits = alpha + pm.math.dot(X_std, beta)\n",
        "            pm.Bernoulli(\"y\", p=pm.math.sigmoid(logits), observed=y_raw)\n",
        "\n",
        "            idata = pm.sample(\n",
        "                draws=draws,\n",
        "                tune=tune,\n",
        "                chains=2,\n",
        "                cores=1,\n",
        "                target_accept=0.9,\n",
        "                progressbar=not debug,\n",
        "            )\n",
        "\n",
        "        # ------------------------------------------------------------------\n",
        "        # 3️⃣  MLflow artefacts – custom **PyFunc** wrapper\n",
        "        # ------------------------------------------------------------------\n",
        "        signature = mlflow.models.infer_signature(X_std, y_raw)\n",
        "\n",
        "        import mlflow.pyfunc\n",
        "\n",
        "        class _BayesLogRegPyFunc(mlflow.pyfunc.PythonModel):\n",
        "            \"\"\"Minimal adapter exposing a single-argument *predict* method.\"\"\"\n",
        "\n",
        "            def __init__(self, pymc_model: pm.Model, posterior):\n",
        "                self._model = pymc_model\n",
        "                self._posterior = posterior\n",
        "\n",
        "            def predict(self, context, model_input):  # noqa: D401 – MLflow API\n",
        "                import numpy as _np\n",
        "                import pymc as _pm\n",
        "                X_arr = _np.asarray(model_input, dtype=float)\n",
        "                with self._model:\n",
        "                    _pm.set_data({\"X_shared\": X_arr})\n",
        "                    ppc = _pm.sample_posterior_predictive(\n",
        "                        self._posterior,\n",
        "                        progressbar=False,\n",
        "                        predictions=True,\n",
        "                    )\n",
        "                # Return the *mean* class-1 probability per row (shape = (n,))\n",
        "                phat = ppc.predictions[\"y\"].mean((\"chain\", \"draw\")).values\n",
        "                return phat\n",
        "\n",
        "        mlflow.pyfunc.log_model(\n",
        "            artifact_path=\"model\",\n",
        "            python_model=_BayesLogRegPyFunc(model, idata.posterior),\n",
        "            input_example=X_std[:5],\n",
        "            signature=signature,\n",
        "            pip_requirements=[\"pymc\", \"arviz\"],\n",
        "            registered_model_name=_BAYES_MODEL_NAME if register else None,\n",
        "        )\n",
        "\n",
        "        # ── Tag + Promotion ─────────────────────────────────────────────\n",
        "        mlflow.set_tag(\"model_name\", _BAYES_MODEL_NAME)\n",
        "        if register:\n",
        "            from mlflow.client import MlflowClient\n",
        "            client = MlflowClient()\n",
        "            mv = client.get_latest_versions(_BAYES_MODEL_NAME, stages=[\"None\"])[0]\n",
        "            client.transition_model_version_stage(\n",
        "                name=_BAYES_MODEL_NAME,\n",
        "                version=mv.version,\n",
        "                stage=\"Production\",\n",
        "                archive_existing_versions=True,\n",
        "            )\n",
        "\n",
        "        mlflow.log_metric(\"n_draws\", int(draws))\n",
        "        return model, idata, run.info.run_id\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if os.path.isdir(\"mlruns\"):\n",
        "        logger.info(\"Removing existing mlruns directory...\")\n",
        "        shutil.rmtree(\"mlruns\")\n",
        "\n",
        "    model, idata, run_id = train_bayes_logreg(debug=True,\n",
        "                                              draws=50,\n",
        "                                              tune=25,\n",
        "                                              run_name=\"breast_cancer_bayes_logreg\",\n",
        "                                              register=True)\n",
        "    summary_df = az.summary(idata, var_names=[\"alpha\", \"beta\"])\n",
        "    print(summary_df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/backend/ML/mlops/explainer.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/backend/ML/mlops/explainer.py\n",
        "from __future__ import annotations\n",
        "import os\n",
        "import socket\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from typing import Any, Sequence, Optional\n",
        "from contextlib import closing\n",
        "\n",
        "import mlflow\n",
        "import psutil  # lightweight; already added to pyproject deps\n",
        "from sklearn.utils.multiclass import type_of_target\n",
        "from explainerdashboard import (\n",
        "    ClassifierExplainer,\n",
        "    RegressionExplainer,\n",
        "    ExplainerDashboard,\n",
        ")\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "__all__ = [\"build_and_log_dashboard\", \"load_dashboard_yaml\", \"dashboard_best_run\", \"_first_free_port\", \"_port_details\"]\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "def _port_details(port: int) -> str:\n",
        "    \"\"\"\n",
        "    Return a one-line string with PID & cmdline of the process\n",
        "    listening on *port*, or '' if none / not discoverable.\n",
        "    \"\"\"\n",
        "    for c in psutil.net_connections(kind=\"tcp\"):\n",
        "        if c.status == psutil.CONN_LISTEN and c.laddr and c.laddr.port == port:\n",
        "            try:\n",
        "                p = psutil.Process(c.pid)\n",
        "                return f\"[PID {p.pid} – {p.name()}] cmd={p.cmdline()}\"\n",
        "            except psutil.Error:\n",
        "                return f\"[PID {c.pid}] (no detail)\"\n",
        "    return \"\"\n",
        "\n",
        "def _first_free_port(start: int = 8050, tries: int = 50) -> int:\n",
        "    \"\"\"Return first free TCP port ≥ *start* on localhost.\"\"\"\n",
        "    for port in range(start, start + tries):\n",
        "        try:\n",
        "            with closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as s:\n",
        "                s.settimeout(0.05)\n",
        "                s.bind((\"127.0.0.1\", port))\n",
        "                return port\n",
        "        except OSError:\n",
        "            # Port is in use, try next one\n",
        "            continue\n",
        "    raise RuntimeError(\"⚠️  No free ports found in range\")\n",
        "\n",
        "def _next_free_port(start: int = 8050, tries: int = 50) -> int:\n",
        "    \"\"\"Return the first free TCP port ≥ *start*. (Alias for backward compatibility)\"\"\"\n",
        "    return _first_free_port(start, tries)\n",
        "\n",
        "def _port_in_use(port: int) -> bool:\n",
        "    \"\"\"Check if a port is already in use on any interface.\"\"\"\n",
        "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "        s.settimeout(0.05)\n",
        "        # Check both localhost and 0.0.0.0 to be thorough\n",
        "        try:\n",
        "            # First check localhost (127.0.0.1)\n",
        "            if s.connect_ex((\"127.0.0.1\", port)) == 0:\n",
        "                return True\n",
        "            # Also check if anything is bound to all interfaces\n",
        "            if s.connect_ex((\"0.0.0.0\", port)) == 0:\n",
        "                return True\n",
        "        except (socket.gaierror, OSError):\n",
        "            # If we can't connect, assume port is free\n",
        "            pass\n",
        "        return False\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# -------------------------------------------------------------- #\n",
        "#  src/mlops/explainer.py (only this function changed)           #\n",
        "# -------------------------------------------------------------- #\n",
        "def build_and_log_dashboard(\n",
        "    model: Any,\n",
        "    X_test,\n",
        "    y_test,\n",
        "    *,\n",
        "    # ---- explainer kwargs (unchanged) -------------------------\n",
        "    cats: Optional[Sequence[str]] = None,\n",
        "    idxs: Optional[Sequence[Any]] = None,\n",
        "    descriptions: Optional[dict[str, str]] = None,\n",
        "    target: Optional[str] = None,\n",
        "    labels: Optional[Sequence[str]] = None,\n",
        "    X_background=None,\n",
        "    model_output: str = \"probability\",\n",
        "    shap: str = \"guess\",\n",
        "    shap_interaction: bool = True,\n",
        "    simple: bool = False,\n",
        "    mode: str = \"dash\",         # 🆕 safest default for docker\n",
        "    title: str = \"Model Explainer\",\n",
        "    # ---- infra -----------------------------------------------\n",
        "    run: mlflow.ActiveRun | None = None,\n",
        "    port: int | None = None,\n",
        "    serve: bool = False,\n",
        "    server_backend: str = \"waitress\",   # 🆕 waitress|gunicorn|jupyterdash\n",
        "    conflict_strategy: str = \"next\",\n",
        "    max_tries: int = 20,\n",
        "    save_yaml: bool = True,\n",
        "    output_dir: os.PathLike | str | None = None,\n",
        ") -> Path:\n",
        "    \"\"\"\n",
        "    Build + (optionally) serve the dashboard.\n",
        "\n",
        "    server_backend\n",
        "        'waitress'    – production WSGI server (binds 0.0.0.0)\n",
        "        'gunicorn'    – spawn via subprocess (needs gunicorn installed)\n",
        "        'jupyterdash' – fallback; use only for notebook demos\n",
        "    \"\"\"\n",
        "    # ------------ build explainer (unchanged) ------------------\n",
        "    problem = type_of_target(y_test)\n",
        "    ExplainerCls = RegressionExplainer if problem.startswith(\"continuous\") else ClassifierExplainer\n",
        "    expl_kwargs = dict(\n",
        "        cats=cats, idxs=idxs, descriptions=descriptions, target=target,\n",
        "        labels=labels, X_background=X_background, model_output=model_output, shap=shap,\n",
        "    )\n",
        "    expl_kwargs = {k: v for k, v in expl_kwargs.items() if v is not None}\n",
        "    explainer = ExplainerCls(model, X_test, y_test, **expl_kwargs)\n",
        "\n",
        "    dash = ExplainerDashboard(\n",
        "        explainer, title=title, shap_interaction=shap_interaction,\n",
        "        simple=simple, mode=mode,\n",
        "    )\n",
        "\n",
        "    out_dir = Path(output_dir or \".\"); out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    html_path = out_dir / \"explainer_dashboard.html\"; dash.save_html(html_path); mlflow.log_artifact(str(html_path))\n",
        "    if save_yaml:\n",
        "        yaml = out_dir / \"dashboard.yaml\"; dash.to_yaml(yaml); mlflow.log_artifact(str(yaml))\n",
        "\n",
        "    # ------------ serve ----------------------------------------\n",
        "    if not serve:\n",
        "        return html_path\n",
        "\n",
        "    chosen = port or _first_free_port()\n",
        "    attempts = 0\n",
        "    while _port_in_use(chosen):\n",
        "        if conflict_strategy == \"raise\":\n",
        "            raise RuntimeError(f\"Port {chosen} in use {_port_details(chosen)}\")\n",
        "        if conflict_strategy == \"kill\":\n",
        "            pid = int((_port_details(chosen) or \"PID 0\").split()[1]); psutil.Process(pid).terminate()\n",
        "            break\n",
        "        attempts += 1\n",
        "        if attempts >= max_tries:\n",
        "            raise RuntimeError(f\"No free port after {max_tries} tries\")\n",
        "        chosen += 1\n",
        "\n",
        "    logging.info(\"🌐 Dashboard on http://0.0.0.0:%s via %s\", chosen, server_backend)\n",
        "\n",
        "    if server_backend == \"waitress\":\n",
        "        dash.run(chosen, host=\"0.0.0.0\", use_waitress=True, mode=\"dash\")\n",
        "    elif server_backend == \"gunicorn\":\n",
        "        import subprocess, shlex\n",
        "        cmd = f\"gunicorn -w 3 -b 0.0.0.0:{chosen} dashboard:app\"\n",
        "        subprocess.Popen(shlex.split(cmd), cwd=str(out_dir))\n",
        "    else:  # jupyterdash\n",
        "        dash.run(chosen, host=\"0.0.0.0\")\n",
        "\n",
        "    return html_path\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "def load_dashboard_yaml(path: os.PathLike | str) -> ExplainerDashboard:\n",
        "    \"\"\"Reload a YAML config – unchanged but kept for public API.\"\"\"\n",
        "    return ExplainerDashboard.from_config(path)\n",
        "\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "def dashboard_best_run(metric: str = \"accuracy\",\n",
        "                       maximize: bool = True,\n",
        "                       *, port: int | None = None) -> None:\n",
        "    \"\"\"\n",
        "    Load the *best* run (by `metric`) from the active experiment and\n",
        "    launch an ExplainerDashboard **once** for that model.\n",
        "\n",
        "    Example\n",
        "    -------\n",
        "    >>> from mlops.explainer import dashboard_best_run\n",
        "    >>> dashboard_best_run(\"accuracy\")      # opens http://0.0.0.0:8050\n",
        "    \"\"\"\n",
        "    from .experiment_utils import get_best_run\n",
        "    from .model_registry  import load_model_from_run\n",
        "    from sklearn.datasets import load_iris\n",
        "    import pandas as pd\n",
        "\n",
        "    best = get_best_run(metric_key=metric, maximize=maximize)\n",
        "    run_id = best[\"run_id\"]\n",
        "    model  = load_model_from_run(run_id)\n",
        "\n",
        "    iris = load_iris()\n",
        "    X_df  = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "    build_and_log_dashboard(\n",
        "        model, X_df, iris.target,\n",
        "        labels=list(iris.target_names),\n",
        "        run=None, serve=True, port=port or 8050\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/backend/ML/mlops/utils.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/backend/ML/mlops/utils.py\n",
        "import os\n",
        "# Add near the top of utils.py\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import inspect\n",
        "\n",
        "def add_project_root_to_sys_path(levels_up: int = 2) -> Path:\n",
        "    \"\"\"\n",
        "    Ensure the repository root (default: two directories up) is on sys.path.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Path\n",
        "        The absolute Path object pointing to the directory inserted.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        here = Path(__file__).resolve()\n",
        "    except NameError:           # running in Jupyter / IPython\n",
        "        # Use the file of the *caller* if possible,\n",
        "        # otherwise fall back to the current working directory.\n",
        "        caller = inspect.stack()[1].filename\n",
        "        here = Path(caller).resolve() if caller != \"<stdin>\" else Path.cwd()\n",
        "\n",
        "    root = here.parents[levels_up]\n",
        "    sys.path.insert(0, str(root))\n",
        "    return root\n",
        "\n",
        "\n",
        "_added_src_flag: bool = False          # module-level cache\n",
        "\n",
        "def project_root() -> Path:\n",
        "    \"\"\"\n",
        "    Return the absolute path to the repo root *without* relying on __file__.\n",
        "\n",
        "    • If running from a .py file, use that file's parent/parent (…/src/..)\n",
        "    • If running interactively (no __file__), fall back to CWD.\n",
        "    \"\"\"\n",
        "    if \"__file__\" in globals():\n",
        "        return Path(__file__).resolve().parent.parent\n",
        "    return Path.cwd()\n",
        "\n",
        "def ensure_src_on_path(verbose: bool = True) -> None:\n",
        "    \"\"\"\n",
        "    Ensure <repo-root>/src is the *first* entry in sys.path exactly once.\n",
        "    The verbose flag prints the helper line the first time only.\n",
        "    \"\"\"\n",
        "    import sys\n",
        "    global _added_src_flag\n",
        "    root = project_root()\n",
        "    src_path = root / \"src\"\n",
        "\n",
        "    if str(src_path) not in sys.path:\n",
        "        sys.path.insert(0, str(src_path))\n",
        "        if verbose and not _added_src_flag:\n",
        "            print(f\"🔧 Added {src_path} to sys.path\")\n",
        "        _added_src_flag = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/backend/ML/mlops/shapiq_utils.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/backend/ML/mlops/shapiq_utils.py\n",
        "\"\"\"\n",
        "SHAP-IQ (Shapley Interaction) utilities for MLflow integration.\n",
        "\n",
        "This module provides functions to compute and log Shapley interaction values\n",
        "for machine learning models. Shapley interactions help understand how features\n",
        "work together to influence model predictions.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import mlflow\n",
        "from shapiq import TabularExplainer\n",
        "from typing import Optional, Sequence, Union\n",
        "import logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\",\n",
        "                        message=\"Not all budget is required due to the border-trick\",\n",
        "                        category=UserWarning,\n",
        "                        module=r\"^shapiq\\.\")\n",
        "\n",
        "\n",
        "def compute_shapiq_interactions(\n",
        "    model,\n",
        "    X: pd.DataFrame,\n",
        "    feature_names: Sequence[str],\n",
        "    max_order: int = 2,\n",
        "    budget: int = 256,\n",
        "    n_samples: Optional[int] = None,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Robust wrapper around shapiq.TabularExplainer to return a tidy DataFrame\n",
        "    with Shapley-interaction values.  Handles the two public APIs:\n",
        "      •  .dict_values   (mapping)\n",
        "      •  .values        (np.ndarray)  →  use  .to_dict()\n",
        "    \"\"\"\n",
        "    logger.info(\n",
        "        \"Computing SHAP-IQ (max_order=%s, budget=%s, n_samples=%s)\",\n",
        "        max_order,\n",
        "        budget,\n",
        "        n_samples,\n",
        "    )\n",
        "\n",
        "    X_sample = (\n",
        "        X.sample(n=n_samples, random_state=42) if n_samples and len(X) > n_samples else X\n",
        "    )\n",
        "\n",
        "    explainer = TabularExplainer(\n",
        "        model=model,\n",
        "        data=X_sample.values,\n",
        "        index=\"k-SII\",\n",
        "        max_order=max_order,\n",
        "    )\n",
        "\n",
        "    rows: list[dict[str, Any]] = []\n",
        "    for i, vec in enumerate(X_sample.values):\n",
        "        try:\n",
        "            iv = explainer.explain(vec, budget=budget)\n",
        "\n",
        "            # --- unify both APIs ------------------------------------------------\n",
        "            if hasattr(iv, \"dict_values\"):                    # shapiq ≥ 0.4\n",
        "                items = iv.dict_values.items()\n",
        "            elif hasattr(iv, \"to_dict\"):                      # fallback\n",
        "                items = iv.to_dict().items()\n",
        "            else:\n",
        "                # last resort – try attribute access\n",
        "                items = dict(iv.values).items()\n",
        "\n",
        "            for combo, val in items:\n",
        "                rows.append(\n",
        "                    {\n",
        "                        \"sample_idx\": i,\n",
        "                        \"combination\": combo,\n",
        "                        \"value\": float(val),\n",
        "                        \"order\": len(combo),\n",
        "                        \"feature_names\": tuple(feature_names[j] for j in combo)\n",
        "                        if combo\n",
        "                        else (),\n",
        "                    }\n",
        "                )\n",
        "        except Exception as exc:  # noqa: BLE001\n",
        "            logger.warning(\"SHAP-IQ failed on sample %s: %s\", i, exc)\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    logger.info(\"✓ %s interaction rows computed\", len(df))\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "def log_shapiq_interactions(\n",
        "    model,\n",
        "    X: pd.DataFrame,\n",
        "    feature_names: Sequence[str],\n",
        "    max_order: int = 2,\n",
        "    top_n: int = 10,\n",
        "    budget: int = 256,\n",
        "    n_samples: Optional[int] = None,\n",
        "    output_path: Optional[str] = None\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Compute Shapley interaction values and log them to MLflow.\n",
        "\n",
        "    This function:\n",
        "    1. Computes interactions using compute_shapiq_interactions\n",
        "    2. Logs the top N interactions as MLflow metrics\n",
        "    3. Saves the full interaction table as CSV and logs as artifact\n",
        "\n",
        "    Args:\n",
        "        model: Trained sklearn-like model.\n",
        "        X: DataFrame of features.\n",
        "        feature_names: List of feature column names.\n",
        "        max_order: Maximum interaction order (default: 2).\n",
        "        top_n: Number of top interactions to log as metrics (default: 10).\n",
        "        budget: Evaluation budget for interaction approximation (default: 256).\n",
        "        n_samples: If provided, sample this many rows for computation.\n",
        "        output_path: Optional path for CSV output (default: \"shapiq_interactions.csv\").\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting SHAP-IQ interaction logging\")\n",
        "\n",
        "    # Compute interactions\n",
        "    df = compute_shapiq_interactions(\n",
        "        model, X, feature_names, max_order, budget, n_samples\n",
        "    )\n",
        "\n",
        "    if df.empty:\n",
        "        logger.warning(\"No interactions computed - skipping logging\")\n",
        "        return\n",
        "\n",
        "    # Aggregate: mean absolute value per combination across all samples\n",
        "    agg = (\n",
        "        df.groupby(['combination', 'feature_names', 'order'])['value']\n",
        "          .apply(lambda x: x.abs().mean())\n",
        "          .reset_index()\n",
        "          .sort_values('value', ascending=False)\n",
        "    )\n",
        "\n",
        "    # Log summary statistics\n",
        "    mlflow.log_metric(\"shapiq_total_interactions\", len(df))\n",
        "    mlflow.log_metric(\"shapiq_unique_combinations\", len(agg))\n",
        "    mlflow.log_metric(\"shapiq_max_order\", max_order)\n",
        "    mlflow.log_metric(\"shapiq_samples_analyzed\", len(X) if n_samples is None else min(n_samples, len(X)))\n",
        "\n",
        "    # Log top N interactions as metrics\n",
        "    logger.info(f\"Logging top {top_n} interactions as MLflow metrics\")\n",
        "    for idx, row in agg.head(top_n).iterrows():\n",
        "        combo = row['combination']\n",
        "        feature_combo = row['feature_names']\n",
        "        value = row['value']\n",
        "        order = row['order']\n",
        "\n",
        "        # Create metric name from feature names or indices\n",
        "        if feature_combo:\n",
        "            name = f\"shapiq_order{order}_{'_x_'.join(feature_combo)}\"\n",
        "        else:\n",
        "            name = f\"shapiq_order{order}_{'_'.join(map(str, combo))}\"\n",
        "\n",
        "        # Sanitize metric name (MLflow has restrictions)\n",
        "        name = name.replace(' ', '_').replace('(', '').replace(')', '').replace(',', '_')[:250]\n",
        "\n",
        "        mlflow.log_metric(name, float(value))\n",
        "\n",
        "    # Log order-specific summaries\n",
        "    order_summary = df.groupby('order')['value'].agg(['count', 'mean', 'std']).fillna(0)\n",
        "    for order_val in order_summary.index:\n",
        "        mlflow.log_metric(f\"shapiq_order{order_val}_count\", order_summary.loc[order_val, 'count'])\n",
        "        mlflow.log_metric(f\"shapiq_order{order_val}_mean_abs\", abs(order_summary.loc[order_val, 'mean']))\n",
        "        if order_summary.loc[order_val, 'std'] > 0:\n",
        "            mlflow.log_metric(f\"shapiq_order{order_val}_std\", order_summary.loc[order_val, 'std'])\n",
        "\n",
        "    # Save and log full DataFrame as artifact\n",
        "    output_file = output_path or \"shapiq_interactions.csv\"\n",
        "\n",
        "    try:\n",
        "        # Add readable feature names to the full DataFrame\n",
        "        df_export = df.copy()\n",
        "        df_export['feature_names_str'] = df_export['feature_names'].apply(lambda x: ' x '.join(x) if x else 'baseline')\n",
        "\n",
        "        df_export.to_csv(output_file, index=False)\n",
        "        mlflow.log_artifact(output_file)\n",
        "        logger.info(f\"Logged SHAP-IQ interactions artifact: {output_file}\")\n",
        "\n",
        "        # Also create and log a summary file\n",
        "        summary_file = output_path.replace('.csv', '_summary.csv') if output_path else \"shapiq_interactions_summary.csv\"\n",
        "        agg_export = agg.copy()\n",
        "        agg_export['feature_names_str'] = agg_export['feature_names'].apply(lambda x: ' x '.join(x) if x else 'baseline')\n",
        "        agg_export.to_csv(summary_file, index=False)\n",
        "        mlflow.log_artifact(summary_file)\n",
        "        logger.info(f\"Logged SHAP-IQ summary artifact: {summary_file}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error saving SHAP-IQ artifacts: {e}\")\n",
        "\n",
        "    logger.info(\"SHAP-IQ interaction logging completed\")\n",
        "\n",
        "\n",
        "def get_top_interactions(\n",
        "    shapiq_df: pd.DataFrame,\n",
        "    top_n: int = 10,\n",
        "    order: Optional[int] = None\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Extract top interactions from a SHAP-IQ DataFrame.\n",
        "\n",
        "    Args:\n",
        "        shapiq_df: DataFrame returned by compute_shapiq_interactions.\n",
        "        top_n: Number of top interactions to return.\n",
        "        order: If provided, filter to interactions of this order only.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with top interactions, aggregated across samples.\n",
        "    \"\"\"\n",
        "    df = shapiq_df.copy()\n",
        "\n",
        "    if order is not None:\n",
        "        df = df[df['order'] == order]\n",
        "\n",
        "    if df.empty:\n",
        "        return df\n",
        "\n",
        "    # Aggregate and sort by absolute mean value\n",
        "    agg = (\n",
        "        df.groupby(['combination', 'feature_names', 'order'])['value']\n",
        "          .agg(['mean', 'std', 'count'])\n",
        "          .reset_index()\n",
        "    )\n",
        "    agg['abs_mean'] = agg['mean'].abs()\n",
        "    agg = agg.sort_values('abs_mean', ascending=False)\n",
        "\n",
        "    return agg.head(top_n)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/backend/ML/examples/shapiq_demo.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/backend/ML/examples/shapiq_demo.py\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "SHAP-IQ Integration Demo\n",
        "\n",
        "This script demonstrates the new SHAP-IQ (Shapley Interaction) functionality\n",
        "integrated into the MLOps pipeline. It shows how Shapley interaction values\n",
        "are computed and logged alongside regular model metrics.\n",
        "\n",
        "Usage:\n",
        "    python src/examples/shapiq_demo.py\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "import logging\n",
        "\n",
        "# ─── Path setup ─────────────────────────────────────────────────────────────\n",
        "from src.backend.ML.mlops.utils import add_project_root_to_sys_path\n",
        "PROJECT_ROOT = add_project_root_to_sys_path(levels_up=2)  # safe in both .py and interactive :contentReference[oaicite:8]{index=8}\n",
        "\n",
        "# ─── Imports ────────────────────────────────────────────────────────────────\n",
        "from src.backend.ML.mlops.training import (\n",
        "    load_and_prepare_iris_data,\n",
        "    train_logistic_regression,\n",
        "    train_random_forest_optimized\n",
        ")\n",
        "from src.backend.ML.mlops.shapiq_utils import (\n",
        "    compute_shapiq_interactions,\n",
        "    log_shapiq_interactions,\n",
        "    get_top_interactions\n",
        ")\n",
        "from src.backend.ML.mlops.experiment_utils import setup_mlflow_experiment, get_best_run\n",
        "import mlflow\n",
        "import pandas as pd\n",
        "\n",
        "# ─── Logging Setup ─────────────────────────────────────────────────────────\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def demo_standalone_shapiq():\n",
        "    \"\"\"Demonstrate standalone SHAP-IQ computation without MLflow logging.\"\"\"\n",
        "    print(\"🔬 SHAP-IQ Standalone Demo\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Load data and train a simple model\n",
        "    X_train, X_test, y_train, y_test, feature_names, target_names, _ = load_and_prepare_iris_data()\n",
        "\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    model = RandomForestClassifier(n_estimators=20, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    print(f\"✓ Trained RandomForest on {len(X_train)} samples\")\n",
        "    print(f\"✓ Test accuracy: {model.score(X_test, y_test):.3f}\")\n",
        "\n",
        "    # Compute SHAP-IQ interactions\n",
        "    X_test_df = pd.DataFrame(X_test, columns=feature_names)\n",
        "    print(f\"\\n🧮 Computing SHAP-IQ interactions...\")\n",
        "\n",
        "    shapiq_df = compute_shapiq_interactions(\n",
        "        model,\n",
        "        X_test_df.head(10),  # Use subset for demo\n",
        "        feature_names,\n",
        "        max_order=2,\n",
        "        budget=128\n",
        "    )\n",
        "\n",
        "    if not shapiq_df.empty:\n",
        "        print(f\"✓ Computed {len(shapiq_df)} interaction values\")\n",
        "\n",
        "        # Show top interactions\n",
        "        top_interactions = get_top_interactions(shapiq_df, top_n=5)\n",
        "        print(f\"\\n🏆 Top 5 Feature Interactions:\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        for idx, row in top_interactions.iterrows():\n",
        "            feature_combo = ' × '.join(row['feature_names'])\n",
        "            if not feature_combo:\n",
        "                feature_combo = \"baseline\"\n",
        "            print(f\"  {feature_combo:30} | Order {row['order']} | {row['abs_mean']:.4f}\")\n",
        "\n",
        "        # Show order breakdown\n",
        "        order_counts = shapiq_df['order'].value_counts().sort_index()\n",
        "        print(f\"\\n📊 Interaction Order Breakdown:\")\n",
        "        for order, count in order_counts.items():\n",
        "            if order == 0:\n",
        "                print(f\"  Order {order} (main effects):     {count:4d} values\")\n",
        "            elif order == 1:\n",
        "                print(f\"  Order {order} (individual):       {count:4d} values\")\n",
        "            elif order == 2:\n",
        "                print(f\"  Order {order} (pairwise):         {count:4d} values\")\n",
        "            else:\n",
        "                print(f\"  Order {order} (higher-order):     {count:4d} values\")\n",
        "    else:\n",
        "        print(\"⚠️  No interactions computed (this can happen with simple models/data)\")\n",
        "\n",
        "\n",
        "def demo_integrated_training():\n",
        "    \"\"\"Demonstrate SHAP-IQ integration in the training pipeline.\"\"\"\n",
        "    print(\"\\n\\n🚀 SHAP-IQ Integrated Training Demo\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Setup MLflow experiment\n",
        "    setup_mlflow_experiment(\"shapiq_demo\")\n",
        "\n",
        "    # Load data\n",
        "    X_train, X_test, y_train, y_test, feature_names, target_names, _ = load_and_prepare_iris_data()\n",
        "    print(f\"✓ Loaded Iris dataset: {len(X_train)} train, {len(X_test)} test samples\")\n",
        "\n",
        "    # Train model with SHAP-IQ integration\n",
        "    print(f\"\\n🤖 Training Logistic Regression with SHAP-IQ...\")\n",
        "    lr_run_id = train_logistic_regression(\n",
        "        X_train, y_train, X_test, y_test,\n",
        "        feature_names, target_names,\n",
        "        run_name=\"lr_with_shapiq\"\n",
        "    )\n",
        "    print(f\"✓ Logistic Regression complete: {lr_run_id[:8]}\")\n",
        "\n",
        "    print(f\"\\n🌲 Training Random Forest with SHAP-IQ...\")\n",
        "    rf_run_id = train_random_forest_optimized(\n",
        "        X_train, y_train, X_test, y_test,\n",
        "        feature_names, target_names,\n",
        "        n_trials=10,  # Reduced for demo\n",
        "        run_name=\"rf_with_shapiq\"\n",
        "    )\n",
        "    print(f\"✓ Random Forest complete: {rf_run_id[:8]}\")\n",
        "\n",
        "    # Show logged SHAP-IQ metrics\n",
        "    print(f\"\\n📊 SHAP-IQ Metrics from MLflow:\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    try:\n",
        "        # Get the latest run (Random Forest)\n",
        "        with mlflow.start_run(run_id=rf_run_id):\n",
        "            run_data = mlflow.get_run(rf_run_id)\n",
        "            metrics = run_data.data.metrics\n",
        "\n",
        "            # Filter SHAP-IQ metrics\n",
        "            shapiq_metrics = {k: v for k, v in metrics.items() if k.startswith('shapiq_')}\n",
        "\n",
        "            if shapiq_metrics:\n",
        "                print(f\"Found {len(shapiq_metrics)} SHAP-IQ metrics:\")\n",
        "                for metric, value in sorted(shapiq_metrics.items()):\n",
        "                    if 'order' in metric and 'count' not in metric:\n",
        "                        print(f\"  {metric:35} = {value:.6f}\")\n",
        "                    elif 'total' in metric or 'unique' in metric or 'max' in metric:\n",
        "                        print(f\"  {metric:35} = {int(value)}\")\n",
        "            else:\n",
        "                print(\"  No SHAP-IQ metrics found (may take longer to compute)\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  Error retrieving metrics: {e}\")\n",
        "\n",
        "    # Compare models\n",
        "    print(f\"\\n🏆 Comparing Models:\")\n",
        "    print(\"-\" * 30)\n",
        "    try:\n",
        "        best_run = get_best_run(\"accuracy\", maximize=True)\n",
        "        run_id = best_run[\"run_id\"]\n",
        "        accuracy = best_run.get(\"metrics.accuracy\", \"N/A\")\n",
        "        print(f\"Best model: {run_id[:8]} (accuracy: {accuracy})\")\n",
        "\n",
        "        # Check if SHAP-IQ metrics are available for best model\n",
        "        shapiq_count = best_run.get(\"metrics.shapiq_total_interactions\")\n",
        "        if shapiq_count:\n",
        "            print(f\"SHAP-IQ interactions: {int(shapiq_count)} computed\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error comparing models: {e}\")\n",
        "\n",
        "\n",
        "def demo_manual_shapiq_logging():\n",
        "    \"\"\"Demonstrate manual SHAP-IQ logging outside of training.\"\"\"\n",
        "    print(f\"\\n\\n🔧 Manual SHAP-IQ Logging Demo\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Load data and train model\n",
        "    X_train, X_test, y_train, y_test, feature_names, target_names, _ = load_and_prepare_iris_data()\n",
        "\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Manual MLflow run with SHAP-IQ logging\n",
        "    setup_mlflow_experiment(\"shapiq_demo\")\n",
        "\n",
        "    with mlflow.start_run(run_name=\"manual_shapiq_demo\"):\n",
        "        # Log basic metrics\n",
        "        accuracy = model.score(X_test, y_test)\n",
        "        mlflow.log_metric(\"accuracy\", accuracy)\n",
        "\n",
        "        # Log SHAP-IQ interactions\n",
        "        X_test_df = pd.DataFrame(X_test, columns=feature_names)\n",
        "        print(\"Computing and logging SHAP-IQ interactions...\")\n",
        "\n",
        "        log_shapiq_interactions(\n",
        "            model,\n",
        "            X_test_df,\n",
        "            feature_names,\n",
        "            max_order=2,\n",
        "            top_n=5,\n",
        "            budget=64,\n",
        "            n_samples=15  # Sample for faster computation\n",
        "        )\n",
        "\n",
        "        current_run = mlflow.active_run()\n",
        "        print(f\"✓ SHAP-IQ logged to run: {current_run.info.run_id[:8]}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Run all SHAP-IQ demos.\"\"\"\n",
        "    print(\"🌟 SHAP-IQ Integration Demonstration\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"This demo shows how Shapley interactions are computed and logged\")\n",
        "    print(\"in the MLOps pipeline to understand feature interactions.\")\n",
        "    print()\n",
        "\n",
        "    try:\n",
        "        # Demo 1: Standalone computation\n",
        "        demo_standalone_shapiq()\n",
        "\n",
        "        # Demo 2: Integrated training\n",
        "        demo_integrated_training()\n",
        "\n",
        "        # Demo 3: Manual logging\n",
        "        demo_manual_shapiq_logging()\n",
        "\n",
        "        print(f\"\\n\\n🎉 SHAP-IQ Demo Complete!\")\n",
        "        print(\"=\" * 60)\n",
        "        print(\"✓ Standalone SHAP-IQ computation\")\n",
        "        print(\"✓ Integrated training with automatic SHAP-IQ logging\")\n",
        "        print(\"✓ Manual SHAP-IQ logging\")\n",
        "        print()\n",
        "        print(\"🔍 Check MLflow UI to see logged SHAP-IQ metrics and artifacts:\")\n",
        "        print(\"   - Metrics: shapiq_order1_*, shapiq_order2_*, etc.\")\n",
        "        print(\"   - Artifacts: shapiq_interactions.csv, shapiq_interactions_summary.csv\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Demo failed: {e}\")\n",
        "        print(f\"\\n❌ Demo failed: {e}\")\n",
        "        print(\"This might be due to SHAP-IQ dependency issues or data problems.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/backend/ML/examples/select_best_and_dashboard.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/backend/ML/examples/select_best_and_dashboard.py\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Select best model and launch dashboard (training is done elsewhere).\n",
        "\n",
        "Usage:\n",
        "    python src/examples/select_best_and_dashboard.py\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "from src.backend.ML.mlops.utils import add_project_root_to_sys_path\n",
        "PROJECT_ROOT = add_project_root_to_sys_path()\n",
        "\n",
        "from src.backend.ML.mlops.experiment_utils import get_best_run\n",
        "from src.backend.ML.mlops.model_registry import load_model_from_run\n",
        "from src.backend.ML.mlops.explainer import dashboard_best_run\n",
        "\n",
        "# Configuration variables\n",
        "METRIC = \"accuracy\"  # Metric to optimize (e.g., 'accuracy', 'f1')\n",
        "PORT = 8050           # Port for the dashboard\n",
        "MAXIMIZE = True       # Whether to maximize (True) or minimize (False) the metric\n",
        "\n",
        "def main() -> None:\n",
        "    print(f\"🔍 Searching MLflow runs by {METRIC}…\")\n",
        "\n",
        "    # Retrieve the best run based on the specified metric\n",
        "    best = get_best_run(metric_key=METRIC, maximize=MAXIMIZE)\n",
        "    run_id = best[\"run_id\"]\n",
        "    score = best.get(f\"metrics.{METRIC}\", \"N/A\")\n",
        "\n",
        "    print(f\"🏆 Best run: {run_id[:8]} — {METRIC}: {score}\")\n",
        "\n",
        "    # Load the model from the run registry\n",
        "    model = load_model_from_run(run_id)\n",
        "    if model is None:\n",
        "        raise RuntimeError(\"Model could not be loaded from registry\")\n",
        "\n",
        "    print(\"✓ Model loaded – launching dashboard\")\n",
        "    # Launch the explainer dashboard for the best model\n",
        "    dashboard_best_run(METRIC, maximize=MAXIMIZE, port=PORT)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/backend/ML/scripts/run_training.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/backend/ML/scripts/run_training.py\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Simple training runner script.\n",
        "\n",
        "Run with:\n",
        "    python src/scripts/run_training.py\n",
        "    # or inside Jupyter:\n",
        "    %run src/scripts/run_training.py\n",
        "\"\"\"\n",
        "\n",
        "from src.backend.ML.mlops.utils import add_project_root_to_sys_path\n",
        "\n",
        "# Ensure src/ is importable in both script and notebook contexts\n",
        "PROJECT_ROOT = add_project_root_to_sys_path()\n",
        "\n",
        "from src.backend.ML.mlops.training import run_all_trainings\n",
        "\n",
        "\n",
        "def main() -> None:\n",
        "    print(\"🚀 Running all training pipelines from\", PROJECT_ROOT)\n",
        "    run_all_trainings(n_trials=20)\n",
        "    print(\"✅ Training complete!\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/backend/ML/examples/iris_classification_example.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/backend/ML/examples/iris_classification_example.py\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Iris Classification Example (argparse-free, notebook-safe).\n",
        "\n",
        "Configuration:\n",
        "    • export EXPLAINER_DASHBOARD=1   # launch dashboard\n",
        "    • export EXPLAINER_PORT=8150     # optional port override\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "import os\n",
        "import logging\n",
        "\n",
        "from src.backend.ML.mlops.utils import ensure_src_on_path\n",
        "ensure_src_on_path()\n",
        "\n",
        "from src.backend.ML.mlops.training import (\n",
        "    load_and_prepare_iris_data,\n",
        "    train_logistic_regression,\n",
        "    train_random_forest_optimized,\n",
        "    compare_models,\n",
        ")\n",
        "from src.backend.ML.mlops.model_registry import load_model_from_run\n",
        "from src.backend.ML.mlops.experiment_utils import get_best_run\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "\n",
        "def _bool_env(var: str, default: bool = False) -> bool:\n",
        "    v = os.getenv(var)\n",
        "    return default if v is None else v.lower() in {\"1\", \"true\", \"yes\"}\n",
        "\n",
        "\n",
        "def main(*, dashboard: bool = False, dashboard_port: int | None = None) -> None:\n",
        "    print(\"🌸 Iris Classification with MLflow\\n\" + \"=\" * 50)\n",
        "\n",
        "    # 1 Load data ------------------------------------------------------------\n",
        "    X_train, X_test, y_train, y_test, feat_names, tgt_names, _ = (\n",
        "        load_and_prepare_iris_data()\n",
        "    )\n",
        "    print(f\"✓ Training samples: {len(X_train)} | Test: {len(X_test)}\")\n",
        "\n",
        "    # 2 Logistic Regression --------------------------------------------------\n",
        "    lr_run = train_logistic_regression(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        X_test,\n",
        "        y_test,\n",
        "        feat_names,\n",
        "        tgt_names,\n",
        "        run_name=\"lr_baseline\",\n",
        "        register=True,\n",
        "    )\n",
        "    print(f\"✓ Logistic run {lr_run[:8]}\")\n",
        "\n",
        "    # 3 Random Forest + Optuna ----------------------------------------------\n",
        "    rf_run = train_random_forest_optimized(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        X_test,\n",
        "        y_test,\n",
        "        feat_names,\n",
        "        tgt_names,\n",
        "        n_trials=20,\n",
        "        run_name=\"rf_optimized\",\n",
        "        register=True,\n",
        "    )\n",
        "    print(f\"✓ RF run {rf_run[:8]}\")\n",
        "\n",
        "    # 4 Compare & test best --------------------------------------------------\n",
        "    compare_models()\n",
        "    best = get_best_run()\n",
        "    mdl = load_model_from_run(best[\"run_id\"])\n",
        "    if mdl is not None:\n",
        "        acc = (mdl.predict(X_test) == y_test).mean()\n",
        "        print(f\"🏆 Best model accuracy: {acc:.4f}\")\n",
        "    else:\n",
        "        print(\"❌ Could not load best model\")\n",
        "\n",
        "    if dashboard:\n",
        "        port = dashboard_port or int(os.getenv(\"EXPLAINER_PORT\", \"8050\"))\n",
        "        print(f\"\\n🚀 ExplainerDashboard running on http://localhost:{port}\")\n",
        "        # Import and run dashboard for best model\n",
        "        from src.backend.ML.mlops.explainer import dashboard_best_run\n",
        "        dashboard_best_run(\"accuracy\", port=port)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main(\n",
        "        dashboard=_bool_env(\"EXPLAINER_DASHBOARD\", False),\n",
        "        dashboard_port=int(os.getenv(\"EXPLAINER_PORT\", \"8050\")),\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/backend/ML/model_api/__init__.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/backend/ML/model_api/__init__.py\n",
        "# Model API package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/backend/ML/model_api/main.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/backend/ML/model_api/main.py\n",
        "\"\"\"\n",
        "FastAPI microservice for NFL kicker model serving.\n",
        "Provides endpoints for model predictions, leaderboard, and analysis.\n",
        "\"\"\"\n",
        "import os\n",
        "import logging\n",
        "from typing import List, Dict, Any, Optional\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from fastapi import FastAPI, HTTPException, Depends\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel, Field\n",
        "import json\n",
        "from datetime import datetime\n",
        "import mlflow\n",
        "import mlflow.pyfunc\n",
        "from pathlib import Path\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "app = FastAPI(\n",
        "    title=\"NFL Kicker Model API\",\n",
        "    description=\"API for serving NFL kicker prediction models\",\n",
        "    version=\"1.0.0\"\n",
        ")\n",
        "\n",
        "# Add CORS middleware\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],  # Configure appropriately for production\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# ─── Data Models ───────────────────────────────────────────────────────────\n",
        "\n",
        "class KickFeatures(BaseModel):\n",
        "    \"\"\"Features for a single kick prediction.\"\"\"\n",
        "    player_id: int = Field(..., description=\"Player ID\", example=123)\n",
        "    player_name: Optional[str] = Field(None, description=\"Player name\", example=\"Justin Tucker\")\n",
        "    distance: float = Field(..., ge=18, le=70, description=\"Kick distance in yards\", example=45.0)\n",
        "    week: int = Field(..., ge=1, le=21, description=\"Week number\", example=6)\n",
        "\n",
        "    class Config:\n",
        "        json_schema_extra = {\n",
        "            \"example\": {\n",
        "                \"player_id\": 123,\n",
        "                \"player_name\": \"Justin Tucker\",\n",
        "                \"distance\": 45.0,\n",
        "                \"week\": 6\n",
        "            }\n",
        "        }\n",
        "\n",
        "class PredictRequest(BaseModel):\n",
        "    \"\"\"Request for model predictions.\"\"\"\n",
        "    model: str = Field(\"rf\", description=\"Model type: 'rf' or 'bayes'\")\n",
        "    rows: List[KickFeatures] = Field(..., description=\"List of kick features\")\n",
        "    posterior_samples: Optional[int] = Field(None, description=\"Number of posterior samples for Bayesian model\")\n",
        "\n",
        "    class Config:\n",
        "        json_schema_extra = {\n",
        "            \"example\": {\n",
        "                \"model\": \"rf\",\n",
        "                \"rows\": [\n",
        "                    {\n",
        "                        \"player_id\": 123,\n",
        "                        \"player_name\": \"Justin Tucker\",\n",
        "                        \"distance\": 45.0,\n",
        "                        \"week\": 6\n",
        "                    }\n",
        "                ]\n",
        "            }\n",
        "        }\n",
        "\n",
        "class PredictResponse(BaseModel):\n",
        "    \"\"\"Response from model predictions.\"\"\"\n",
        "    predictions: List[float] = Field(..., description=\"Predicted success probabilities\")\n",
        "    model_used: str = Field(..., description=\"Model that was used\")\n",
        "    uncertainty: Optional[List[Dict[str, float]]] = Field(None, description=\"Uncertainty estimates for Bayesian model\")\n",
        "\n",
        "class LeaderboardEntry(BaseModel):\n",
        "    \"\"\"Single entry in the leaderboard.\"\"\"\n",
        "    player_id: int\n",
        "    player_name: str\n",
        "    rating: float\n",
        "    rank: int\n",
        "    attempts: int\n",
        "    accuracy: float\n",
        "    confidence_interval: Optional[Dict[str, float]] = None\n",
        "\n",
        "class AnalysisResponse(BaseModel):\n",
        "    \"\"\"Analysis response with key insights.\"\"\"\n",
        "    total_kickers: int\n",
        "    total_attempts: int\n",
        "    average_accuracy: float\n",
        "    distance_breakdown: Dict[str, Any]\n",
        "    top_performers: List[LeaderboardEntry]\n",
        "    insights: List[str]\n",
        "\n",
        "# ─── Model Service Implementation ─────────────────────────────────────────────\n",
        "\n",
        "class NFLKickerModelService:\n",
        "    \"\"\"Service for loading and serving NFL kicker models from MLflow.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.rf_model = None\n",
        "        self.bayes_model = None\n",
        "        self.models_loaded = False\n",
        "        self._sample_data = self._generate_sample_data()\n",
        "        self._load_models()\n",
        "        logger.info(\"NFL Kicker Model Service initialized\")\n",
        "\n",
        "    def _load_models(self):\n",
        "        \"\"\"Load or create mock models.\"\"\"\n",
        "        try:\n",
        "            mlflow_uri = os.getenv(\"MLFLOW_TRACKING_URI\", \"http://mlflow:5000\")\n",
        "            if mlflow_uri.startswith(\"http://mlflow\") and not os.getenv(\"DOCKER_ENV\"):\n",
        "                # local dev outside Docker\n",
        "                mlflow_uri = \"http://localhost:5001\"\n",
        "            mlflow.set_tracking_uri(mlflow_uri)\n",
        "            logger.info(f\"MLflow tracking URI set to: {mlflow_uri}\")\n",
        "\n",
        "            # Create mock models for demo\n",
        "            self.rf_model = self._create_mock_rf_model()\n",
        "            self.bayes_model = self._create_mock_bayes_model()\n",
        "            self.models_loaded = True\n",
        "            logger.info(\"Mock models loaded successfully\")\n",
        "\n",
        "            # Generate sample data\n",
        "            self._generate_sample_data()\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading models: {e}\")\n",
        "            self.models_loaded = False\n",
        "            raise\n",
        "\n",
        "    def _create_mock_rf_model(self):\n",
        "        \"\"\"Create a mock random forest model.\"\"\"\n",
        "        class MockRFModel:\n",
        "            def predict(self, X):\n",
        "                # Convert to DataFrame if needed\n",
        "                if isinstance(X, pd.DataFrame):\n",
        "                    distances = X['distance'].values if 'distance' in X.columns else X.iloc[:, 0].values\n",
        "                else:\n",
        "                    distances = X[:, 0] if len(X.shape) > 1 else [X[0]]\n",
        "\n",
        "                probabilities = []\n",
        "                for dist in distances:\n",
        "                    # Realistic probability curve: high at short distances, decreasing with distance\n",
        "                    prob = max(0.1, min(0.99, 1.0 / (1.0 + np.exp((dist - 42) / 8))))\n",
        "                    # Add some noise\n",
        "                    prob += np.random.normal(0, 0.05)\n",
        "                    prob = max(0.1, min(0.99, prob))\n",
        "                    probabilities.append(prob)\n",
        "                return np.array(probabilities)\n",
        "\n",
        "        return MockRFModel()\n",
        "\n",
        "    def _create_mock_bayes_model(self):\n",
        "        \"\"\"Create a mock Bayesian model.\"\"\"\n",
        "        class MockBayesModel:\n",
        "            def predict(self, X, n_samples=None):\n",
        "                # Convert to DataFrame if needed\n",
        "                if isinstance(X, pd.DataFrame):\n",
        "                    distances = X['distance'].values if 'distance' in X.columns else X.iloc[:, 0].values\n",
        "                else:\n",
        "                    distances = X[:, 0] if len(X.shape) > 1 else [X[0]]\n",
        "\n",
        "                probabilities = []\n",
        "                uncertainties = []\n",
        "\n",
        "                for dist in distances:\n",
        "                    base_prob = max(0.1, min(0.99, 1.0 / (1.0 + np.exp((dist - 42) / 8))))\n",
        "\n",
        "                    if n_samples:\n",
        "                        # Generate posterior samples\n",
        "                        samples = np.random.beta(\n",
        "                            base_prob * 20 + 1,\n",
        "                            (1 - base_prob) * 20 + 1,\n",
        "                            n_samples\n",
        "                        )\n",
        "                        mean_prob = np.mean(samples)\n",
        "                        uncertainty = {\n",
        "                            \"mean\": float(mean_prob),\n",
        "                            \"std\": float(np.std(samples)),\n",
        "                            \"ci_lower\": float(np.percentile(samples, 2.5)),\n",
        "                            \"ci_upper\": float(np.percentile(samples, 97.5))\n",
        "                        }\n",
        "                        probabilities.append(mean_prob)\n",
        "                        uncertainties.append(uncertainty)\n",
        "                    else:\n",
        "                        probabilities.append(base_prob)\n",
        "\n",
        "                if n_samples:\n",
        "                    return np.array(probabilities), uncertainties\n",
        "                else:\n",
        "                    return np.array(probabilities)\n",
        "\n",
        "        return MockBayesModel()\n",
        "\n",
        "    def _generate_sample_data(self):\n",
        "        \"\"\"Generate sample leaderboard and analysis data.\"\"\"\n",
        "        np.random.seed(42)  # For reproducible results\n",
        "\n",
        "        # Famous NFL kickers with realistic stats\n",
        "        kickers = [\n",
        "            {\"player_id\": 1, \"player_name\": \"Justin Tucker\", \"base_accuracy\": 0.91},\n",
        "            {\"player_id\": 2, \"player_name\": \"Harrison Butker\", \"base_accuracy\": 0.89},\n",
        "            {\"player_id\": 3, \"player_name\": \"Tyler Bass\", \"base_accuracy\": 0.87},\n",
        "            {\"player_id\": 4, \"player_name\": \"Daniel Carlson\", \"base_accuracy\": 0.86},\n",
        "            {\"player_id\": 5, \"player_name\": \"Younghoe Koo\", \"base_accuracy\": 0.85},\n",
        "            {\"player_id\": 6, \"player_name\": \"Matt Gay\", \"base_accuracy\": 0.84},\n",
        "            {\"player_id\": 7, \"player_name\": \"Brandon McManus\", \"base_accuracy\": 0.83},\n",
        "            {\"player_id\": 8, \"player_name\": \"Robbie Gould\", \"base_accuracy\": 0.82},\n",
        "            {\"player_id\": 9, \"player_name\": \"Jake Elliott\", \"base_accuracy\": 0.81},\n",
        "            {\"player_id\": 10, \"player_name\": \"Greg Zuerlein\", \"base_accuracy\": 0.80},\n",
        "        ]\n",
        "\n",
        "        leaderboard = []\n",
        "        for i, kicker in enumerate(kickers):\n",
        "            # Cast numpy types → native Python for JSON serialization\n",
        "            attempts = int(np.random.randint(15, 35))\n",
        "            accuracy = float(kicker[\"base_accuracy\"] + np.random.normal(0, 0.02))\n",
        "            accuracy = max(0.7, min(0.95, accuracy))\n",
        "\n",
        "            # Calculate rating (similar to frontend logic)\n",
        "            rating = float(accuracy * 100 + np.random.normal(0, 2))\n",
        "            rating = max(70, min(100, rating))\n",
        "\n",
        "            leaderboard.append({\n",
        "                \"player_id\": kicker[\"player_id\"],\n",
        "                \"player_name\": kicker[\"player_name\"],\n",
        "                \"rating\": round(float(rating), 2),\n",
        "                \"rank\": i + 1,\n",
        "                \"attempts\": int(attempts),\n",
        "                \"accuracy\": round(float(accuracy), 3),\n",
        "                \"confidence_interval\": {\n",
        "                    \"lower\": round(float(accuracy) - 0.05, 3),\n",
        "                    \"upper\": round(float(accuracy) + 0.05, 3)\n",
        "                }\n",
        "            })\n",
        "\n",
        "        # Sort by rating\n",
        "        leaderboard.sort(key=lambda x: x[\"rating\"], reverse=True)\n",
        "        for i, entry in enumerate(leaderboard):\n",
        "            entry[\"rank\"] = i + 1\n",
        "\n",
        "        return {\n",
        "            \"leaderboard\": leaderboard,\n",
        "            \"analysis\": {\n",
        "                \"total_kickers\": len(kickers),\n",
        "                \"total_attempts\": sum(k[\"attempts\"] for k in leaderboard),\n",
        "                \"average_accuracy\": sum(k[\"accuracy\"] for k in leaderboard) / len(leaderboard),\n",
        "                \"distance_breakdown\": {\n",
        "                    \"short\": {\"range\": \"18-29 yards\", \"expected_accuracy\": 0.95},\n",
        "                    \"medium_short\": {\"range\": \"30-39 yards\", \"expected_accuracy\": 0.90},\n",
        "                    \"medium\": {\"range\": \"40-49 yards\", \"expected_accuracy\": 0.82},\n",
        "                    \"long\": {\"range\": \"50-59 yards\", \"expected_accuracy\": 0.68},\n",
        "                    \"extreme\": {\"range\": \"60+ yards\", \"expected_accuracy\": 0.35}\n",
        "                },\n",
        "                \"top_performers\": leaderboard[:5],\n",
        "                \"insights\": [\n",
        "                    \"Justin Tucker leads with exceptional accuracy across all distances\",\n",
        "                    \"Kickers show 95%+ accuracy for kicks under 30 yards\",\n",
        "                    \"Success rate drops significantly beyond 50 yards\",\n",
        "                    \"Weather conditions and pressure situations affect performance\",\n",
        "                    \"Rookie kickers typically show higher variability in performance\"\n",
        "                ]\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def predict(self, model_type: str, features: List[Dict], posterior_samples: Optional[int] = None):\n",
        "        \"\"\"Make predictions using the specified model.\"\"\"\n",
        "        try:\n",
        "            # Convert features to DataFrame\n",
        "            df = pd.DataFrame(features)\n",
        "\n",
        "            # Prepare features for prediction\n",
        "            if 'distance' not in df.columns:\n",
        "                raise ValueError(\"Distance feature is required\")\n",
        "\n",
        "            # Select model\n",
        "            if model_type == \"rf\":\n",
        "                model = self.rf_model\n",
        "                predictions = model.predict(df)\n",
        "                return {\n",
        "                    \"predictions\": predictions.tolist(),\n",
        "                    \"model_used\": \"Random Forest\",\n",
        "                    \"uncertainty\": None\n",
        "                }\n",
        "            elif model_type == \"bayes\":\n",
        "                model = self.bayes_model\n",
        "                if posterior_samples and hasattr(model, 'predict') and 'n_samples' in model.predict.__code__.co_varnames:\n",
        "                    predictions, uncertainties = model.predict(df, n_samples=posterior_samples)\n",
        "                    return {\n",
        "                        \"predictions\": predictions.tolist(),\n",
        "                        \"model_used\": \"Bayesian\",\n",
        "                        \"uncertainty\": uncertainties\n",
        "                    }\n",
        "                else:\n",
        "                    predictions = model.predict(df)\n",
        "                    return {\n",
        "                        \"predictions\": predictions.tolist(),\n",
        "                        \"model_used\": \"Bayesian\",\n",
        "                        \"uncertainty\": None\n",
        "                    }\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown model type: {model_type}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Prediction error: {e}\")\n",
        "            raise HTTPException(status_code=400, detail=str(e))\n",
        "\n",
        "    def get_leaderboard(self):\n",
        "        \"\"\"Get the current leaderboard.\"\"\"\n",
        "        return self._sample_data[\"leaderboard\"]\n",
        "\n",
        "    def get_analysis(self):\n",
        "        \"\"\"Get analysis insights.\"\"\"\n",
        "        return self._sample_data[\"analysis\"]\n",
        "\n",
        "# Initialize the model service\n",
        "model_service = NFLKickerModelService()\n",
        "\n",
        "# ─── API Endpoints ─────────────────────────────────────────────────────────\n",
        "\n",
        "@app.get(\"/health\")\n",
        "async def health_check():\n",
        "    \"\"\"Health check endpoint.\"\"\"\n",
        "    return {\n",
        "        \"status\": \"healthy\",\n",
        "        \"timestamp\": datetime.now().isoformat(),\n",
        "        \"models_loaded\": model_service.models_loaded,\n",
        "        \"mlflow_uri\": os.getenv(\"MLFLOW_TRACKING_URI\", \"http://mlflow:5000\")\n",
        "    }\n",
        "\n",
        "@app.post(\"/predict\", response_model=PredictResponse)\n",
        "async def predict(request: PredictRequest):\n",
        "    \"\"\"Make predictions using the specified model.\"\"\"\n",
        "    try:\n",
        "        # Convert features to list of dictionaries\n",
        "        features = [feature.dict() for feature in request.rows]\n",
        "\n",
        "        # Make prediction\n",
        "        result = model_service.predict(\n",
        "            model_type=request.model,\n",
        "            features=features,\n",
        "            posterior_samples=request.posterior_samples\n",
        "        )\n",
        "\n",
        "        return PredictResponse(\n",
        "            predictions=result[\"predictions\"],\n",
        "            model_used=result[\"model_used\"],\n",
        "            uncertainty=result[\"uncertainty\"]\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Prediction endpoint error: {e}\")\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "@app.get(\"/leaderboard\")\n",
        "async def get_leaderboard():\n",
        "    \"\"\"Get the current kicker leaderboard.\"\"\"\n",
        "    try:\n",
        "        leaderboard = model_service.get_leaderboard()\n",
        "        return leaderboard\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Leaderboard endpoint error: {e}\")\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "@app.get(\"/analysis\", response_model=AnalysisResponse)\n",
        "async def get_analysis():\n",
        "    \"\"\"Get analysis insights and statistics.\"\"\"\n",
        "    try:\n",
        "        analysis = model_service.get_analysis()\n",
        "        return AnalysisResponse(\n",
        "            total_kickers=analysis[\"total_kickers\"],\n",
        "            total_attempts=analysis[\"total_attempts\"],\n",
        "            average_accuracy=analysis[\"average_accuracy\"],\n",
        "            distance_breakdown=analysis[\"distance_breakdown\"],\n",
        "            top_performers=analysis[\"top_performers\"],\n",
        "            insights=analysis[\"insights\"]\n",
        "        )\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Analysis endpoint error: {e}\")\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "    \"\"\"Root endpoint with API information.\"\"\"\n",
        "    return {\n",
        "        \"message\": \"NFL Kicker Model API\",\n",
        "        \"version\": \"1.0.0\",\n",
        "        \"endpoints\": {\n",
        "            \"health\": \"/health\",\n",
        "            \"predict\": \"/predict\",\n",
        "            \"leaderboard\": \"/leaderboard\",\n",
        "            \"analysis\": \"/analysis\",\n",
        "            \"docs\": \"/docs\"\n",
        "        }\n",
        "    }\n",
        "if __name__ == \"__main__\":\n",
        "    import uvicorn\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/backend/ML/model_api/iris_cancer_fastapi.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/backend/ML/model_api/iris_cancer_fastapi.py\n",
        "\"\"\"\n",
        "FastAPI service for Iris and Breast-Cancer predictions.\n",
        "\n",
        "This module provides a dual-dataset prediction service that:\n",
        "1. Loads/trains point-estimate models for Iris (Random Forest)\n",
        "2. Loads/trains Bayesian models for Breast Cancer (PyMC)\n",
        "3. Exposes them at /predict/iris and /predict/cancer endpoints\n",
        "4. Includes an embedded smoke test\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "import os\n",
        "import logging\n",
        "from typing import List, Dict, Any, Optional, Sequence, Callable\n",
        "from contextlib import asynccontextmanager\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import mlflow\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel, Field, validator\n",
        "from sklearn.datasets import load_iris, load_breast_cancer\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Initialize FastAPI app with lifespan\n",
        "@asynccontextmanager\n",
        "async def lifespan(app: FastAPI):\n",
        "    # Load models once at startup\n",
        "    global service\n",
        "    service = _ModelService()\n",
        "    service._load()\n",
        "    yield\n",
        "    # Optional cleanup can go here\n",
        "\n",
        "app = FastAPI(\n",
        "    title=\"Iris & Cancer Prediction API\",\n",
        "    description=\"Dual-dataset prediction service with uncertainty estimates\",\n",
        "    lifespan=lifespan\n",
        ")\n",
        "\n",
        "# Add CORS middleware\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],  # For demo; restrict in production\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# ─── Pydantic Models ──────────────────────────────────────────────────────────\n",
        "class IrisFeatures(BaseModel):\n",
        "    \"\"\"Single Iris-flower measurement.\"\"\"\n",
        "    sepal_length: float = Field(..., ge=4.0, le=8.0, description=\"Sepal length (cm)\")\n",
        "    sepal_width: float = Field(..., ge=2.0, le=4.5, description=\"Sepal width (cm)\")\n",
        "    petal_length: float = Field(..., ge=1.0, le=7.0, description=\"Petal length (cm)\")\n",
        "    petal_width: float = Field(..., ge=0.1, le=2.5, description=\"Petal width (cm)\")\n",
        "\n",
        "    class Config:\n",
        "        json_schema_extra = {\n",
        "            \"example\": {\n",
        "                \"sepal_length\": 5.1,\n",
        "                \"sepal_width\": 3.5,\n",
        "                \"petal_length\": 1.4,\n",
        "                \"petal_width\": 0.2\n",
        "            }\n",
        "        }\n",
        "\n",
        "class CancerFeatures(BaseModel):\n",
        "    \"\"\"30‑dimensional input in **exact** `load_breast_cancer().feature_names` order.\"\"\"\n",
        "    values: List[float] = Field(..., example=[14.23, 19.82, 92.72, 654.6, 0.1, 0.16, 0.2,\n",
        "                                             0.15, 0.25, 0.08, 0.45, 1.21, 2.93, 33.4, 0.009,\n",
        "                                             0.01, 0.03, 0.02, 0.03, 0.004, 16.2, 25.7, 104.3,\n",
        "                                             819.8, 0.113, 0.23, 0.32, 0.16, 0.39, 0.11])\n",
        "\n",
        "    @validator(\"values\")\n",
        "    def _length_exactly_30(cls, v: Sequence[float]):  # noqa: D401\n",
        "        if len(v) != 30:\n",
        "            raise ValueError(\"Must provide exactly 30 features for breast cancer prediction\")\n",
        "        return v\n",
        "\n",
        "class IrisPredictRequest(BaseModel):\n",
        "    rows: List[IrisFeatures]\n",
        "\n",
        "class CancerPredictRequest(BaseModel):\n",
        "    rows: List[CancerFeatures]\n",
        "    posterior_samples: Optional[int] = Field(None, description=\"n posterior draws\")\n",
        "\n",
        "class PredictResponse(BaseModel):\n",
        "    predictions: List[float]\n",
        "    model_used: str\n",
        "    uncertainty: Optional[List[Dict[str, float]]] = None\n",
        "\n",
        "# ─── Model Service ──────────────────────────────────────────────────────────\n",
        "class _ModelService:\n",
        "    \"\"\"Global model service that loads/caches models.\"\"\"\n",
        "    \n",
        "    def __init__(self) -> None:\n",
        "        \"\"\"Initialize empty service - actual loading happens in lifespan.\"\"\"\n",
        "        self.iris_model = None\n",
        "        self.cancer_model = None\n",
        "        \n",
        "    def _load_model(self, uri: str, trainer: Callable[[], None] | None = None):\n",
        "        \"\"\"\n",
        "        Try to load *uri* from the MLflow registry.  \n",
        "        If missing **and** DEV_AUTOTRAIN=1, invoke *trainer* once then reload.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            return mlflow.pyfunc.load_model(uri)\n",
        "        except Exception as exc:\n",
        "            if os.getenv(\"DEV_AUTOTRAIN\") == \"1\" and trainer:\n",
        "                logger.warning(\"Model %s not found – training due to DEV_AUTOTRAIN\", uri)\n",
        "                trainer()\n",
        "                return mlflow.pyfunc.load_model(uri)\n",
        "            raise RuntimeError(f\"Failed to load model {uri} and DEV_AUTOTRAIN not enabled\") from exc\n",
        "            \n",
        "    def _load(self):\n",
        "        \"\"\"Load both models from registry.\"\"\"\n",
        "        from src.backend.ML.mlops.training import train_random_forest_optimized, load_and_prepare_iris_data\n",
        "        from src.backend.ML.mlops.training_bayes import train_bayes_logreg\n",
        "        \n",
        "        # Load Iris model (RF)\n",
        "        self.iris_model = self._load_model(\n",
        "            \"models:/iris_random_forest/Production\",\n",
        "            lambda: train_random_forest_optimized(*load_and_prepare_iris_data())\n",
        "        )\n",
        "        logger.info(\"✓ Loaded Iris model\")\n",
        "        \n",
        "        # Load Cancer model (Bayes)\n",
        "        self.cancer_model = self._load_model(\n",
        "            \"models:/iris_bayes_logreg/Production\",\n",
        "            lambda: train_bayes_logreg()\n",
        "        )\n",
        "        logger.info(\"✓ Loaded Cancer model\")\n",
        "\n",
        "# Global model service instance\n",
        "service = _ModelService()\n",
        "\n",
        "# ─── API Routes ─────────────────────────────────────────────────────────────\n",
        "@app.get(\"/health\")\n",
        "async def health():\n",
        "    \"\"\"Health check endpoint.\"\"\"\n",
        "    return {\n",
        "        \"status\": \"healthy\",\n",
        "        \"models\": {\n",
        "            \"iris\": service.iris_model is not None,\n",
        "            \"cancer\": service.cancer_model is not None\n",
        "        }\n",
        "    }\n",
        "\n",
        "@app.post(\"/predict/iris\", response_model=PredictResponse)\n",
        "async def predict_iris(req: IrisPredictRequest):\n",
        "    \"\"\"Predict Iris species.\"\"\"\n",
        "    if not service.iris_model:\n",
        "        raise HTTPException(status_code=503, detail=\"Iris model not loaded\")\n",
        "\n",
        "    # Convert to DataFrame with correct column names\n",
        "    X = pd.DataFrame([\n",
        "        {\n",
        "            \"sepal length (cm)\": r.sepal_length,\n",
        "            \"sepal width (cm)\": r.sepal_width,\n",
        "            \"petal length (cm)\": r.petal_length,\n",
        "            \"petal width (cm)\": r.petal_width\n",
        "        }\n",
        "        for r in req.rows\n",
        "    ])\n",
        "\n",
        "    try:\n",
        "        preds = service.iris_model.predict(X)\n",
        "        return PredictResponse(\n",
        "            predictions=preds.tolist(),\n",
        "            model_used=\"Random Forest\",\n",
        "            uncertainty=None  # Point estimates only\n",
        "        )\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "@app.post(\"/predict/cancer\", response_model=PredictResponse)\n",
        "async def predict_cancer(req: CancerPredictRequest):\n",
        "    \"\"\"Predict breast cancer malignancy with uncertainty.\"\"\"\n",
        "    if not service.cancer_model:\n",
        "        raise HTTPException(status_code=503, detail=\"Cancer model not loaded\")\n",
        "\n",
        "    X = np.asarray([r.values for r in req.rows], dtype=float)\n",
        "    try:\n",
        "        if req.posterior_samples:\n",
        "            # Get posterior predictive samples\n",
        "            samples = service.cancer_model.predict(X, n_samples=req.posterior_samples)\n",
        "            means = samples.mean(axis=0)\n",
        "            unc = [\n",
        "                {\n",
        "                    \"mean\": float(m),\n",
        "                    \"std\": float(s.std()),\n",
        "                    \"ci_lower\": float(np.percentile(s, 2.5)),\n",
        "                    \"ci_upper\": float(np.percentile(s, 97.5))\n",
        "                }\n",
        "                for m, s in zip(means, samples.T)\n",
        "            ]\n",
        "            return PredictResponse(\n",
        "                predictions=means.tolist(),\n",
        "                model_used=\"Bayesian LogReg\",\n",
        "                uncertainty=unc\n",
        "            )\n",
        "        else:\n",
        "            # Point estimates only\n",
        "            preds = service.cancer_model.predict(X)\n",
        "            return PredictResponse(\n",
        "                predictions=preds.tolist(),\n",
        "                model_used=\"Bayesian LogReg\",\n",
        "                uncertainty=None\n",
        "            )\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "# ─── Smoke Test ────────────────────────────────────────────────────────────\n",
        "if __name__ == \"__main__\":\n",
        "    from fastapi.testclient import TestClient\n",
        "    from sklearn.datasets import load_iris, load_breast_cancer\n",
        "    import numpy as np\n",
        "\n",
        "    client = TestClient(app)\n",
        "\n",
        "    # Test health endpoint\n",
        "    print(\"\\n🏥 Testing /health endpoint...\")\n",
        "    health = client.get(\"/health\").json()\n",
        "    print(f\"Response: {health}\")\n",
        "\n",
        "    # Test Iris endpoint with first Setosa sample\n",
        "    print(\"\\n🌸 Testing /predict/iris endpoint with first Setosa sample...\")\n",
        "    iris = load_iris()\n",
        "    setosa_sample = iris.data[0]  # First sample is Setosa\n",
        "    iris_request = {\n",
        "        \"rows\": [{\n",
        "            \"sepal_length\": float(setosa_sample[0]),\n",
        "            \"sepal_width\": float(setosa_sample[1]),\n",
        "            \"petal_length\": float(setosa_sample[2]),\n",
        "            \"petal_width\": float(setosa_sample[3])\n",
        "        }]\n",
        "    }\n",
        "    iris_response = client.post(\"/predict/iris\", json=iris_request).json()\n",
        "    print(f\"True class: Setosa (0)\")\n",
        "    print(f\"Features: {dict(zip(iris.feature_names, setosa_sample))}\")\n",
        "    print(f\"Response: {iris_response}\")\n",
        "\n",
        "    # Test Cancer endpoint with first malignant sample\n",
        "    print(\"\\n🔬 Testing /predict/cancer endpoint with first malignant sample...\")\n",
        "    cancer = load_breast_cancer()\n",
        "    malignant_idx = np.where(cancer.target == 1)[0][0]  # First malignant sample\n",
        "    malignant_sample = cancer.data[malignant_idx]\n",
        "    cancer_request = {\n",
        "        \"rows\": [{\n",
        "            \"values\": [float(x) for x in malignant_sample]\n",
        "        }],\n",
        "        \"posterior_samples\": 500  # Request uncertainty estimates\n",
        "    }\n",
        "    cancer_response = client.post(\"/predict/cancer\", json=cancer_request).json()\n",
        "    print(f\"True class: Malignant (1)\")\n",
        "    print(f\"Features: First 5 of {len(cancer.feature_names)} features:\")\n",
        "    for name, value in list(zip(cancer.feature_names, malignant_sample))[:5]:\n",
        "        print(f\"  {name}: {value:.3f}\")\n",
        "    print(f\"Response: {cancer_response}\")\n",
        "\n",
        "    print(\"\\n✅ All tests passed!\") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting tests/test_mlflow_integration.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile tests/test_mlflow_integration.py\n",
        "\n",
        "\"\"\"Tests for MLflow integration modules.\"\"\"\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add src to path\n",
        "sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))\n",
        "\n",
        "from backend.ML.mlops.experiment_utils import setup_mlflow_experiment\n",
        "from backend.ML.mlops.training import (\n",
        "    load_and_prepare_iris_data,\n",
        "    train_logistic_regression\n",
        ")\n",
        "from backend.ML.mlops.model_registry import load_model_from_run\n",
        "\n",
        "\n",
        "def test_data_loading():\n",
        "    \"\"\"Test that data loading works correctly.\"\"\"\n",
        "    data = load_and_prepare_iris_data()\n",
        "    X_train, X_test, y_train, y_test, feature_names, target_names, scaler = data\n",
        "\n",
        "    assert len(X_train) > 0\n",
        "    assert len(X_test) > 0\n",
        "    assert len(feature_names) == 4\n",
        "    assert len(target_names) == 3\n",
        "    assert X_train.shape[1] == 4  # 4 features\n",
        "\n",
        "\n",
        "def test_experiment_setup():\n",
        "    \"\"\"Test that MLflow experiment setup works.\"\"\"\n",
        "    # This should not raise an exception\n",
        "    setup_mlflow_experiment(\"test_experiment\")\n",
        "\n",
        "\n",
        "def test_model_training_and_loading():\n",
        "    \"\"\"Test end-to-end model training and loading.\"\"\"\n",
        "    # Load data\n",
        "    data = load_and_prepare_iris_data()\n",
        "    X_train, X_test, y_train, y_test, feature_names, target_names, scaler = data\n",
        "\n",
        "    # Train a simple model\n",
        "    run_id = train_logistic_regression(\n",
        "        X_train, y_train, X_test, y_test,\n",
        "        feature_names, target_names,\n",
        "        run_name=\"test_lr\",\n",
        "        register=False  # Don't register for tests\n",
        "    )\n",
        "\n",
        "    assert run_id is not None\n",
        "    assert len(run_id) > 0\n",
        "\n",
        "    # Load the model back\n",
        "    model = load_model_from_run(run_id, \"model\")\n",
        "\n",
        "    # Test prediction\n",
        "    predictions = model.predict(X_test)\n",
        "    assert len(predictions) == len(y_test)\n",
        "\n",
        "    # Check accuracy is reasonable (should be > 0.8 for iris)\n",
        "    accuracy = (predictions == y_test).mean()\n",
        "    assert accuracy > 0.8\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run tests\n",
        "    test_data_loading()\n",
        "    print(\"✓ Data loading test passed\")\n",
        "\n",
        "    test_experiment_setup()\n",
        "    print(\"✓ Experiment setup test passed\")\n",
        "\n",
        "    test_model_training_and_loading()\n",
        "    print(\"✓ Model training and loading test passed\")\n",
        "\n",
        "    print(\"\\nAll tests passed! 🎉\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting tests/test_explainer.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile tests/test_explainer.py\n",
        "import sys\n",
        "import os\n",
        "import pytest\n",
        "from pathlib import Path\n",
        "\n",
        "# Add src to path\n",
        "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
        "\n",
        "def test_yaml_roundtrip(tmp_path):\n",
        "    \"\"\"Test that a dashboard can be saved to YAML and reloaded.\"\"\"\n",
        "    from src.backend.ML.mlops.explainer import build_and_log_dashboard, load_dashboard_yaml\n",
        "    from sklearn.datasets import load_iris\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    import mlflow\n",
        "    import pandas as pd\n",
        "\n",
        "    iris = load_iris()\n",
        "    X, y = iris.data, iris.target\n",
        "    X_df = pd.DataFrame(X, columns=iris.feature_names)\n",
        "    model = LogisticRegression(max_iter=1000).fit(X, y)\n",
        "    with mlflow.start_run():\n",
        "        yaml_path = build_and_log_dashboard(\n",
        "            model, X_df, y,\n",
        "            serve=False,\n",
        "            save_yaml=True,\n",
        "            output_dir=tmp_path\n",
        "        )\n",
        "        # Reload\n",
        "        dash = load_dashboard_yaml(yaml_path)\n",
        "        assert dash.explainer.model.__class__.__name__ == \"LogisticRegression\"\n",
        "\n",
        "\n",
        "def test_build_dashboard(tmp_path):\n",
        "    \"\"\"Test that a dashboard can be built and saved.\"\"\"\n",
        "    from src.backend.ML.mlops.explainer import build_and_log_dashboard\n",
        "    from sklearn.datasets import load_iris\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    import mlflow\n",
        "    import pandas as pd\n",
        "\n",
        "    iris = load_iris()\n",
        "    X, y = iris.data, iris.target\n",
        "    X_df = pd.DataFrame(X, columns=iris.feature_names)\n",
        "    model = LogisticRegression(max_iter=1000).fit(X, y)\n",
        "    with mlflow.start_run():\n",
        "        html = build_and_log_dashboard(\n",
        "            model, X_df, y,\n",
        "            serve=False,\n",
        "            save_yaml=False,\n",
        "            output_dir=tmp_path\n",
        "        )\n",
        "        assert html.exists() and html.suffix == \".html\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting tests/test_shapiq_utils.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile tests/test_shapiq_utils.py\n",
        "\"\"\"\n",
        "Tests for SHAP-IQ utilities module.\n",
        "\"\"\"\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import pytest\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tempfile\n",
        "from unittest.mock import patch, MagicMock\n",
        "\n",
        "# Add src to path\n",
        "sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "from backend.ML.mlops.shapiq_utils import (\n",
        "    compute_shapiq_interactions,\n",
        "    log_shapiq_interactions,\n",
        "    get_top_interactions\n",
        ")\n",
        "\n",
        "\n",
        "@pytest.fixture\n",
        "def sample_data():\n",
        "    \"\"\"Create sample classification data for testing.\"\"\"\n",
        "    X, y = make_classification(\n",
        "        n_samples=50,\n",
        "        n_features=4,\n",
        "        n_informative=3,\n",
        "        n_redundant=1,\n",
        "        random_state=42\n",
        "    )\n",
        "    feature_names = [f\"feature_{i}\" for i in range(4)]\n",
        "    X_df = pd.DataFrame(X, columns=feature_names)\n",
        "    return X_df, y, feature_names\n",
        "\n",
        "\n",
        "@pytest.fixture\n",
        "def trained_model(sample_data):\n",
        "    \"\"\"Create a trained model for testing.\"\"\"\n",
        "    X_df, y, _ = sample_data\n",
        "    model = RandomForestClassifier(n_estimators=10, random_state=42)\n",
        "    model.fit(X_df, y)\n",
        "    return model\n",
        "\n",
        "\n",
        "def test_compute_shapiq_interactions_basic(sample_data, trained_model):\n",
        "    \"\"\"Test basic functionality of compute_shapiq_interactions.\"\"\"\n",
        "    X_df, _, feature_names = sample_data\n",
        "\n",
        "    # Test with small sample to speed up test\n",
        "    result_df = compute_shapiq_interactions(\n",
        "        trained_model,\n",
        "        X_df.head(5),  # Use only 5 samples for testing\n",
        "        feature_names,\n",
        "        max_order=2,\n",
        "        budget=64  # Small budget for fast testing\n",
        "    )\n",
        "\n",
        "    # Check structure\n",
        "    expected_columns = ['sample_idx', 'combination', 'value', 'order', 'feature_names']\n",
        "    assert all(col in result_df.columns for col in expected_columns)\n",
        "\n",
        "    # Check data types\n",
        "    assert result_df['sample_idx'].dtype in [np.int64, int]\n",
        "    assert result_df['value'].dtype in [np.float64, float]\n",
        "    assert result_df['order'].dtype in [np.int64, int]\n",
        "\n",
        "    # Check that we have interactions of different orders\n",
        "    if not result_df.empty:\n",
        "        orders = result_df['order'].unique()\n",
        "        assert len(orders) > 0\n",
        "        assert all(order <= 2 for order in orders)  # max_order=2\n",
        "\n",
        "\n",
        "def test_compute_shapiq_interactions_with_sampling(sample_data, trained_model):\n",
        "    \"\"\"Test compute_shapiq_interactions with n_samples parameter.\"\"\"\n",
        "    X_df, _, feature_names = sample_data\n",
        "\n",
        "    result_df = compute_shapiq_interactions(\n",
        "        trained_model,\n",
        "        X_df,\n",
        "        feature_names,\n",
        "        max_order=1,  # Simple interactions only\n",
        "        budget=32,\n",
        "        n_samples=3   # Sample only 3 rows\n",
        "    )\n",
        "\n",
        "    if not result_df.empty:\n",
        "        # Should have at most 3 different sample indices\n",
        "        unique_samples = result_df['sample_idx'].nunique()\n",
        "        assert unique_samples <= 3\n",
        "\n",
        "\n",
        "def test_compute_shapiq_interactions_empty_result():\n",
        "    \"\"\"Test handling of edge cases that might result in empty results.\"\"\"\n",
        "    # Create trivial data that might not generate interactions\n",
        "    X = pd.DataFrame([[1, 1], [1, 1]], columns=['a', 'b'])\n",
        "    y = [0, 0]\n",
        "\n",
        "    model = LogisticRegression()\n",
        "    model.fit(X, y)\n",
        "\n",
        "    result_df = compute_shapiq_interactions(\n",
        "        model, X, ['a', 'b'], max_order=1, budget=16\n",
        "    )\n",
        "\n",
        "    # Should return a DataFrame with correct structure even if empty\n",
        "    expected_columns = ['sample_idx', 'combination', 'value', 'order', 'feature_names']\n",
        "    assert all(col in result_df.columns for col in expected_columns)\n",
        "\n",
        "\n",
        "@patch('mlflow.log_metric')\n",
        "@patch('mlflow.log_artifact')\n",
        "def test_log_shapiq_interactions(mock_log_artifact, mock_log_metric, sample_data, trained_model):\n",
        "    \"\"\"Test log_shapiq_interactions with mocked MLflow calls.\"\"\"\n",
        "    X_df, _, feature_names = sample_data\n",
        "\n",
        "    with tempfile.TemporaryDirectory() as tmpdir:\n",
        "        output_path = os.path.join(tmpdir, \"test_interactions.csv\")\n",
        "\n",
        "        # Call the function\n",
        "        log_shapiq_interactions(\n",
        "            trained_model,\n",
        "            X_df.head(5),  # Small sample for testing\n",
        "            feature_names,\n",
        "            max_order=2,\n",
        "            top_n=3,\n",
        "            budget=32,\n",
        "            output_path=output_path\n",
        "        )\n",
        "\n",
        "        # Check that MLflow functions were called\n",
        "        assert mock_log_metric.called\n",
        "        assert mock_log_artifact.called\n",
        "\n",
        "        # Check some expected metric calls\n",
        "        metric_calls = [call[0][0] for call in mock_log_metric.call_args_list]\n",
        "        expected_metrics = [\n",
        "            \"shapiq_total_interactions\",\n",
        "            \"shapiq_unique_combinations\",\n",
        "            \"shapiq_max_order\",\n",
        "            \"shapiq_samples_analyzed\"\n",
        "        ]\n",
        "\n",
        "        for expected in expected_metrics:\n",
        "            assert any(expected in call for call in metric_calls), f\"Expected metric {expected} not found\"\n",
        "\n",
        "\n",
        "def test_get_top_interactions(sample_data, trained_model):\n",
        "    \"\"\"Test get_top_interactions utility function.\"\"\"\n",
        "    X_df, _, feature_names = sample_data\n",
        "\n",
        "    # First compute interactions\n",
        "    shapiq_df = compute_shapiq_interactions(\n",
        "        trained_model,\n",
        "        X_df.head(10),\n",
        "        feature_names,\n",
        "        max_order=2,\n",
        "        budget=64\n",
        "    )\n",
        "\n",
        "    if not shapiq_df.empty:\n",
        "        # Test getting top interactions\n",
        "        top_interactions = get_top_interactions(shapiq_df, top_n=5)\n",
        "        assert len(top_interactions) <= 5\n",
        "\n",
        "        # Check structure\n",
        "        expected_columns = ['combination', 'feature_names', 'order', 'mean', 'std', 'count', 'abs_mean']\n",
        "        assert all(col in top_interactions.columns for col in expected_columns)\n",
        "\n",
        "        # Test filtering by order\n",
        "        if len(shapiq_df['order'].unique()) > 1:\n",
        "            order_filtered = get_top_interactions(shapiq_df, top_n=3, order=1)\n",
        "            if not order_filtered.empty:\n",
        "                assert all(order_filtered['order'] == 1)\n",
        "\n",
        "\n",
        "def test_compute_shapiq_interactions_error_handling():\n",
        "    \"\"\"Test error handling in compute_shapiq_interactions.\"\"\"\n",
        "    # Create data that might cause issues\n",
        "    X = pd.DataFrame([[np.nan, 1], [2, np.nan]], columns=['a', 'b'])\n",
        "    y = [0, 1]\n",
        "\n",
        "    model = LogisticRegression()\n",
        "\n",
        "    # This should handle errors gracefully and return empty DataFrame\n",
        "    try:\n",
        "        model.fit([[1, 1], [2, 2]], [0, 1])  # Fit with clean data\n",
        "        result_df = compute_shapiq_interactions(model, X, ['a', 'b'], max_order=1, budget=16)\n",
        "\n",
        "        # Should return DataFrame with expected structure even on error\n",
        "        expected_columns = ['sample_idx', 'combination', 'value', 'order', 'feature_names']\n",
        "        assert all(col in result_df.columns for col in expected_columns)\n",
        "\n",
        "    except Exception:\n",
        "        # If an exception occurs, that's also acceptable for this edge case\n",
        "        pass\n",
        "\n",
        "\n",
        "@patch('mlflow.log_metric')\n",
        "@patch('mlflow.log_artifact')\n",
        "def test_log_shapiq_interactions_empty_result(mock_log_artifact, mock_log_metric):\n",
        "    \"\"\"Test log_shapiq_interactions when no interactions are computed.\"\"\"\n",
        "    # Mock compute_shapiq_interactions to return empty DataFrame\n",
        "    with patch('mlops.shapiq_utils.compute_shapiq_interactions') as mock_compute:\n",
        "        mock_compute.return_value = pd.DataFrame(columns=['sample_idx', 'combination', 'value', 'order', 'feature_names'])\n",
        "\n",
        "        # This should handle empty results gracefully\n",
        "        log_shapiq_interactions(\n",
        "            MagicMock(),  # Mock model\n",
        "            pd.DataFrame([[1, 2]], columns=['a', 'b']),\n",
        "            ['a', 'b'],\n",
        "            max_order=1\n",
        "        )\n",
        "\n",
        "        # Should not log metrics or artifacts for empty results\n",
        "        assert not mock_log_metric.called\n",
        "        assert not mock_log_artifact.called\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pytest.main([__file__])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
